{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "247f9220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "\n",
    "import os\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from markdown import markdown\n",
    "from swifter import swifter\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from bertopic.representation import KeyBERTInspired, PartOfSpeech, MaximalMarginalRelevance\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.dimensionality import BaseDimensionalityReduction\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from hdbscan import HDBSCAN\n",
    "from umap import UMAP\n",
    "\n",
    "from modules.constants import *\n",
    "\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aa74769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def markdown_to_text(markdown_string):\n",
    "    \"\"\" Converts a markdown string to plaintext \"\"\"\n",
    "\n",
    "    # md -> html -> text since BeautifulSoup can extract text cleanly\n",
    "    html = markdown(markdown_string)\n",
    "\n",
    "    # remove code snippets\n",
    "    html = re.sub(r'<pre>(.*?)</pre>', ' ', html)\n",
    "    html = re.sub(r'<code>(.*?)</code >', ' ', html)\n",
    "\n",
    "    # extract text\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    text = ''.join(soup.findAll(text=True))\n",
    "\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    text = markdown_to_text(text).strip()\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    for agent in AGENTS:\n",
    "        text = text.replace(agent.lower(), \"\")\n",
    "\n",
    "    text = re.sub(r\"http\\S+\", \" <URL> \",text) #Removing URLs \n",
    "    \n",
    "    # html=re.compile(r'<.*?>') \n",
    "    \n",
    "    # text = html.sub(r'',text) #Removing html tags\n",
    "    \n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               \"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "                               \"\\U0001F300-\\U0001F5FF\"  # Symbols & pictographs\n",
    "                               \"\\U0001F680-\\U0001F6FF\"  # Transport & map symbols\n",
    "                               \"\\U0001F700-\\U0001F77F\"  # Alchemical symbols\n",
    "                               \"\\U0001F780-\\U0001F7FF\"  # Geometric shapes\n",
    "                               \"\\U0001F800-\\U0001F8FF\"  # Supplemental arrows\n",
    "                               \"\\U0001F900-\\U0001F9FF\"  # Supplemental symbols and pictographs\n",
    "                               \"\\U0001FA00-\\U0001FA6F\"  # Symbols and pictographs extended-A\n",
    "                               \"\\U0001FA70-\\U0001FAFF\"  # Symbols and pictographs extended-B\n",
    "                               \"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "                               \"\\U000024C2-\\U0001F251\"  # Enclosed characters\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text) #Removing emojis\n",
    "    #common_punctuation = r'.,?!:;\"\\'-\\\\/'\n",
    "    #text = re.sub(rf'[^\\w\\s{re.escape(common_punctuation)}]', '', text)\n",
    "    #text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae186a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"google/embeddinggemma-300m\", device=\"cuda:0\" )\n",
    "tokenizer = model.tokenizer\n",
    "\n",
    "def get_embeddings(\n",
    "    text: str,\n",
    "    token_limit: int = 2000,\n",
    "    stride: int = 1024\n",
    ") -> np.ndarray:\n",
    "\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    total_tokens = len(tokens)\n",
    "\n",
    "    embeddings = []\n",
    "    start = 0\n",
    "    while start < total_tokens:\n",
    "        end = min(start + token_limit, total_tokens)\n",
    "        chunk_tokens = tokens[start:end]\n",
    "        chunk_text = tokenizer.decode(chunk_tokens)\n",
    "        chunk_text = \"task: clustering | query: \" + chunk_text\n",
    "\n",
    "        # Encode chunk with task type\n",
    "        emb = model.encode(\n",
    "            chunk_text,\n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=True\n",
    "        )\n",
    "        embeddings.append(emb)\n",
    "        start += stride\n",
    "\n",
    "    if not embeddings:\n",
    "        print(f\"No embeddings generated — check your input text or model setup. text: {text}\")\n",
    "\n",
    "    pooled_embedding = np.mean(np.vstack(embeddings), axis=0)\n",
    "\n",
    "    return pooled_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f41dee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_embeddings(df):\n",
    "    embeddings = []\n",
    "    embeddings_title = []\n",
    "    data = []\n",
    "    ids = []\n",
    "    for _, row in tqdm(df.iterrows()):\n",
    "        title_emb = get_embeddings(row[\"title\"]) if row[\"title\"].strip() else None\n",
    "        body_emb  = get_embeddings(row[\"body\"])  if row[\"body\"].strip() else None\n",
    "\n",
    "        if title_emb is None and body_emb is None:\n",
    "            continue\n",
    "\n",
    "        if title_emb is None:\n",
    "            print(\"title is none\")\n",
    "            title_emb = np.zeros(body_emb.shape)\n",
    "        if body_emb is None:\n",
    "            print(\"body is none\")\n",
    "            body_emb = np.zeros(title_emb.shape)\n",
    "            \n",
    "\n",
    "        ids.append(row[\"id\"])\n",
    "        embeddings.append(np.concatenate((title_emb, body_emb), axis=0))\n",
    "        embeddings_title.append(title_emb)\n",
    "        data.append(row[\"title\"] + row[\"body\"])\n",
    "        \n",
    "    return np.array(embeddings_title), np.array(embeddings)\n",
    "\n",
    "def get_data(df):\n",
    "    data = []\n",
    "    data_title = []\n",
    "    for _, row in tqdm(df.iterrows()):\n",
    "        data.append(row[\"title\"] + row[\"body\"])\n",
    "        data_title.append(row[\"title\"])\n",
    "\n",
    "    return np.array(data_title), np.array(data)\n",
    "\n",
    "def load_embeddings(type):\n",
    "    emb_title = None\n",
    "    emb = None\n",
    "    \n",
    "    if type == \"ai\":\n",
    "        emb_title = np.load(\"./Outputs/Embeddings/AIEmbeddings_title.npy\")\n",
    "        emb = np.load(\"./Outputs/Embeddings/AIEmbeddings.npy\")\n",
    "    elif type == \"human\":\n",
    "        emb_title = np.load(\"./Outputs/Embeddings/HumanEmbeddings_title.npy\")\n",
    "        emb = np.load(\"./Outputs/Embeddings/HumanEmbeddings.npy\")\n",
    "\n",
    "    return emb_title, emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48000bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833130/656511650.py:13: DeprecationWarning:\n",
      "\n",
      "Call to deprecated method findAll. (Replaced by find_all) -- Deprecated since version 4.0.0.\n",
      "\n",
      "/tmp/ipykernel_833130/656511650.py:13: DeprecationWarning:\n",
      "\n",
      "Call to deprecated method findAll. (Replaced by find_all) -- Deprecated since version 4.0.0.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ai = pd.read_csv(\"Outputs/PerformancePRs/POP_PULL_Requests_LLM_filtered.csv\")\n",
    "df_ai[\"title\"] = df_ai[\"title\"].fillna(\"\").apply(lambda x: clean_text(x))\n",
    "df_ai[\"body\"] = df_ai[\"body\"].fillna(\"\").apply(lambda x: clean_text(x))\n",
    "\n",
    "df_human = pd.read_csv(\"Outputs/PerformancePRs/HUMAN_PULL_Requests_llm_filtered.csv\")\n",
    "df_human[\"title\"] = df_human[\"title\"].fillna(\"\").apply(lambda x: clean_text(x))\n",
    "df_human[\"body\"] = df_human[\"body\"].fillna(\"\").apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc9ab58",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"Outputs/Embeddings\", exist_ok=True)\n",
    "\n",
    "embeddings_ai_title, embeddings_ai = prepare_embeddings(df_ai) \n",
    "np.save(\"./Outputs/Embeddings/AIEmbeddings.npy\", embeddings_ai)\n",
    "np.save(\"./Outputs/Embeddings/AIEmbeddings_title.npy\", embeddings_ai_title)\n",
    "\n",
    "data_ai_title, data_ai = get_data(df_ai)\n",
    "\n",
    "embeddings_human_title, embeddings_human = prepare_embeddings(df_human) \n",
    "np.save(\"./Outputs/Embeddings/HumanEmbeddings.npy\", embeddings_human)\n",
    "np.save(\"./Outputs/Embeddings/HumanEmbeddings_title.npy\", embeddings_human_title)\n",
    "\n",
    "data_human_title, data_human = get_data(df_human)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f0591147",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1160it [00:00, 18495.09it/s]\n",
      "273it [00:00, 18575.73it/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings_ai_title, embeddings_ai = load_embeddings(\"ai\") \n",
    "data_ai_title, data_ai = get_data(df_ai)\n",
    "\n",
    "embeddings_human_title, embeddings_human = load_embeddings(\"human\") \n",
    "data_human_title, data_human = get_data(df_human)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b529348e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"Outputs/BERTopic\", exist_ok=True)\n",
    "\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=5, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "umap_model = UMAP(n_neighbors=5, n_components=10, min_dist=0.0, metric='cosine', random_state=seed)\n",
    "\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "vectorizer_model = CountVectorizer(stop_words= \"english\", ngram_range=(1, 1))\n",
    "representation_model = [KeyBERTInspired(), MaximalMarginalRelevance(diversity=0.3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ee2e1536",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-07 13:03:42,383 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-12-07 13:03:45,235 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-12-07 13:03:45,236 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-12-07 13:03:45,371 - BERTopic - Cluster - Completed ✓\n",
      "2025-12-07 13:03:45,372 - BERTopic - Representation - Extracting topics using c-TF-IDF for topic reduction.\n",
      "2025-12-07 13:03:45,504 - BERTopic - Representation - Completed ✓\n",
      "2025-12-07 13:03:45,504 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2025-12-07 13:03:45,523 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-12-07 13:04:07,584 - BERTopic - Representation - Completed ✓\n",
      "2025-12-07 13:04:07,585 - BERTopic - Topic reduction - Reduced number of topics from 58 to 43\n"
     ]
    }
   ],
   "source": [
    "topic_model = BERTopic(\n",
    "    embedding_model=model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    representation_model=representation_model,\n",
    "    ctfidf_model=ctfidf_model,\n",
    "    calculate_probabilities=True,\n",
    "    top_n_words=10,\n",
    "    verbose=True,\n",
    "    nr_topics=\"auto\"\n",
    ")\n",
    "\n",
    "docs = data_ai\n",
    "embs = embeddings_ai\n",
    "\n",
    "topics, probs = topic_model.fit_transform(docs, embeddings=embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4c90eac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Topic",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Representation",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Representative_Docs",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "672faa34-d900-4877-86e8-ac7c34a41d12",
       "rows": [
        [
         "0",
         "-1",
         "208",
         "-1_github_dev_fixes_coding",
         "['github', 'dev', 'fixes', 'coding', 'gradle', 'pnpm', 'agents_api', 'development', 'codeql', 'risedev']",
         "[\"add get-sqldscserverprotocol public command with cim supportsummary\\nimplements the enhancement requested in issue #2104 by creating a new public command get-sqldscserverprotocol that provides access to sql server protocol information using cim instances with smo fallback support.\\nchanges made\\nnew public command\\n\\nget-sqldscserverprotocol: new public command that replaces the need to use the private get-serverprotocolobject function directly\\nsupports all three sql server network protocols: tcpip, namedpipes, and sharedmemory\\nincludes comprehensive comment-based help with examples\\n\\ncim instance implementation\\n\\nget-serverprotocolobjectbycim: new private function implementing cim-based approach as suggested in the issue\\nautomatically detects sql server version and uses appropriate namespace (supports sql server 2008-2022)\\nuses get-ciminstance -classname 'servernetworkprotocol' for better performance\\n\\nsmo fallback support\\n\\nget-serverprotocolobjectbysmo: extracted existing smo logic into separate private function\\nprovides automatic fallback when cim instances are unavailable\\nmaintains full backward compatibility\\n\\nenhanced features\\n\\nintelligent fallback: automatically tries cim first, falls back to smo if needed\\n-usecim parameter: allows forcing cim usage when desired\\nmultiple namespace support: tries multiple sql server cim namespaces for version compatibility\\ncomprehensive error handling: proper error messages with localization support\\n\\nusage examples\\n```powershell\\nbasic usage - get tcp/ip protocol for default instance\\nget-sqldscserverprotocol -instancename 'mssqlserver' -protocolname 'tcpip'\\nget named pipes protocol for named instance on remote server\\nget-sqldscserverprotocol -servername 'remoteserver' -instancename 'sql2019' -protocolname 'namedpipes'\\nforce cim usage\\nget-sqldscserverprotocol -instancename 'mssqlserver' -protocolname 'sharedmemory' -usecim\\n```\\nbackward compatibility\\n no breaking changes - all existing dsc resources continue to work unchanged as the original private get-serverprotocolobject function remains available and unmodified.\\ntesting\\n\\nadded comprehensive unit tests following pester v5 syntax and sqlserverdsc conventions\\ntests cover cim approach, smo fallback, error handling, and parameter validation\\nall tests pass and follow existing module patterns\\n\\ndocumentation\\n\\nadded 10 new localized strings for all function messages\\ncomplete comment-based help with synopsis, description, parameters, examples, and notes\\nfollowing all sqlserverdsc documentation standards\\n\\nfiles added/modified\\nadded files\\n\\nsource/public/get-sqldscserverprotocol.ps1\\nsource/private/get-serverprotocolobjectbycim.ps1\\nsource/private/get-serverprotocolobjectbysmo.ps1\\ntests/unit/public/get-sqldscserverprotocol.tests.ps1\\n\\nmodified files\\n\\nsource/en-us/sqlserverdsc.strings.psd1 (added localized strings)\\nsource/prefix.ps1 (added cim preference variable)\\n\\nbenefits\\n\\nbetter performance: cim instances provide better performance than smo objects\\nmodern approach: uses current powershell and cim standards\\npublic api: now available for broader community use beyond dsc resources\\nfuture-proof: automatic detection of sql server versions\\nreliable fallback: works in environments where cim may not be available\\n\\nresolves #2104\\ntype of change\\n\\n[x] new feature (non-breaking change which adds functionality)\\n[x] enhancement (improvement to existing functionality)\\n\\nchecklist\\n\\n[x] code follows the project's coding standards\\n[x] self-review of code completed\\n[x] code has been commented, particularly in hard-to-understand areas\\n[x] corresponding changes to documentation have been made\\n[x] unit tests added that prove the fix is effective or that the feature works\\n[x] new and existing unit tests pass locally\\n[x] no breaking changes introduced\\n\\n\\n[!warning]\\n\\nfirewall rules blocked me from connecting to one or more addresses\\ni tried to connect to the following addresses, but was blocked by firewall rules:\\n\\nwww.powershellgallery.com\\ntriggering command: pwsh -c ./build.ps1 -tasks &#39;build&#39; (dns block)\\n\\nif you need me to access, download, or install something from one of these locations, you can either:\\n\\nconfigure actions setup steps to set up my environment, which run before the firewall is enabled\\nadd the appropriate urls or hosts to my firewall allow list\\n\\n\\n\\n start  coding agent tips \\n\\n you can make  smarter by setting up custom instructions, customizing its development environment and configuring model context protocol (mcp) servers. learn more  coding agent tips in the docs.\\n reviewable:start \\n\\nthis change is\\u2002\\n reviewable:end\", 'implement intelligent test selection for github actionssummary\\nthis pr implements an intelligent test selection system that reduces ci runtime from 5+ minutes to targeted windows (1-5 minutes) based on the scope of changes in a pull request. the system analyzes git diff to determine which tests are relevant and executes only those tests, providing 60-80% time savings for focused changes while maintaining comprehensive coverage for complex changes.\\nproblem statement\\nthe current ci system runs the full test suite for every pr, regardless of the scope of changes. this results in:\\n- consistent 5+ minute runtime even for documentation-only changes\\n- inefficient use of ci resources\\n- slower feedback for developers\\n- no differentiation between small focused changes and large complex changes\\nsolution\\n core components\\n\\ntools/test_selector.py - intelligent test selection engine\\nanalyzes git diff to categorize file changes\\nmaps file patterns to relevant test categories\\nprovides both human-readable and json output for ci integration\\n\\nimplements fallback to full test suite for complex changes\\n\\n\\ntools/test_docs_build.py - lightweight documentation testing\\n\\nvalidates markdown and rst files for basic formatting\\nchecks configuration files exist and are valid\\n\\ncompletes in ~30 seconds vs full documentation build\\n\\n\\n.github/workflows/intelligent-testing.yml - enhanced ci workflow\\n\\ndynamic test matrix generation based on change analysis\\nparallel execution paths for fast tests vs comprehensive tests\\n\\nautomatic fallback mechanism for edge cases\\n\\n\\ntools/validate_test_selection.py - system validation\\n\\ndemonstrates functionality and validates correct operation\\nshows expected benefits and time savings\\n\\n test categories & performance\\n| change type | previous runtime | new runtime | improvement | test strategy |\\n|-------------|-----------------|-------------|-------------|---------------|\\n| documentation-only | ~5+ minutes | ~1-2 minutes | 60-80% faster | lightweight docs validation |\\n| superanimal changes | ~5+ minutes | ~3-4 minutes | 20-40% faster | superanimal-specific tests |\\n| focused components | ~5+ minutes | ~2-3 minutes | 40-60% faster | component-specific tests |\\n| complex/mixed changes | ~5+ minutes | ~5+ minutes | maintains coverage | full test suite |\\n smart categorization\\nthe system categorizes changes into:\\n\\ndocs: documentation files (*.md, *.rst, docs/, config files)\\nsuperanimal: modelzoo and superanimal components (deeplabcut/modelzoo/, *superanimal*)\\ncore: core deeplabcut functionality (deeplabcut/core/, deeplabcut/pose_estimation_*/)\\nmultianimal: multi-animal specific features (*multianimal*, *multi*)\\nvideo: video processing components (*video*, prediction apis)\\ntools: development tools (tools/)\\n\\nusage examples\\n```bash\\nanalyze current changes and show what tests would run\\npython tools/test_selector.py --dry-run\\nget json output for ci integration\\npython tools/test_selector.py --output-json --base main\\nvalidate the system works correctly\\npython tools/validate_test_selection.py\\ntest documentation build independently\\npython tools/test_docs_build.py\\n```\\nexample scenarios\\ndocumentation-only pr\\nbash\\n$ python tools/test_selector.py --dry-run\\n found 1 changed files: docs/installation.md\\n categories: docs\\n tests to run: python tools/test_docs_build.py\\n⏱  estimated runtime: 1-2 minutes\\nsuperanimal model changes\\nbash\\n$ python tools/test_selector.py --dry-run  \\n found 3 changed files: deeplabcut/modelzoo/superanimal_*.py\\n categories: superanimal\\n tests to run: pytest tests/test_predict_supermodel.py tests/pose_estimation_pytorch/modelzoo/\\n⏱  estimated runtime: 3-4 minutes\\nmixed/complex changes\\nbash\\n$ python tools/test_selector.py --dry-run\\n found 12 changed files across multiple components\\n categories: core, superanimal, video, uncategorized  \\n tests to run: python examples/testscript.py, pytest\\n⏱  estimated runtime: 5+ minutes (full test suite)\\nintegration\\nthe system integrates seamlessly with the existing ci pipeline:\\n\\nmaintains backward compatibility - falls back to full test suite when unsure\\nzero false negatives - never skips tests that should run for a given change\\nconservative approach - prefers running extra tests over missing important ones\\njson api - easy integration with github actions and other ci systems\\n\\nvalidation\\nthe implementation includes comprehensive validation:\\n```bash\\n$ python tools/validate_test_selection.py\\n success: intelligent test selection system is working correctly!\\nkey benefits:\\n• documentation-only changes: ~1-2 minutes (vs 5+ minutes)\\n• superanimal changes: ~3-4 minutes (vs 5+ minutes)\\n• focused component changes: ~2-3 minutes (vs 5+ minutes)\\n• complex changes: falls back to full test suite (~5+ minutes)\\n expected ci time reduction: 60-80% for focused changes\\n```\\nexpected impact\\n\\n60-80% reduction in ci time for focused changes (docs, single components)\\nfaster feedback for developers on common change types\\nresource efficiency - reduced compute usage for github actions\\nmaintained quality - full test coverage preserved for complex changes\\ndeveloper experience - quicker iteration cycles for documentation and focused feature work\\n\\nthis implementation addresses the core requirement of reducing test suite runtime to approximately 5 minutes while intelligently adapting to the scope of changes in each pull request.\\n start  coding agent tips \\n\\n share your feedback on  coding agent for the chance to win a $200 gift card! click here to start the survey.', 'refactor asset conversion/management: move to cli tool and enable direct wad loadingthis pr implements a comprehensive refactoring of openkeeper\\'s asset conversion and management system to address the requirements outlined in #486.\\n key changes\\n1. standalone asset converter cli\\n\\nnew assetconvertercli.java: replaces gui-based conversion integrated into game startup\\ncommand-line interface with options for custom dk ii folder, force overwrite, and skipping models\\ngradle integration: added assetconvertercli task for easy execution\\ncross-platform friendly: enables android builds without requiring runtime conversion\\n\\n```bash\\nconvert all assets\\n./gradlew assetconvertercli\\nskip model conversion and load directly from wad\\n./gradlew assetconvertercli -pcliargs=\"--skip-models\"\\n```\\n2. direct wad file loading\\n\\nnew wadassetlocator.java: loads assets directly from original dk ii files\\npriority system: extracted assets take precedence over wad files for modding\\nsupport for multiple formats:\\nkmf models from meshes.wad\\ntextures from enginetextures.dat\\nsounds from sounds.wad\\n\\n3. removed .j3o model generation\\n\\nmodified convertmodels.java: extracts raw kmf files instead of converting to .j3o\\nruntime loading: models converted from kmf to jme objects at runtime using existing kmfmodelloader\\nbetter editability: raw kmf files can be viewed/modified by external tools\\nreduced storage: eliminates duplicate data in both kmf and .j3o formats\\n\\n4. game startup optimization\\n\\nmodified main.java: removed asset conversion dependency from startup\\nfaster launches: game starts immediately without conversion delays\\ndual asset loading: registers both filelocator (extracted) and wadassetlocator (original)\\n\\n benefits\\nfor users\\n\\ninstant startup: no more waiting for asset conversion when launching the game\\nsmaller disk usage: can skip model conversion entirely\\nbetter modding support: easy to replace individual assets while keeping others original\\n\\nfor developers\\n\\ncross-platform builds: especially important for android deployment (#218)\\ncleaner architecture: asset conversion separated from game logic\\neasier testing: can test game without full asset conversion\\n\\n asset loading priority\\n\\nextracted/converted assets (highest priority) - located in assets folder\\noriginal wad files (fallback) - loaded directly from dk ii installation  \\nbuilt-in defaults (lowest priority) - embedded in jar\\n\\n migration\\nexisting users: no action required. the game automatically uses the new priority system.\\nfresh installs: \\n1. set dk ii folder in game settings\\n2. optionally run ./gradlew assetconvertercli for faster loading\\n documentation\\nadded comprehensive asset_conversion_refactor.md with:\\n- usage examples and command-line options\\n- migration guide for existing users\\n- technical implementation details\\n- troubleshooting information\\n testing\\n\\ncli argument parsing validated with automated tests\\nbackward compatibility maintained for existing converted assets\\nasset loading priority system verified\\n\\nthis refactoring achieves all the goals outlined in the issue: moves conversion out of startup, enables direct wad loading, removes .j3o generation, and maintains excellent modding support.\\nfixes #486.\\n\\n[!warning]\\n\\nfirewall rules blocked me from connecting to one or more addresses\\ni tried to connect to the following addresses, but was blocked by firewall rules:\\n\\njitpack.io\\ntriggering command: /usr/lib/jvm/temurin-17-jdk-amd64/bin/java --add-opens=java.base/java.lang=all-unnamed --add-opens=java.base/java.lang.invoke=all-unnamed --add-opens=java.base/java.util=all-unnamed --add-opens=java.prefs/java.util.prefs=all-unnamed --add-exports=jdk.compiler/com.sun.tools.javac.api=all-unnamed --add-exports=jdk.compiler/com.sun.tools.javac.util=all-unnamed --add-opens=java.base/java.util=all-unnamed --add-opens=java.prefs/java.util.prefs=all-unnamed --add-opens=java.base/java.nio.charset=all-unnamed --add-opens=java.base/java.net=all-unnamed --add-opens=java.base/java.util.concurrent.atomic=all-unnamed --add-opens=java.xml/javax.xml.namespace=all-unnamed -xx:maxmetaspacesize=384m -xx:&#43;heapdumponoutofmemoryerror -xms256m -xmx512m -dfile.encoding=utf-8 -duser.country -duser.language=en -duser.variant -cp /home/redacted/.gradle/wrapper/dists/gradle-8.14-bin/38aieal9i53h9rfe7vjup95b9/gradle-8.14/lib/gradle-daemon-main-8.14.jar -javaagent:/home/redacted/.gradle/wrapper/dists/gradle-8.14-bin/38aieal9i53h9rfe7vjup95b9/gradle-8.14/lib/agents/gradle-instrumentation-agent-8.14.jar org.gradle.launcher.daemon.bootstrap.gradledaemon 8.14 (dns block)\\n\\nif you need me to access, download, or install something from one of these locations, you can either:\\n\\nconfigure actions setup steps to set up my environment, which run before the firewall is enabled\\nadd the appropriate urls or hosts to my firewall allow list\\n\\n\\n\\n start  coding agent tips \\n\\n you can make  smarter by setting up custom instructions, customizing its development environment and configuring model context protocol (mcp) servers. learn more  coding agent tips in the docs.']"
        ],
        [
         "1",
         "0",
         "360",
         "0_fixes_coding_lint_implementation",
         "['fixes', 'coding', 'lint', 'implementation', 'ui', 'eslint', 'typescript', 'caching', 'coderabbit', 'documentation']",
         "['feat: implement async notification and telemetry system (phase 1-3)summary\\nthis pr implements the first three phases of the async notification and telemetry system as outlined in #833. it introduces a non-blocking event bus architecture that decouples error reporting from notification/telemetry processing, preventing any blocking operations during error handling.\\nrelated issues\\n\\nimplements phases 1-3 of #833 (async notification/telemetry system)\\naddresses performance concerns from #825 (error handling optimization)\\nincludes error deduplication from #827 (reduce telemetry noise)\\n\\nchanges\\nphase 1: core event bus infrastructure \\n\\ncreated internal/events package with non-blocking event bus\\nimplemented worker pool pattern with configurable workers (default: 4)\\nadded trypublish() method that never blocks (drops events if buffer full)\\ncomprehensive unit tests with 100% coverage\\nstructured logging with internal/logging package\\natomic operations for thread-safe metrics\\n\\nphase 2: error deduplication system \\n\\nhash-based deduplication with configurable ttl (default: 5 minutes)\\nlru eviction for memory-bounded cache (max 10,000 entries)\\nperiodic cleanup goroutine for expired entries\\ncomprehensive deduplication metrics (hit rate, suppression count)\\nreduces telemetry volume by suppressing duplicate errors\\n\\nphase 3: error package integration \\n\\nenhanced enhancederror to implement errorevent interface\\ncreated eventpublisher interface to avoid circular dependencies\\nadapter pattern connects errors and events packages\\nmaintains backward compatibility - falls back to sync processing if event bus not initialized\\nverified no circular dependencies through compilation tests\\n\\narchitecture\\nerrors package → eventbus → deduplication → notification workers (future)\\n                                         ↘ → telemetry workers (future)\\nkey design principles\\n\\nzero-cost when disabled: no overhead when telemetry/notifications are off\\nnon-blocking guarantees: trypublish() never blocks, uses select with default\\nno circular dependencies: uses interfaces to decouple packages\\nbackward compatible: falls back to legacy sync processing\\nproduction ready: proper error handling, metrics, and tests\\n\\nperformance characteristics\\n\\nerror creation overhead: < 100ns when telemetry disabled (maintains #825 optimizations)\\nevent publishing: non-blocking with overflow protection\\ndeduplication: o(1) hash lookup with < 100ns overhead\\nmemory usage: bounded by configuration (10k events max)\\nzero goroutine leaks verified\\n\\ntesting\\n\\ncomprehensive unit tests for all components\\nintegration tests verify no circular dependencies\\nfixed deadlock issues in error hooks\\nproper test isolation and cleanup\\nall tests pass without timeouts or race conditions\\n\\nconfiguration\\nthe system supports configuration through the new event bus config:\\n```go\\ntype config struct {\\n    buffersize    int                    // event buffer size (default: 10,000)\\n    workers       int                    // worker goroutines (default: 4)\\n    enabled       bool                   // enable event bus (default: true)\\n    deduplication *deduplicationconfig   // deduplication settings\\n}\\ntype deduplicationconfig struct {\\n    enabled         bool          // enable deduplication (default: true)\\n    ttl             time.duration // duplicate window (default: 5m)\\n    maxentries      int          // max cache size (default: 10,000)\\n    cleanupinterval time.duration // cleanup frequency (default: 1m)\\n}\\n```\\nnext steps\\nthis pr lays the foundation for async processing. future phases will:\\n- phase 4: migrate notification system to use event bus workers\\n- phase 5: migrate telemetry system with batching and circuit breakers\\n- phase 6: remove legacy sync processing code\\n- phase 7: add monitoring and production tuning\\nbreaking changes\\nnone. the system maintains full backward compatibility.\\nchecklist\\n\\n[x] tests pass\\n[x] linter passes (golangci-lint run)\\n[x] no circular dependencies\\n[x] backward compatible\\n[x] performance requirements met\\n[x] documentation updated\\n\\nhow to test\\n\\nrun tests: go test ./internal/events/... ./internal/errors/...\\nverify no circular dependencies compile\\ncheck deduplication with repeated errors\\nconfirm non-blocking behavior under load\\n\\n generated with  code\\nco-authored-by:  noreply@anthropic.com\\n this is an auto-generated comment: release notes by coderabbit.ai \\nsummary by coderabbit\\n\\nnew features\\nintroduced an asynchronous event bus system for non-blocking error event processing with deduplication and metrics.\\nadded error deduplication to suppress duplicate error events within a configurable time window.\\nprovided integration between error reporting and the event bus for improved decoupling and extensibility.\\n\\nadded new error accessors for retrieving underlying error and message details.\\n\\n\\nbug fixes\\n\\n\\nimproved thread safety and encapsulation in error context handling.\\n\\n\\ntests\\n\\n\\nadded comprehensive unit and integration tests for event bus, deduplication, and error-event integration.\\n\\n\\nrefactor\\n\\nupdated error category handling to use string values for improved consistency.\\n\\nimproved synchronization and state management in error hook and telemetry logic.\\n\\n\\ndocumentation\\n\\nexpanded best practices and lessons learned on defensive coding, testing, atomic usage, performance, and error handling.\\n\\n end of auto-generated comment: release notes by coderabbit.ai', \"performance optimizations and code quality improvements for gsy github app flutter overview\\nthis pr implements comprehensive performance optimizations and code quality improvements for the gsy github app flutter project. the changes focus on reducing memory usage, improving build performance, and enhancing code maintainability while preserving all existing functionality.\\n key optimizations\\nwidget performance improvements\\n\\nconst constructor optimizations: converted widgets like gsycarditem, gsyflexbutton, and gsyicontext to use const constructors with static constants, reducing object creation during builds\\nstatelesswidget conversion: converted gsyinputwidget from statefulwidget to statelesswidget for better performance\\nwidget building logic: optimized gsytitlebar with better conditional rendering and sizedbox.shrink() instead of empty container()\\n\\nnetwork layer optimizations\\n\\n <URL>  singleton: implemented proper singleton pattern with lazy initialization and batch interceptor addition\\nresource management: improved memory management and reduced object creation\\n\\ndatabase layer improvements\\n\\nsqlmanager optimization: added singleton pattern with initialization state management and optimized table existence queries\\nbasedbprovider enhancement: improved type safety, error handling, and resource management\\n\\napplication layer optimizations\\n\\nlocale checking: optimized language locale checking in app.dart using any() instead of loops\\nlogger performance: made logger debug-aware to reduce memory usage in production builds\\nerror handling: improved errorpage with constants and better code structure\\n\\nbuild & dependency management\\n\\npubspec.yaml cleanup: removed commented dependencies and organized remaining ones by category with documentation\\nanalysis_options.yaml: added performance-oriented lint rules including const constructor checks\\nenhanced .gitignore: added more build artifacts and platform-specific exclusions\\n\\n performance benefits\\nmemory usage\\n\\nreduced runtime object creation through static constants\\nbetter resource management with proper singleton patterns\\ndebug-aware logging reduces production memory footprint\\n\\nbuild performance\\n\\nmore const constructors enable flutter's build optimizations\\nreduced widget tree rebuilds through statelesswidget conversions\\noptimized database initialization prevents redundant operations\\n\\ncode quality\\n\\nimproved type safety and null safety handling\\nbetter error handling and resource cleanup\\nenhanced code documentation and organization\\n\\n technical details\\nbefore/after examples\\nwidget optimization example:\\n```dart\\n// before: runtime object creation\\nwidget build(buildcontext context) {\\n  edgeinsets? margin = this.margin;\\n  margin ??= const edgeinsets.only(left: 10.0, top: 10.0, right: 10.0, bottom: 10.0);\\n  // ...\\n}\\n// after: compile-time constants\\nstatic const edgeinsets _defaultmargin = edgeinsets.only(left: 10.0, top: 10.0, right: 10.0, bottom: 10.0);\\nconst gsycarditem({\\n  this.margin = _defaultmargin,\\n  // ...\\n});\\n```\\ndatabase optimization example:\\n```dart\\n// before: potential multiple initializations\\nstatic init() async {\\n  _database = await opendatabase(path, version: _version);\\n}\\n// after: singleton with state management\\nstatic future init() async {\\n  if (_database != null) return;\\n  if (_isinitializing) {\\n    await _dbcompleter.future;\\n    return;\\n  }\\n  // safe initialization logic\\n}\\n```\\n documentation\\nadded comprehensive optimizations.md documenting all changes and their performance impact, serving as a reference for future optimization efforts.\\n testing\\nall optimizations maintain backward compatibility and preserve existing functionality. the changes focus on implementation improvements without altering public apis.\\n impact\\nthese optimizations provide:\\n-  faster app startup and navigation\\n-  reduced memory consumption\\n-  better code maintainability\\n-  improved user experience\\n-  enhanced developer experience with better linting\\n\\nnote: these changes follow flutter best practices and are designed to be non-breaking while providing measurable performance improvements.\\n\\n[!warning]\\n\\nfirewall rules blocked me from connecting to one or more addresses\\ni tried to connect to the following addresses, but was blocked by firewall rules:\\n\\ncanonical-bos01.cdn.snapcraftcontent.com\\nesm.ubuntu.com\\ntriggering command: /usr/lib/apt/methods/ <URL>  (dns block)\\n <URL> \\ntriggering command: curl --retry 3 --continue-at - --location --output /home/redacted/flutter/bin/cache/dart-sdk-linux-x64.zip redacted (http block)\\n <URL> \\ntriggering command: wget -o flutter.tar.xz redacted (http block)\\n\\nif you need me to access, download, or install something from one of these locations, you can either:\\n\\nconfigure actions setup steps to set up my environment, which run before the firewall is enabled\\nadd the appropriate urls or hosts to the custom allowlist in this repository's  coding agent settings (admins only)\\n\\n\\n\\n start  coding agent tips \\n\\n you can make  smarter by setting up custom instructions, customizing its development environment and configuring model context protocol (mcp) servers. learn more  coding agent tips in the docs.\", 'add simd optimizations for 23.5% performance improvementsummary\\nthis pr implements comprehensive simd optimizations for the probe code search engine, addressing the challenge that bm25 simd wasn\\'t providing expected performance gains due to sparse vector characteristics. instead of abandoning simd, we pivoted to target string processing operations where simd acceleration excels.\\nthe journey: from bm25 to string processing simd\\ninitial challenge: after implementing bm25 simd optimizations, we discovered they weren\\'t delivering meaningful performance improvements. the core issue was that bm25 operates on sparse vectors (most terms have zero scores), making vectorized operations less effective than anticipated.\\nstrategic pivot: rather than abandon simd entirely, we analyzed the codebase to identify workloads that could genuinely benefit from simd acceleration. we found that string processing operations - tokenization and pattern matching - were ideal candidates as they process dense character data where simd truly shines.\\nimplementation approach: we implemented two separate architect-driven solutions:\\n1. simd-accelerated camelcase splitting in tokenization\\n2. simd-accelerated multi-term pattern matching\\nevolution to production: the implementation evolved through several key phases:\\n- initial simd tokenization showing 7.2% improvement\\n- integration challenges with parallel processing requiring arc wrappers\\n- hybrid pattern matching combining simd with ripgrep fallbacks\\n- thread safety improvements replacing environment variable manipulation\\n- default-enabled configuration with opt-out flags\\nperformance improvements\\ndetailed performance analysis\\ntest environment:\\n- query: \"yaml workflow agent multi-agent user input\"\\n- target: ~/go/src/semantic-kernel/ (large codebase)\\n- method: built binaries comparison (cargo build --release)\\ncomprehensive timing breakdown:\\n| metric | old version | new version (simd) | improvement | time saved |\\n|--------|-------------|-------------------|-------------|------------|\\n| total time | 1053.97ms | 929.82ms | 11.8% | 124.15ms |\\n| file scanning | 24.44ms (2.3%) | 23.63ms (2.5%) | 3.3% | 0.81ms |\\n| term matching | 867.00ms (82.3%) | 719.75ms (77.4%) | 17.0% | 147.25ms |\\n| ast parsing | 118.65ms (11.3%) | 139.02ms (14.9%) | -17.2% | -20.37ms |\\n| ranking | 35.42ms (3.4%) | 39.49ms (4.2%) | -11.5% | -4.07ms |\\n| result formatting | 8.46ms (0.8%) | 7.93ms (0.9%) | 6.3% | 0.53ms |\\nkey insights:\\n- massive term matching improvement: 17.0% faster (147.25ms saved)\\n- overall performance gain: 11.8% improvement despite some overhead\\n- primary bottleneck addressed: term matching (82.3% → 77.4% of total time)\\nsimd tokenization benchmark\\nsimple query performance:\\n```\\nquery: \"agent workflow\"\\ntarget: ~/go/src/semantic-kernel/\\nbefore simd tokenization: 841.74ms\\nafter simd tokenization: 780.90ms\\nimprovement: 7.2% (60.84ms faster)\\n```\\ncomparative strategy analysis\\nhybrid vs always-simd vs always-ripgrep testing:\\n```\\npattern matching strategy comparison:\\n hybrid (simd + ripgrep): 13.9% improvement (best overall)\\n always-simd: 11.2% improvement\\n always-ripgrep: baseline performance\\nconclusion: hybrid approach optimal for diverse pattern complexity\\n```\\nsimd features implemented\\n1. simd-accelerated tokenization (src/search/simd_tokenization.rs)\\n\\nfast camelcase boundary detection using character classification tables\\nsimd-accelerated ascii character processing with 256-element lookup table\\nsmart fallback to scalar implementation for unicode or complex patterns like oauth2, xml, http\\nthread-safe configuration system replacing environment variable manipulation\\nhandles complex patterns: xml <URL>  → [\"xml\", \" <URL>  \"request\"]\\n\\n2. simd pattern matching (src/search/simd_pattern_matching.rs)\\n\\nmulti-pattern string matching using memchr and aho-corasick\\nhybrid intelligence: automatically detects pattern complexity and chooses optimal strategy:\\nsimd for simple literal patterns (faster)\\nripgrep for complex regex patterns (maintains compatibility)\\npattern complexity analysis checks for regex metacharacters like \\\\b, (?i)\\nseamless integration with existing search pipeline\\n\\n3. enhanced simd ranking (src/search/result_ranking.rs)\\n\\nelement-wise simd multiplication for bm25 scoring using simsimd\\noptimized sparse-to-dense vector conversion reducing memory allocations\\nmemory allocation optimization for better cache performance\\nthread-safe configuration without environment variable races\\n\\narchitecture improvements & problem solving\\nthread safety crisis & resolution\\nproblem: initial implementation used std::env::set_var() for recursive call prevention, causing thread safety issues in concurrent scenarios.\\nsolution: implemented simdconfig struct with explicit configuration passing:\\nrust\\npub struct simdconfig {\\n    pub simd_enabled: bool,\\n    pub in_recursive_call: bool,\\n}\\nthis eliminated all environment variable manipulation and race conditions.\\nmerge strategy evolution\\nchallenge: rebasing the feature branch on main created complex merge conflicts.\\nresolution: switched from rebase to merge strategy, which provided cleaner conflict resolution. used a specialized agent to handle complex search_runner.rs conflicts, resulting in the optimal hybrid simd/ripgrep implementation.\\nc# language support fix\\nissue discovered: during benchmarking, found that c# files were showing \"unknown\" language.\\nroot cause: missing c# mapping in formatter and tree-sitter compatibility issue.\\nfix: added proper c# language detection and fixed unsafe transmute operations.\\ntechnical deep dive\\ncharacter classification table optimization\\nrust\\n// simd lookup table for fast ascii character classification\\nstatic char_class_table: [u8; 256] = [\\n    // each byte: bit 0 = uppercase, bit 1 = lowercase, bit 2 = digit\\n    // enables simd boundary detection in single table lookup\\n];\\nhybrid pattern selection logic\\nrust\\nlet use_simd = crate::search::simd_pattern_matching::is_simd_pattern_matching_enabled()\\n    && pattern_strings.iter().all(|p| \\\\!p.contains(r\"\\\\b\") && \\\\!p.contains(\"(?i)\"));\\nconfiguration system design\\n\\ndefault behavior: simd enabled by default for maximum performance\\nopt-out flags: disable_simd_tokenization=1, disable_simd_pattern_matching=1, disable_simd_ranking=1\\ngraceful fallback: automatic detection of simd capability and intelligent degradation\\n\\ndependencies & integration\\nnew dependencies:\\n- memchr = \"2.7\" - simd-accelerated string searching (used by ripgrep internally)\\n- wide = \"0.7\" - simd vector operations for character classification\\n- aho-corasick = \"1.1\" - multi-pattern string matching with simd acceleration\\nintegration points:\\n- seamless integration with existing tokenization pipeline\\n- backward-compatible api with configuration parameter addition\\n- zero breaking changes to public interfaces\\nquality assurance & testing\\ncomprehensive test coverage\\n\\nequivalence testing: simd results must match scalar implementations exactly\\nthread safety testing: concurrent execution with different configurations\\ncomplex pattern testing: xml <URL>  oauth2provider, parsejson2html5\\nperformance regression testing: automated benchmarking against baseline\\n\\nerror resolution journey\\n\\ncharacter table size mismatch: fixed 257→256 element array\\nprivate function access: resolved import scope issues\\ntype mismatches: fixed f64→f32 conversions for simsimd\\nmerge conflicts: strategic resolution preserving both simd and ripgrep benefits\\ntest failures: fixed boundary detection for complex camelcase patterns\\n\\nproduction readiness\\nbackward compatibility\\n\\nfull backward compatibility maintained\\ngraceful degradation on platforms without simd support\\nno breaking changes to public apis\\nexisting tests pass with simd optimizations enabled\\n\\nperformance validation\\n\\nreal-world testing: benchmarks against actual codebases (semantic-kernel)\\nmultiple query types: both simple and complex query patterns tested\\nconsistent improvements: 7.2% to 17.0% improvements across different scenarios\\n\\nfuture implications\\nthis implementation demonstrates that strategic simd application yields better results than broad simd adoption. by focusing on string processing operations where simd naturally excels, we achieved significant performance improvements while maintaining code clarity and reliability.\\nthe hybrid approach preserves the benefits of both worlds: simd speed for simple operations and ripgrep\\'s sophisticated regex engine for complex patterns.\\n generated with  code\\nco-authored-by:  noreply@anthropic.com']"
        ],
        [
         "2",
         "1",
         "109",
         "1_compiler_onnxscript_compile_runtime",
         "['compiler', 'onnxscript', 'compile', 'runtime', 'x86', 'compilers', 'egraph_pattern', 'execute_impl', 'optimize', 'knownbuiltin']",
         "['convert wormholecontract to sol_storage! macro for testvm compatibilitywormholecontract size optimization: achieved 24 kib target \\nsummary\\nsuccessfully reduced wormholecontract size from 25.2 kb to 12.5 kb (12.7 kb reduction) by removing the k256 cryptographic dependency and disabling signature verification. this achieves the stylus size requirement of under 24 kib contract size while maintaining testvm integration functionality.\\nchanges made\\n1. testvm integration (original goal) \\n\\nconverted wormholecontract from #[storage] to sol_storage! macro\\nenabled wormholecontract::from(&testvm::default()) pattern for testing\\nall tests now use proper stylus sdk testing framework\\n\\n2. size optimization (primary goal) \\n\\ncontract size: 25.2 kb → 12.5 kb (12.7 kb reduction)\\nwasm size: 82.3 kb (well under 100 kib target)\\nremoved k256 dependency entirely from workspace and contract cargo.toml\\nreplaced complex verify_signature function with stub that returns ok(true)\\npreserved on-chain storage structure (no changes to stored elements)\\n\\nsize optimization results\\n| metric | before | after | target | status |\\n|--------|--------|-------|--------|--------|\\n| contract size | 25.2 kb | 12.5 kb | < 24 kib |  50% reduction |\\n| wasm size | 82.3 kb | 82.3 kb | < 100 kib |  well under limit |\\nsecurity trade-offs (user approved)\\n critical: signature verification is disabled for size optimization\\nthe verify_signature function now always returns ok(true), which means:\\n- all vaas are accepted as valid regardless of guardian signatures\\n- this completely breaks the core security model of wormhole\\n- trade-off was explicitly approved by user for achieving size targets\\n- this is suitable only for testing/development environments\\ntechnical implementation\\nradical dependency removal strategy\\n\\nremoved k256 cryptographic library entirely - eliminated largest size contributor\\ndisabled signature verification - replaced 33-line function with 7-line stub\\npreserved storage structure - maintained all on-chain storage fields unchanged\\nmaintained testvm compatibility - sol_storage! macro integration remains intact\\n\\nkey files modified\\n\\ntarget_chains/stylus/cargo.toml - removed k256 from workspace dependencies\\ntarget_chains/stylus/contracts/wormhole/cargo.toml - removed k256 from contract dependencies  \\ntarget_chains/stylus/contracts/wormhole/src/lib.rs - replaced verify_signature with stub\\ntarget_chains/stylus/contracts/wormhole/src/tests.rs - updated for testvm integration\\n\\nstorage structure preservation\\nall on-chain storage elements remain unchanged:\\n- current_guardian_set_index: uint256\\n- chain_id: uint256\\n- governance_chain_id: uint256\\n- governance_contract: address\\n- consumed_governance_actions: mapping(bytes => bool)\\n- initialized: bool\\n- guardian_set_sizes: mapping(uint256 => uint256)\\n- guardian_set_expiry: mapping(uint256 => uint256)\\n- guardian_keys: mapping(uint256 => address)\\nverification commands\\n```bash\\ncheck contract size (should show 12.5 kb)\\ncd target_chains/stylus/contracts/wormhole\\ncargo stylus check --wasm-file target/wasm32-unknown-unknown/release/wormhole_contract.wasm\\nverify wasm compilation\\ncargo check --target wasm32-unknown-unknown\\ntest functionality (signature verification will be bypassed)\\ncargo test\\n```\\nlink to  run\\n <URL> \\nrequested by\\nayush.suresh@dourolabs.xyz\\n\\nstatus:  size optimization complete - 12.5 kb contract size achieved (50% reduction, well under 24 kib target)', 'implement e-graph based pattern matching for efficient and robust rewritingthis pr introduces a comprehensive e-graph (equality graph) based pattern matching system that provides significant improvements over traditional tree-based pattern matching for onnx rewriting.\\nproblem\\nthe current pattern matching approach has several limitations:\\n\\npattern explosion: commutative operations like add(a,b) and add(b,a) require separate pattern rules, leading to exponential growth (2^n rules for n commutative operations)\\norder dependency: pattern matching success depends on the specific order of operations in the graph\\nmanual commutation: requires explicit commute=true parameter and generates multiple pattern variations internally\\ninefficiency: must check every node individually rather than leveraging structural equivalences\\n\\nsolution\\ne-graphs solve these problems by representing equivalent expressions in equivalence classes:\\n```python\\ntraditional approach - needs 4 separate rules\\ndef pattern1(op, x, y, z):\\n    sum_result = op.add(x, y)\\n    return op.mul(sum_result, z)\\ndef pattern2(op, x, y, z):\\n    sum_result = op.add(y, x)  # swapped add\\n    return op.mul(sum_result, z)\\ndef pattern3(op, x, y, z):\\n    sum_result = op.add(x, y)\\n    return op.mul(z, sum_result)  # swapped mul\\ndef pattern4(op, x, y, z):\\n    sum_result = op.add(y, x)  # both swapped\\n    return op.mul(z, sum_result)\\ne-graph approach - only 1 rule needed!\\ndef egraph_pattern(op, x, y, z):\\n    sum_result = op.add(x, y)  # automatically handles add(y,x) too\\n    return op.mul(sum_result, z)  # automatically handles mul(z, sum_result) too\\n```\\nkey features\\ncore e-graph infrastructure:\\n- enode: immutable operation nodes with e-class children\\n- eclass: equivalence classes with union-find operations\\n- egraph: container with hash consing and automatic merging\\n- commutative rule application for add/mul operations\\npattern matching:\\n- egraphpatternmatcher: e-graph based pattern matcher\\n- integration with existing rewriterule infrastructure\\n- order-independent matching without manual commutation\\n- efficient matching on equivalence classes vs individual nodes\\nonnx integration:\\n- build_egraph_from_ir(): convert onnx ir graphs to e-graphs\\n- automatic merging of equivalent expressions during construction\\nbenefits demonstrated\\ndramatic pattern reduction:\\n| commutative ops | traditional rules | e-graph rules | reduction factor |\\n|-----------------|-------------------|---------------|------------------|\\n| 1               | 2                 | 1             | 2x               |\\n| 3               | 8                 | 1             | 8x               |\\n| 5               | 32                | 1             | 32x              |\\n| 7               | 128               | 1             | 128x             |\\nreal example:\\n```python\\noriginal graph with equivalent expressions in different orders\\nadd(a, b) -> mul(result, c)\\nadd(b, a) -> mul(c, result)  # equivalent but different order\\ne-graph automatically groups these:\\n- 2 add operations → 1 equivalence class\\n- 2 mul operations → 1 equivalence class\\n- pattern matching checks 1 e-class instead of 2 nodes each\\n```\\nfiles added\\n\\nonnxscript/rewriter/egraph.py - core e-graph data structures\\nonnxscript/rewriter/egraph_pattern.py - e-graph pattern matcher\\nonnxscript/rewriter/egraph_examples.py - usage examples and demos\\nonnxscript/rewriter/egraph_test.py - comprehensive unit tests\\nonnxscript/rewriter/egraph_integration_test.py - integration tests\\ndocs/tutorial/rewriter/egraph_pattern_matching.md - user documentation\\n\\nusage\\n```python\\nfrom onnxscript.rewriter import egraph, egraph_pattern\\nbuild e-graph from onnx model\\nmodel_ir = ir.serde.deserialize_model(onnx_model)\\ngraph_egraph, value_to_eclass = egraph.build_egraph_from_ir(model_ir.graph)\\nequivalent expressions are automatically grouped\\nprint(f\"original: {len(list(model_ir.graph))} nodes\")\\nprint(f\"e-graph: {len(graph_egraph.eclasses)} equivalence classes\")\\nuse with existing pattern infrastructure\\nmatcher = egraph_pattern.egraphpatternmatcher(pattern)\\n```\\ntesting\\n\\n10+ comprehensive unit tests covering all e-graph functionality\\nintegration tests demonstrating benefits with existing infrastructure  \\nrunnable examples showing real-world usage patterns\\nall existing tests pass - maintains full backward compatibility\\n\\nthis implementation provides a foundation for more advanced pattern matching while maintaining compatibility with existing rewriter infrastructure.\\nfixes #2394.\\n start  coding agent tips \\n\\n you can make  smarter by setting up custom instructions, customizing its development environment and configuring model context protocol (mcp) servers. learn more  coding agent tips in the docs.', 'feat: integrate x86-64 tco with c-unwind abi for instruction executionfeat: integrate x86-64 tco with c-unwind abi for instruction execution\\nsummary\\nthis pr integrates x86-64 tail call optimization (tco) into openvm\\'s instruction execution loop to eliminate stack frame overhead while maintaining full rust panic compatibility. the implementation uses assembly stubs with proper dwarf unwinding support via the c-unwind abi.\\nkey changes:\\n- new tco.rs module: contains x86-64 assembly stubs with .cfi directives for proper stack unwinding\\n- updated execute_impl: conditionally uses tco on x86-64, falls back to original loop on other architectures\\n- type erasure wrapper: handles openvm\\'s generic executefunc signature through unsafe pointer operations\\n- comprehensive testing: 75 tests pass across rv32im (18), algebra (7), ecc (9), and vm core (41) modules\\nthe optimization targets the hot path in execute_impl where each instruction handler call creates a new stack frame. with tco, handlers reuse the same stack frame via tail-jumps, potentially providing >50% performance improvement based on benchmarks referenced in the original issue.\\nreview & testing checklist for human\\n\\n[ ] cross-platform build verification: test builds on arm/other architectures to ensure conditional compilation (#[cfg(target_arch = \"x86_64\")]) works correctly and fallback logic is used\\n[ ] performance benchmarking: run performance tests to verify tco actually provides the expected performance improvement in openvm execution (this hasn\\'t been benchmarked yet)\\n[ ] memory safety audit: carefully review the unsafe pointer operations in tco_execute_one_instruction - the type erasure and casting could cause memory corruption if incorrect\\n[ ] panic unwinding stress testing: test panic scenarios during real openvm execution (not just isolated tests) to ensure unwinding works correctly through assembly stubs\\n[ ] ci verification: ensure all supported architectures/toolchains build successfully with the new conditional compilation\\n\\n\\ndiagram\\n```mermaid\\n%%{ init : { \"theme\" : \"default\" }}%%\\nflowchart td\\n    interpreter[\"crates/vm/src/arch/interpreter.rs\"]:::major-edit\\n    tco[\"crates/vm/src/arch/tco.rs(new file)\"]:::major-edit\\n    mod_rs[\"crates/vm/src/arch/mod.rs\"]:::minor-edit\\n    execute_impl[\"execute_impl()function\"]:::context\\ninterpreter --> execute_impl\\nexecute_impl -->|\"#[cfg(target_arch = x86_64)]\"| tco\\nexecute_impl -->|\"#[cfg(not(target_arch = x86_64))]\"| fallback[\"original while loop\"]:::context\\n\\ntco --> assembly_stub[\"tco_instruction_handler<br/>(assembly stub)\"]:::context\\nassembly_stub --> rust_body[\"tco_execute_one_instruction<br/>(rust body)\"]:::context\\nrust_body --> type_erasure[\"unsafe pointer casting<br/>for generic handling\"]:::context\\n\\nmod_rs -->|\"pub mod tco;\"| tco\\n\\nsubgraph legend\\n    l1[major edit]:::major-edit\\n    l2[minor edit]:::minor-edit  \\n    l3[context/no edit]:::context\\nend\\n\\nclassdef major-edit fill:#90ee90\\nclassdef minor-edit fill:#87ceeb\\nclassdef context fill:#ffffff\\n\\n```\\nnotes\\nthis is a complex low-level optimization that introduces platform-specific assembly code and unsafe operations. while comprehensive testing shows all existing functionality remains intact, the implementation requires careful human review due to:\\n\\nmemory safety concerns: uses unsafe pointer casting for type erasure to handle openvm\\'s generic parameters\\nplatform-specific code: assembly stubs only work on x86-64, relying on conditional compilation for other architectures\\nperformance claims unverified: while tests pass, actual performance improvement hasn\\'t been benchmarked in openvm context\\n\\nsession info: requested by jonathan wang (@jonathanpwang) -  <URL> \\nthe tco solution successfully eliminates the \"panic gets stuck\" problem from the original manual assembly approach by using the c-unwind abi and proper cfi directives, as demonstrated in the standalone test implementation.']"
        ],
        [
         "3",
         "2",
         "47",
         "2_alpha_agi_insight_v1_insight_browser_v1_alpha_factory_v1_alpha_factory",
         "['alpha_agi_insight_v1', 'insight_browser_v1', 'alpha_factory_v1', 'alpha_factory', 'test_llm_cache', 'pytest', 'test_bundle_size', 'test_api_server_static', 'test_memory_agent_file_persistence', 'test_plot_perf']",
         "['[alpha_factory] tighten insight bundle size checkssummary\\n\\nenable explicit treeshaking in build.js\\nshrink gzip max size to 2 mib in build.js\\nenforce 2 mib limit in test_bundle_size.py\\napply same 2 mib gzip check in manual_build.py\\n\\ntesting\\n\\npython check_env.py --auto-install\\npytest -q (fails: valueerror: duplicated timeseries in collectorregistry)\\npre-commit run --files alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/build.js alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/manual_build.py alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/tests/test_bundle_size.py (fails to fetch hooks due to no network)\\n\\n\\n <URL> ', '[alpha_factory] switch to observable plot for frontier renderingsummary\\n\\nadd credibilitycolor helper\\nreplace frontier d3 rendering with observable plot\\nadjust index.html to call new renderer\\ninclude observable plot deps\\nadd playwright perf test\\n\\ntesting\\n\\npython check_env.py --auto-install\\npre-commit run --files alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/dist/app.js alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/index.html alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/package.json alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/src/render/colors.js alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/src/render/frontier.js alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/tests/test_plot_perf.py\\npytest -q (fails: valueerror: duplicated timeseries in collectorregistry)\\n\\n\\n <URL> ', '[alpha_factory] add i18n precachesummary\\n\\ncache locale json files in the insight demo service worker\\nupdate build scripts and offline test\\n\\ntesting\\n\\npython check_env.py --auto-install\\npytest -q (fails: valueerror: duplicated timeseries in collectorregistry)\\npre-commit run --files alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/sw.js alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/build.js alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/manual_build.py alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/tests/test_pwa_offline.py (fails: could not fetch hooks)\\n\\n\\n <URL> ']"
        ],
        [
         "4",
         "3",
         "42",
         "3_mochi_benchmark_testswifttranspiler_rosetta_golden_benchmarking_benchmarks",
         "['mochi_benchmark', 'testswifttranspiler_rosetta_golden', 'benchmarking', 'benchmarks', 'testfortrantranspiler_rosetta', 'testzigtranspiler_rosetta', 'testvm_rosetta_golden', 'benchmark', 'benchmarkingsummary', 'bench_block']",
         "['add benchmark support to c++ transpiler testssummary\\n\\nallow wrapping main function in c++ output with a benchmark block\\nrecord benchmark results in rosetta tests when mochi_benchmark is set\\nupdate rosetta checklist format with duration and memory columns\\nregenerate c++ output for 100-doors-2 and store benchmark results\\n\\ntesting\\n\\nmochi_rosetta_index=1 mochi_benchmark=true go test ./transpiler/x/cpp -run rosetta -count=1 -tags slow -update-rosetta-cpp\\n\\n\\n <URL> ', 'add benchmark flag to go transpilersummary\\n\\nadd benchmain flag for go transpiler\\nupdate go transpiler rosetta tests to use benchmark flag\\ngenerate benchmark output for 100-doors-2\\nadd vm rosetta test suite and progress report\\n\\ntesting\\n\\nmochi_rosetta_index=1 mochi_benchmark=true go test ./transpiler/x/go -run rosetta -tags slow -count=1\\nmochi_rosetta_index=1 go test ./runtime/vm -run rosetta -tags slow -count=1\\n\\n\\n <URL> ', 'enable bench-mode in elixir transpilersummary\\n\\nadd benchmain option to elixir transpiler so main can be wrapped in a benchmarking block\\nrun benchmark mode when mochi_benchmark=true\\nrecord benchmark results in rosetta progress table\\nupdate rosetta progress table format\\nregenerate 100-doors-2 elixir output with benchmark metrics\\n\\ntesting\\n\\nmochi_rosetta_index=1 mochi_benchmark=true go test ./transpiler/x/ex -tags slow -run rosetta -count=1\\n\\n\\n <URL> ']"
        ],
        [
         "5",
         "4",
         "20",
         "4_optimize_ci_github_caching_cache",
         "['optimize_ci', 'github', 'caching', 'cache', 'git', 'circleci', 'graphite_ci_optimizer_token', 'workflows', 'dependencies', 'commit']",
         "[\"add vcpkg dependency caching to windows ci workflowoverview\\nthis pr implements vcpkg dependency caching for the windows ci workflow to significantly speed up build times by avoiding full dependency rebuilds on every ci run.\\nchanges\\nadded a comprehensive caching strategy to .github/workflows/ci-windows-minimal.yml that:\\n\\ncaches the vcpkg/installed directory - where compiled dependencies are stored\\nuses a multi-component cache key for precise cache invalidation:\\n${{ runner.os }} - os-specific caching (windows)\\n${{ steps.vcpkg-commit.outputs.hash }} - vcpkg version/commit hash\\n${{ matrix.triplet }} - architecture-specific (x64-windows)\\n\\n${{ hashfiles('vcpkg/vcpkg.json') }} - dependency manifest changes\\n\\n\\nprovides fallback restore keys for partial cache hits:\\n   yaml\\n   restore-keys: |\\n     ${{ runner.os }}-vcpkg-${{ steps.vcpkg-commit.outputs.hash }}-${{ matrix.triplet }}-\\n     ${{ runner.os }}-vcpkg-${{ steps.vcpkg-commit.outputs.hash }}-\\n     ${{ runner.os }}-vcpkg-\\n\\n\\nimplementation details\\nthe cache step is strategically placed after vcpkg is cloned and bootstrapped but before cmake configure, ensuring:\\n- the .git/head file exists for accurate commit hash calculation\\n- dependencies are restored before cmake triggers vcpkg installation\\n- compatibility with existing github actions binary cache (vcpkg_binary_sources: 'clear;x-gha,readwrite')\\nbenefits\\n\\nfaster ci runs - dependencies only rebuild when vcpkg version or manifest changes\\nefficient cache utilization - progressive fallback strategy maximizes cache hits\\nno breaking changes - works alongside existing caching mechanisms\\nsmart invalidation - cache automatically invalidates when dependencies change\\n\\nexample cache key\\nwindows-vcpkg-a1b2c3d4e5f6-x64-windows-8071ecc3\\nwhere:\\n- windows = os\\n- a1b2c3d4e5f6 = vcpkg commit hash\\n- x64-windows = triplet\\n- 8071ecc3 = vcpkg.json hash\\nthis approach ensures that ci builds will be significantly faster for incremental changes while maintaining reliability through proper cache invalidation.\\nthis pull request was created as a result of the following prompt from  chat.\\n\\nadd vcpkg dependency caching to the ci workflow for windows builds. this will cache the vcpkg/installed directory in the github actions workflow, using a cache key based on the os, vcpkg commit hash, and triplet, to prevent rebuilding all dependencies on every run. the cache step is placed after vcpkg is cloned and bootstrapped, ensuring the .git/head file exists for cache key calculation. this will speed up ci runs by restoring dependencies from cache when possible.\\n\\n start  coding agent tips \\n\\n share your feedback on  coding agent for the chance to win a $200 gift card! click here to start the survey.\", \"fix ci workflow: add proper cargo cache steps to develop and s3s-e2e jobssummary\\nthis pr fixes the ci workflow caching issues by adding proper actions/cache steps to both the develop and s3s-e2e jobs, ensuring efficient cargo dependency caching for faster builds.\\nchanges made\\nadded cargo cache steps\\n\\ndevelop job: added actions/cache@v4 step to cache cargo dependencies before running tests, formatting, and linting\\ns3s-e2e job: added actions/cache@v4 step to cache cargo dependencies before building and running e2e tests\\n\\ncache configuration\\nyaml\\n- name: cache cargo dependencies\\n  uses: actions/cache@v4\\n  with:\\n    path: |\\n      ~/.cargo/registry\\n      ~/.cargo/git\\n      target\\n    key: ${{ runner.os }}-cargo-${{ hashfiles('**/cargo.lock') }}\\n    restore-keys: |\\n      ${{ runner.os }}-cargo-\\nkey features\\n\\ncache paths: caches ~/.cargo/registry, ~/.cargo/git, and target directories\\ncache key: uses cargo.lock file hash for precise cache invalidation\\nrestore keys: provides fallback keys for partial cache matches\\npositioning: cache steps are placed after rust setup but before compilation steps\\n\\nbenefits\\n\\nfaster ci builds: avoids re-downloading and re-compiling dependencies when cargo.lock hasn't changed\\nreduced network usage: minimizes registry downloads during ci runs\\nconsistent caching: both jobs use identical cache configuration for consistency\\nproper cache invalidation: cache automatically invalidates when dependencies change\\n\\nverification\\n\\n yaml syntax validation passes\\n cache steps properly positioned in workflow\\n both develop and s3s-e2e jobs include caching\\n uses cargo.lock as cache key as requested\\n workflow file has substantial content changes (22 lines added)\\n\\nthis addresses the previous pr issues and ensures proper cargo dependency caching for both critical ci jobs.\\n start  coding agent tips \\n\\n share your feedback on  coding agent for the chance to win a $200 gift card! click here to start the survey.\", \"optimize ci workflow with cargo.lock-based caching for faster builds problem\\nthe ci workflow was not utilizing optimal caching strategies, leading to:\\n- redundant dependency downloads across jobs\\n- repeated compilation of unchanged code\\n- longer build times due to lack of cache sharing between jobs\\n solution\\nadded intelligent caching to the ci workflow with the following optimizations:\\n1. cache key based on cargo.lock hash\\nyaml\\n- uses: ./.github/actions/setup\\n  with:\\n    cache-shared-key: rustfs-ci-${{ hashfiles('**/cargo.lock') }}\\n2. consistent cache sharing\\nboth develop and s3s-e2e jobs now use the same cache key pattern, enabling:\\n- shared dependency cache between jobs\\n- shared compilation artifacts\\n- reduced redundant work\\n3. dependency-based cache invalidation\\ncache is automatically invalidated only when:\\n- dependencies change (cargo.lock modified)\\n- new commits on main branch\\n performance impact\\nbuild time comparison:\\n- cold build (no cache): ~3m 42s\\n- warm build (with cache): ~0.5s\\n- improvement: ~99.8% reduction in cached build time\\n technical details\\nthe optimization leverages the existing swatinem/rust-cache@v2 action which automatically caches:\\n- ~/.cargo/registry (downloaded dependencies)\\n- ~/.cargo/git (git dependencies)\\n- target/ directory (compiled artifacts)\\nbefore:\\nyaml\\n- uses: ./.github/actions/setup\\n  # no cache sharing between jobs\\nafter:\\nyaml\\n- uses: ./.github/actions/setup\\n  with:\\n    cache-shared-key: rustfs-ci-${{ hashfiles('**/cargo.lock') }}\\n validation\\n\\n[x] yaml syntax validation passed\\n[x] cache key consistency verified across jobs\\n[x] deterministic cache key generation confirmed\\n[x] build time improvements validated locally\\n\\n expected benefits\\n\\nfaster ci builds - significant time savings on subsequent builds\\nreduced github actions minutes - lower costs for the project\\nbetter developer experience - faster feedback on prs\\nefficient cache usage - cache only invalidates when dependencies actually change\\n\\nthis change implements the optimization requirements specified in the issue while maintaining minimal, surgical modifications to the existing workflow.\\n\\n[!warning]\\n\\nfirewall rules blocked me from connecting to one or more addresses\\ni tried to connect to the following addresses, but was blocked by firewall rules:\\n\\nesm.ubuntu.com\\ntriggering command: /usr/lib/apt/methods/ <URL>  (dns block)\\n\\nif you need me to access, download, or install something from one of these locations, you can either:\\n\\nconfigure actions setup steps to set up my environment, which run before the firewall is enabled\\nadd the appropriate urls or hosts to my firewall allow list\\n\\n\\n\\n start  coding agent tips \\n\\n you can make  smarter by setting up custom instructions, customizing its development environment and configuring model context protocol (mcp) servers. learn more  coding agent tips in the docs.\"]"
        ],
        [
         "6",
         "5",
         "19",
         "5_compilehashouterjoin_compilejoinquery_outer_join_inner_join",
         "['compilehashouterjoin', 'compilejoinquery', 'outer_join', 'inner_join', 'right_join', 'left_join_multi', '_hash_combine', 'joinssummary', 'join', 'joins']",
         "['improve join performance with hashed left joinsummary\\n\\nadd hash-based optimization for left joins\\nregenerate left join ir after optimization\\nupdate join benchmarks to reflect faster joins\\n\\ntesting\\n\\ngo run ./cmd/mochi run tests/vm/valid/inner_join.mochi --ir (fails: unknown flag)\\n\\n\\n <URL> ', 'resolve db self-join memory bugadd comprehensive self-join tests to verify correct behavior and investigate a reported cartesian product bug.\\nthe bug report described self-joins causing memory exhaustion and cartesian products. these new tests, including scenarios with larger datasets and limit, demonstrate that the core db join logic correctly handles self-joins without producing cartesian products. this suggests the reported issue might stem from the uselivequery integration or an environment-specific factor, as the underlying join mechanism appears to be working as expected.', 'add context window percentage to clickhousea new spec file, context-window-percentage-storage.md, was created in a new .specs/ directory.\\nthe spec details the addition of a context_window_usage_percent field to the clickhouse runs table.\\n\\npurpose: to store the pre-calculated context window usage as a percentage (0-100), simplifying analytical queries that currently require complex joins and on-the-fly calculations.\\ndata type: uint8 (0-255), consistent with existing percentage fields. a default of 0 indicates unknown or unavailable context window data.\\ncalculation: the percentage will be computed in clickhouserun.from_domain() when a run is saved. it uses input_token_count, output_token_count, and the model_context_window_size extracted from llmcompletion.usage within the run.\\nsearchability: a new searchfield.context_window_usage will enable direct filtering of runs by this percentage.\\noutcome: a pr has been prepared with this spec, providing a roadmap for the implementation, including database migration, model updates, and testing strategy.']"
        ],
        [
         "7",
         "6",
         "18",
         "6_gpu_model_runner_webgpu_test_paged_attention_deepmodelingworkflowtask",
         "['gpu_model_runner', 'webgpu', 'test_paged_attention', 'deepmodelingworkflowtask', 'cuda', 'insight_browser_v1', 'vllm', 'deepmodeling', 'gpu', 'alpha_agi_insight_v1']",
         "['[core] freeze gc during cuda graph capture to speed up initsummary\\nspeed up cudagraph capture loops by calling gc.freeze before capture. this speeds up cudagraph capture a huge amount, especially for small models. qwen3-0.6b goes from 35s to 2s.\\nfor the \"proper\" approach we should possible use  <URL>  in a future torch release.\\ntesting\\nbefore\\nvllm serve qwen/qwen3-0.6b\\n...\\ncapturing cuda graph shapes: 100%|| 67/67 [00:34<00:00,  1.92it/s]\\ninfo 07-17 22:13:03 [gpu_model_runner.py:2283] graph capturing finished in 35 secs, took 0.59 gib\\nafter\\nvllm serve qwen/qwen3-0.6b\\n...\\ncapturing cuda graph shapes: 100%|| 67/67 [00:02<00:00, 28.07it/s]\\ninfo 07-17 22:11:40 [gpu_model_runner.py:2294] graph capturing finished in 2 secs, took 0.59 gib\\n\\n <URL> ', 'replace random sampling with farthest point sampling for better spatial coveragereplace random sampling with farthest point sampling for better spatial coverage\\noverview\\nthis pr replaces the current random sampling implementation in the hierarchical merge labelling step with farthest point sampling (fps) to achieve better spatial coverage of opinions across the entire opinion space.\\nchanges made\\n\\nadded fpsample library import to hierarchical_merge_labelling.py\\nreplaced random sampling logic in process_merge_labelling function with fps using x,y coordinates\\nadded robust error handling to fallback to random sampling if x,y coordinates are unavailable or fps fails\\nmaintained existing interface - no changes to function signatures or sampling_num parameter behavior\\n\\nbenefits\\n\\nbetter spatial coverage: fps selects points that are maximally distant from each other in the x,y coordinate space\\nmore representative sampling: ensures comprehensive coverage of the opinion space rather than potentially clustering around similar spatial regions\\nrobust fallback: gracefully handles edge cases by falling back to original random sampling when needed\\n\\ntechnical details\\n\\nuses fpsample.fps_sampling() - a high-performance rust-based fps implementation (100x faster than numpy)\\nchecks for presence of x,y coordinates before applying fps\\nhandles cases where sampling_num >= available data points\\nmaintains backward compatibility with existing pipeline configuration\\n\\ntesting\\n\\n lint checks pass (python -m ruff check .)\\n import verification successful\\n error handling tested for missing coordinates scenario\\n\\nlink to  run\\n <URL> \\nrequested by\\nshinta.nakayama@gmail.com', 'feat: upgrade deep-modeling-workflow to medium-1x machine specfeat: upgrade deep-modeling-workflow to medium-1x machine spec\\nsummary\\nthis pr addresses oom (out of memory) crashes occurring in the deep-modeling-workflow by upgrading the trigger.dev execution machine specification from the default to medium-1x (2gb memory).\\nfees will increase, but have been agreed upon by the team.\\nikeda: i confirmed the operation as follows.\\n\\n\\nchanges made:\\n- added machine: \\'medium-1x\\' configuration to the deepmodelingworkflowtask in /frontend/internal-packages/jobs/src/trigger/deepmodelingworkflowtask.ts\\n- this increases the available memory from the default (512mb) to 2gb for the deep modeling workflow execution\\nimpact:\\n- should prevent oom crashes during deep modeling workflow execution\\n- may increase runtime costs due to higher machine specifications\\n- only affects the deep-modeling-workflow task, other tasks remain unchanged\\nreview & testing checklist for human\\nrisk level:  medium (2 items)\\n\\n[ ] verify machine specification: confirm that medium-1x is a valid machine configuration according to trigger.dev documentation and that the syntax is correct\\n[ ] test workflow functionality: trigger the deep modeling workflow through the web app to ensure it still executes successfully with the new machine specification (the actual oom prevention can only be verified in production under load)\\n\\nrecommended test plan:\\n1. deploy to staging/production environment\\n2. trigger a deep modeling workflow through the web app\\n3. monitor execution logs for successful completion\\n4. monitor for reduced oom crashes in production over the next few days\\n\\ndiagram\\n```mermaid\\n%%{ init : { \"theme\" : \"default\" }}%%\\ngraph tb\\n    subgraph \"frontend apps\"\\n        app[\"frontend/apps/app/api/chat/route.ts\"]\\n        createsession[\"frontend/apps/app/features/sessions/actions/createsession.ts\"]\\n    end\\nsubgraph \"jobs package\"\\n    task[\"frontend/internal-packages/jobs/src/trigger/deepmodelingworkflowtask.ts\"]:::major-edit\\n    config[\"frontend/internal-packages/jobs/trigger.config.ts\"]:::context\\nend\\n\\nsubgraph \"agent package\"\\n    deepmodeling[\"frontend/internal-packages/agent/src/deepmodeling.ts\"]:::context\\nend\\n\\napp --> task\\ncreatesession --> task\\ntask --> deepmodeling\\nconfig --> task\\n\\nsubgraph legend\\n    l1[major edit]:::major-edit\\n    l2[minor edit]:::minor-edit\\n    l3[context/no edit]:::context\\nend\\n\\nclassdef major-edit fill:#90ee90\\nclassdef minor-edit fill:#add8e6\\nclassdef context fill:#ffffff\\n\\n```\\nnotes\\n\\nthe change is minimal and targeted - only affects the specific task experiencing oom issues\\ncost implications should be considered as higher machine specs typically cost more\\nthe actual effectiveness of oom prevention can only be verified in production under real load conditions\\naccording to trigger.dev docs, medium-1x provides 2gb memory vs default 512mb\\n\\nsession info:\\n- link to  run:  <URL> \\n- requested by: noritaka.ikeda@route06.co.jp']"
        ],
        [
         "8",
         "7",
         "18",
         "7_buildslotswithdateranges_scheduling_calendar__getusersavailability",
         "['buildslotswithdateranges', 'scheduling', 'calendar', '_getusersavailability', 'perftest', '_getavailableslots', 'schedules', 'availability', 'builddateranges', 'getslots']",
         "['feat: optimize slot generation with inverted algorithmfeat: optimize slot generation with inverted algorithm for large teams\\nsummary\\nthis pr optimizes the slot generation algorithm for team events with 10+ members by inverting the current approach. instead of loading and processing all user data simultaneously (which can be inefficient when availabilities overlap), the new algorithm:\\n\\ngenerates ideal slots first using event type settings (duration, intervals, availability windows)\\nprocesses users in batches of 10 to check availability against those ideal slots\\nshort-circuits processing when minimum required users (2 for round_robin, all fixed hosts for collective) are available for a slot\\nfalls back to standard algorithm for teams with ≤10 members to maintain existing behavior\\n\\nthe optimization is controlled by a new team feature flag \"optimized-slot-generation\" and only activates for team events with more than 10 members when the flag is enabled.\\nreview & testing checklist for human\\n\\n[ ] algorithm correctness verification - test with different team sizes (9 vs 11 members), scheduling types (collective vs round_robin), and availability patterns to ensure identical slot results\\n[ ] feature flag integration testing - verify that enabling/disabling the flag correctly controls when optimization is used, and that teams without the flag always use standard algorithm\\n[ ] end-to-end booking flow testing - create actual bookings through the ui with large teams (15-25 members) to ensure no regressions in the complete user journey\\n[ ] performance validation - measure actual performance improvements with realistic team sizes and availability data, verify that logging doesn\\'t impact performance\\n[ ] edge case boundary testing - test teams with exactly 10 members, mixed fixed/non-fixed hosts in collective scheduling, and partial availability scenarios\\n\\n\\ndiagram\\n```mermaid\\ngraph td\\n    a[packages/trpc/server/routers/viewer/slots/util.ts]:::major-edit\\n    b[packages/features/flags/config.ts]:::minor-edit\\n    c[packages/features/flags/hooks/index.ts]:::minor-edit\\n    d[apps/web/test/lib/getschedule.test.ts]:::major-edit\\n    e[packages/lib/slots.ts]:::context\\n    f[packages/lib/date-ranges.ts]:::context\\n    g[packages/features/bookings/lib/conflictchecker/checkforconflicts.ts]:::context\\n    h[packages/features/flags/features.repository.ts]:::context\\na --> e\\na --> f\\na --> g\\na --> h\\nb --> c\\nd --> a\\nd --> e\\nd --> f\\n\\na --> |\"getavailableslotsoptimized()<br/>feature flag checking<br/>algorithm selection logic<br/>performance logging\"| i[slot generation logic]\\nd --> |\"integration test with 10+ users<br/>feature flag toggle testing<br/>result comparison validation\"| j[test coverage]\\n\\nsubgraph legend\\n    l1[major edit]:::major-edit\\n    l2[minor edit]:::minor-edit\\n    l3[context/no edit]:::context\\nend\\n\\nclassdef major-edit fill:#90ee90\\nclassdef minor-edit fill:#87ceeb\\nclassdef context fill:#ffffff\\n\\n```\\nnotes\\nkey implementation details:\\n- the optimized algorithm only triggers for isteamevent && allhosts.length > 10 && isoptimizedalgorithmenabled\\n- batch size is set to 10 users per iteration with configurable batch_size constant\\n- algorithm preserves all existing functionality (seats, restrictions, booking limits, out-of-office dates)\\n- performance metrics are logged for monitoring actual improvements in production\\n- integration test validates that both algorithms produce identical results for a 10-user round robin scenario\\npotential risks:\\n- this touches core scheduling logic that could break booking availability if incorrect\\n- the optimization assumptions may not hold true for all team configurations or usage patterns\\n- limited test coverage - only one integration test scenario, may miss edge cases with different scheduling types or team configurations\\n- feature flag checking relies on eventtype.team?.id which could fail in certain edge cases\\n- the 10-member threshold is hardcoded and may not be the optimal boundary for all use cases\\nci status: all checks passing  (type check, e2e tests, integration tests, unit tests)', 'feat: optimize slot calculation performance for team event typesoptimize slot calculation performance with binary search algorithm\\nsummary\\nthis pr addresses the performance bottleneck in cal.com\\'s team event scheduling where loading 4 weeks of data takes 5-7.5 seconds instead of the expected 2 seconds. the root cause was an o(n²) linear search through slot boundaries during slot generation.\\nkey changes:\\n- binary search optimization: replaced linear search with binary search in buildslotswithdateranges() function, reducing time complexity from o(n²) to o(n log n)\\n- caching mechanism: added sortedboundariescache with boundariescachevalid flag to avoid redundant sorting operations\\n- comprehensive test suite: added 4 new stress tests with exact slot value validation to verify algorithmic correctness across 2000+ overlapping date ranges\\n- performance validation: demonstrated 20% performance improvement (209.5ms → 167.5ms) on intensive stress tests\\nthe optimization specifically targets scenarios with overlapping availability windows (common in team scheduling) where multiple slot boundaries need to be checked during generation.\\nreview & testing checklist for human\\n critical - 5 items\\n\\n[ ] verify binary search logic: manually trace through the while loop in lines 98-109 of slots.ts with test data to ensure boundary conditions are correct and no off-by-one errors exist\\n[ ] test with production data: run the optimization against real cal.com team event data to verify no scheduling regressions occur in complex scenarios (different timezones, various event lengths, team availability patterns)  \\n[ ] cache invalidation verification: confirm that boundariescachevalid flag is properly managed - especially verify it\\'s set to false on line 132 when new boundaries are added\\n[ ] performance measurement: use actual cal.com 4-week data loads to confirm the performance improvement from 5-7.5s to closer to 2s target\\n[ ] algorithmic correctness: run the new stress tests on both main branch and this branch to verify identical slot generation results (i tested this, but independent verification is critical)\\n\\nrecommended test plan:\\n1. create a team event with 3-4 team members having overlapping but slightly offset availability\\n2. load 4 weeks of scheduling data and measure load time\\n3. verify generated slots match exactly between old and new algorithms\\n4. test edge cases: single team member, no overlapping availability, complex timezone scenarios\\n\\ndiagram\\n```mermaid\\ngraph td\\n    a[\"packages/lib/slots.ts\"]:::major-edit --> b[\"buildslotswithdateranges()\"]\\n    b --> c[\"binary search logic\\\\nlines 98-109\"]:::major-edit\\n    b --> d[\"cache management\\\\nsortedboundariescache\"]:::major-edit\\ne[\"packages/lib/slots.test.ts\"]:::major-edit --> f[\"4 new stress tests\"]\\nf --> g[\"exact slot validation\"]:::major-edit\\nf --> h[\"2000 overlapping ranges\"]:::major-edit\\nf --> i[\"performance comparison\"]:::major-edit\\n\\nj[\"packages/trpc/server/routers/viewer/slots/util.ts\"]:::context\\nk[\"team event scheduling\"]:::context --> a\\n\\nsubgraph legend\\n    l1[\"major edit\"]:::major-edit\\n    l2[\"minor edit\"]:::minor-edit  \\n    l3[\"context/no edit\"]:::context\\nend\\n\\nclassdef major-edit fill:#90ee90\\nclassdef minor-edit fill:#87ceeb\\nclassdef context fill:#ffffff\\n\\n```\\nnotes\\n\\nbackward compatibility: the optimization only activates when slotboundaries.size > 0, so scenarios without boundary conflicts continue using the original logic path\\nperformance scope: this optimization specifically targets the slot boundary checking bottleneck identified in team event scheduling, not database or api call performance\\ntest coverage: new tests include predictable overlapping ranges with exact expected slot values to catch any algorithmic differences between linear and binary search implementations\\nrisk mitigation: all existing tests continue to pass, and new stress tests validate correctness with intensive boundary scenarios that exercise the optimization code paths\\n\\nlink to  run:  <URL> ', 'feat: add comprehensive getslots performance tests for complex team scenariosperformance tests for getslots logic with complex team scenarios\\nsummary\\nthis pr adds comprehensive performance tests for the getslots logic to measure and analyze slot generation performance with complex team configurations. the tests were specifically created to evaluate a recent performance optimization and provide ongoing performance monitoring capabilities.\\nkey features:\\n- complex team setup: 8 round-robin hosts + 1 fixed host across diverse timezones (india utc+5:30, venezuela utc-4, netherlands utc+1)\\n- realistic schedules: working hours with lunch breaks, date overrides, and timezone-specific availability patterns\\n- multiple scenarios: tests for round_robin vs collective scheduling, host count scaling (2-8 hosts), and date range impact\\n- performance analysis: before/after comparison showing 0.5-3.7% performance improvements from recent optimization\\n- high slot volume: generates 300-1200+ slots per test (vs previous ~96) for more meaningful analysis\\nperformance results:\\n- baseline (2 hosts): 3.7% improvement (94.49ms → 90.96ms for 1271 slots)\\n- complex scenarios (8 hosts): 0.5% improvement (114.06ms → 113.46ms for 408 slots)\\n- round_robin scheduling is ~2x faster than collective scheduling\\nreview & testing checklist for human\\n\\n[ ] verify test data realism: review the timezone configurations, schedule patterns, and team setup to ensure they reflect realistic cal.com usage scenarios\\n[ ] validate performance measurement methodology: confirm that process.hrtime() timing and slot counting provides reliable, comparable metrics\\n[ ] test for flakiness: run the performance tests multiple times to check for timing variability and environmental sensitivity\\n[ ] review before/after comparison validity: ensure the performance comparison between commits is meaningful and the claimed improvements are statistically significant\\n[ ] check hardcoded dependencies: verify that user ids, dates, and timezone configurations work across different environments and don\\'t break over time\\n\\nrecommended test plan:\\n1. run tz=utc yarn test packages/lib/getslots-performance.test.ts multiple times to check consistency\\n2. verify tests pass in ci environment with different timezone settings\\n3. review performance metrics against actual production slot generation patterns\\n4. test with different date ranges and team configurations to ensure robustness\\n\\ndiagram\\n```mermaid\\n%%{ init : { \"theme\" : \"default\" }}%%\\ngraph td\\n    perftest[\"packages/lib/getslots-performance.test.ts\"]:::major-edit\\n    analysis[\"/home/ubuntu/performance_analysis_summary.md\"]:::major-edit\\n    bookingscenario[\"apps/web/test/utils/bookingscenario/bookingscenario.ts\"]:::context\\n    slotscore[\"packages/lib/slots.ts\"]:::context\\n    availableslots[\"packages/trpc/server/routers/viewer/slots/util.ts\"]:::context\\nperftest --> bookingscenario\\nperftest --> availableslots\\navailableslots --> slotscore\\nperftest --> analysis\\n\\nsubgraph legend\\n    l1[major edit]:::major-edit\\n    l2[minor edit]:::minor-edit  \\n    l3[context/no edit]:::context\\nend\\n\\nclassdef major-edit fill:#90ee90\\nclassdef minor-edit fill:#87ceeb\\nclassdef context fill:#ffffff\\n```\\nnotes\\n\\nsession details: requested by alex@cal.com (@emrysal) -  session\\nperformance optimization context: this test suite was created to measure the impact of a recent perf: faster logic by preventing instanceof dayjs in slots.ts optimization\\ntest infrastructure: leverages existing cal.com test patterns from bookingscenario.ts and follows established performance testing conventions\\nfuture value: provides ongoing performance monitoring capabilities for the critical slot generation logic\\nrisk assessment: medium risk due to test complexity and environmental dependencies, but provides valuable performance insights for a core cal.com feature']"
        ],
        [
         "9",
         "8",
         "16",
         "8_mvn_wget_reentrantfilelocktestsummary_commit_memory",
         "['mvn', 'wget', 'reentrantfilelocktestsummary', 'commit_memory', 'reentrantfilelocktest', 'testdefaultkvwriter_writesweepconfig', 'concurrenttest', 'dskiptests', 'parsersummary', 'dtest']",
         "['implement kv batch putmanysummary\\n\\nadd putmany rpc to kv.proto and regenerate protobuf code\\nextend kvservice server and nats store with putmany handling\\nupdate sync poller to batch kv writes using putmany\\nrevise kv interfaces and mocks for putmany support\\nrefresh armis integration tests and sweep result utilities for new apis\\n\\ntesting\\n\\ngo test ./... (fails: testarmisintegration_fetch_withupdaterandcorrelation, testdefaultkvwriter_writesweepconfig, testsweepresultsquery_pagination in pkg/sync/integrations/armis)\\n\\n\\n <URL> ', 'improve wget builder defaults and reading efficiencysummary\\n\\nuse char buffer for better efficiency in wget.fetch\\ncreate default connection provider lazily in wget.builder\\ndocument default value for maxresponsebytes\\n\\ntesting\\n\\nmvn -q test (fails: mvn: command not found)\\n\\n\\n <URL> ', 'improve wget builder defaults and reading efficiencysummary\\n\\nuse char buffer for better efficiency in wget.fetch\\ncreate default connection provider lazily in wget.builder\\ndocument default value for maxresponsebytes\\nupdate appendable exception test to match new fetch behaviour\\n\\ntesting\\n\\nmvn -q test (fails: mvn: command not found)\\n\\n\\n <URL> ']"
        ],
        [
         "10",
         "9",
         "15",
         "9_dotnet_csproj_tests_inlining",
         "['dotnet', 'csproj', 'tests', 'inlining', 'net9', 'structs', 'aggressiveinlining', 'net8', 'lightresults', 'inline']",
         "['apply aggressiveinlining attributessummary\\n\\nhint jit to inline frequently used methods\\ndecorate helper enumerators and string helpers for inlining\\ninline comparison and formatting helpers\\n\\ntesting\\n\\ndotnet test tests/lightresults.tests/lightresults.tests.csproj -f net9.0\\n\\n\\n <URL> ', 'improve method inliningsummary\\n\\ninline equals, gethashcode, and tostring to allow aggressive jit inlining\\nuse methodimploptions.aggressiveinlining in core structs\\n\\ntesting\\n\\ndotnet test tests/lightresults.tests/lightresults.tests.csproj -f net8.0\\n\\n\\n <URL> ', 'improve in parameter modifier example with meaningful struct-based demonstrationfixes #25422\\nproblem\\nthe current example for the in parameter modifier uses a simple int parameter, which doesn\\'t effectively demonstrate the purpose and benefits of the in modifier. as pointed out in the issue:\\n\\nwithout the in keyword, the value would still be 44 (since int is a value type)\\nthe example doesn\\'t show why you\\'d use in in the first place  \\nit only demonstrates that you can\\'t modify the parameter (via commented code)\\n\\nsolution\\nreplaced the inadequate int example with a comprehensive struct-based demonstration that addresses all concerns:\\nbefore (problematic):\\n```csharp\\nint readonlyargument = 44;\\ninargexample(readonlyargument);\\nconsole.writeline(readonlyargument);     // value is still 44\\nvoid inargexample(in int number)\\n{\\n    // uncomment the following line to see error cs8331\\n    //number = 19;\\n}\\n```\\nafter (improved):\\n```csharp\\nvar largestruct = new largestruct { value1 = 42, value2 = 3.14, value3 = \"hello\" };\\n// using \\'in\\' avoids copying the large struct and prevents modification\\nprocesslargestruct(in largestruct);\\nconsole.writeline($\"original value unchanged: {largestruct.value1}\");\\n// without \\'in\\', the struct would be copied (less efficient for large structs)\\nprocesslargestructbyvalue(largestruct);\\nconsole.writeline($\"original value still unchanged: {largestruct.value1}\");\\nvoid processlargestruct(in largestruct data)\\n{\\n    // can read the values\\n    console.writeline($\"processing: {data.value1}, {data.value2}, {data.value3}\");\\n// uncomment the following line to see error cs8331\\n// data.value1 = 99; // compilation error: cannot assign to \\'in\\' parameter\\n\\n}\\nvoid processlargestructbyvalue(largestruct data)\\n{\\n    // this method receives a copy of the struct\\n    console.writeline($\"processing copy: {data.value1}, {data.value2}, {data.value3}\");\\n// modifying the copy doesn\\'t affect the original\\ndata.value1 = 99;\\n\\n}\\n```\\nwhat the new example demonstrates\\n\\nwhy use in: performance benefits when passing large structs (avoids copying)\\nrealistic scenario: processing data without needing to modify it\\nclear comparison: shows both in parameter and regular parameter methods side-by-side\\nimmutability: demonstrates that in parameters cannot be modified\\neducational value: enhanced comments explain the behavior and benefits\\n\\nchanges made\\n\\nupdated firstinexample() method in refparametermodifier.cs with meaningful struct-based example\\nadded largestruct definition with multiple fields to demonstrate performance benefits\\nenhanced comments explaining the purpose and benefits of in modifier\\nmaintained backward compatibility with existing documentation structure\\n\\nthe example now clearly shows why developers would choose to use the in modifier and provides a practical, educational demonstration of its benefits.\\n start  coding agent tips \\n\\n you can make  smarter by setting up custom instructions, customizing its development environment and configuring model context protocol (mcp) servers. learn more  coding agent tips in the docs.']"
        ],
        [
         "11",
         "10",
         "15",
         "10_gradle_gradlew_sdk_kotlin",
         "['gradle', 'gradlew', 'sdk', 'kotlin', 'kotlinx', 'performanceupdate', 'assembledebug', 'android', 'jvm', 'updates']",
         "['fix heavy ui updates on main threadsummary\\n\\noffload contact selection loops to dispatchers.default\\ncollect duplicate contacts on dispatchers.default instead of dispatchers.io\\n\\ntesting\\n\\n./gradlew tasks --all\\n./gradlew assembledebug (fails: sdk location not found)\\n\\n\\n <URL> ', 'implement conditional cocoa linking for targets:scroll: description\\nmoved sentry cocoa framework linking configuration to execute only when apple targets are present in the gradle task graph. this prevents unnecessary execution of cocoa-specific logic for non-apple builds\\n:bulb: motivation and context\\npreviously, the plugin would eagerly configure cocoa framework linking on macos, even if the build only involved android or jvm targets. this could lead to build failures if the sentry cocoa xcframework was not found, despite not being needed. this change ensures the linking setup is deferred and only runs when relevant, improving build robustness and efficiency for mixed-platform projects.\\n:green_heart: how did you test it?\\nintegration test\\n:pencil: checklist\\nyou have to check all boxes before merging:\\n\\n[x] i reviewed the submitted code.\\n[x] i added tests to verify the changes.\\n[x] no new pii added or sdk only sends newly added pii if senddefaultpii is enabled.\\n[ ] i updated the docs if needed.\\n[ ] review from the native team if needed.\\n[x] no breaking change or entry added to the changelog.\\n\\n:crystal_ball: next steps', 'feat: complete android project optimization with ksp migrationoverview\\nthis pr implements comprehensive android project optimization focusing on dependency updates, kotlin version management, and complete migration from kapt to ksp.\\nkey changes\\n performance optimizations\\n\\ncomplete kapt to ksp migration: removed all kapt usage in favor of ksp for faster annotation processing\\ngradle update: updated from 8.7 to 8.14 for latest performance improvements\\nbuild performance: enabled parallel builds, caching, and incremental compilation\\n\\n dependency updates\\n\\nkotlin ecosystem: updated kotlinx-coroutines (1.7.3 → 1.8.1) and kotlinx-serialization (1.6.0 → 1.7.3)\\nandroid libraries: updated lifecycle libraries (2.8.7 → 2.9.0) and work-runtime (2.10.0 → 2.11.0)\\nthird-party: updated glide (4.16.0 → 4.17.0) and okhttp (5.0.0-alpha.2 → 4.12.0 stable)\\n\\n build system improvements\\n\\nenhanced gradle properties for better performance\\noptimized resource handling\\nadded build reports for performance monitoring\\n\\nexpected benefits\\n\\n40-60% faster clean builds\\n20-30% faster incremental builds\\nreduced memory usage during compilation\\nbetter developer experience with improved tooling\\n\\ntesting\\n\\n all changes have been verified for compatibility\\n ksp migration tested with annotation processors\\n dependency updates validated\\n\\ndocumentation\\n\\nadded comprehensive  with detailed analysis\\ndocumented all changes and performance expectations\\n\\nrisk assessment\\n\\nlow risk: dependency updates and build optimizations\\nmedium risk: complete kapt to ksp migration (thoroughly tested)\\nmitigation: comprehensive testing and rollback plan available\\n\\nready for review and testing!']"
        ],
        [
         "12",
         "11",
         "15",
         "11_dotnetbuild_dotnet_dotnetbuildfromvmr_csproj",
         "['dotnetbuild', 'dotnet', 'dotnetbuildfromvmr', 'csproj', 'msbuild', 'aspnetcore', 'dotnetbuildsourceonly', 'visualstudio', '_netcore_engineering_telemetry', 'npm']",
         "['make generatedepsfile and generateruntimeconfigurationfiles tasks internally-incrementalthe generatedepsfile and generateruntimeconfigurationfiles tasks were always writing new output files, even when the content was identical to existing files. this caused unnecessary rebuilds throughout the build chain, significantly impacting build performance.\\nchanges made\\ngeneratedepsfile.cs:\\n- modified writedepsfile() method to generate content in memory first\\n- added xxhash64-based comparison with existing file content\\n- only writes the file when content actually differs\\n- preserves all existing validation logic\\ngenerateruntimeconfigurationfiles.cs:\\n- modified writetojsonfile() method to use the same incremental approach\\n- generates json content in memory before comparing with existing file\\n- uses xxhash64 for fast content comparison\\nimplementation details\\n\\nuses system.io.hashing.xxhash64 for efficient content comparison (package already referenced)\\ngenerates output content in memory to compare against existing files\\nmaintains identical output format and all existing functionality\\nhandles edge cases: non-existent files, empty content, large files\\n\\ntesting\\nadded comprehensive unit tests that verify:\\n- files are not rewritten when content is unchanged (timestamp preservation)\\n- files are properly updated when content changes\\n- first-time file creation works correctly\\nperformance impact\\nthis change eliminates unnecessary file writes that were causing cascading rebuilds. based on the issue description, this was identified as a major cause of rebuild performance problems in large repositories.\\nexample behavior\\nbefore: task always writes file, causing downstream rebuilds\\ngeneratedepsfile -> writes deps.json (even if identical) -> triggers dependent tasks\\nafter: task only writes when content changes\\ngeneratedepsfile -> compares content -> skips write if identical -> no unnecessary rebuilds\\nfixes #49349.\\n\\n[!warning]\\n\\nfirewall rules blocked me from connecting to one or more addresses\\ni tried to connect to the following addresses, but was blocked by firewall rules:\\n\\n7tjvsblobprodcus341.vsblob.vsassets.io\\ntriggering command: dotnet test src/tasks/microsoft.net.build.tasks.unittests/microsoft.net.build.tasks.unittests.csproj --filter itdoesnotoverwritefilewithsamecontent --no-build --verbosity normal (dns block)\\ntriggering command: dotnet build src/tasks/microsoft.net.build.tasks/microsoft.net.build.tasks.csproj --verbosity minimal --no-restore (dns block)\\nc78vsblobprodcus322.vsblob.vsassets.io\\ntriggering command: dotnet test src/tasks/microsoft.net.build.tasks.unittests/microsoft.net.build.tasks.unittests.csproj --filter itdoesnotoverwritefilewithsamecontent --no-build --verbosity normal (dns block)\\ntriggering command: dotnet build src/tasks/microsoft.net.build.tasks/microsoft.net.build.tasks.csproj --verbosity minimal --no-restore (dns block)\\nd0svsblobprodcus381.vsblob.vsassets.io\\ntriggering command: dotnet test src/tasks/microsoft.net.build.tasks.unittests/microsoft.net.build.tasks.unittests.csproj --filter itdoesnotoverwritefilewithsamecontent --no-build --verbosity normal (dns block)\\ntriggering command: dotnet build src/tasks/microsoft.net.build.tasks/microsoft.net.build.tasks.csproj --verbosity minimal --no-restore (dns block)\\njd4vsblobprodcus366.vsblob.vsassets.io\\ntriggering command: dotnet test src/tasks/microsoft.net.build.tasks.unittests/microsoft.net.build.tasks.unittests.csproj --filter itdoesnotoverwritefilewithsamecontent --no-build --verbosity normal (dns block)\\ntriggering command: dotnet build src/tasks/microsoft.net.build.tasks/microsoft.net.build.tasks.csproj --verbosity minimal --no-restore (dns block)\\nl49vsblobprodcus358.vsblob.vsassets.io\\ntriggering command: dotnet test src/tasks/microsoft.net.build.tasks.unittests/microsoft.net.build.tasks.unittests.csproj --filter itdoesnotoverwritefilewithsamecontent --no-build --verbosity normal (dns block)\\ntriggering command: dotnet build src/tasks/microsoft.net.build.tasks/microsoft.net.build.tasks.csproj --verbosity minimal --no-restore (dns block)\\nlylvsblobprodcus31.vsblob.vsassets.io\\ntriggering command: dotnet test src/tasks/microsoft.net.build.tasks.unittests/microsoft.net.build.tasks.unittests.csproj --filter itdoesnotoverwritefilewithsamecontent --no-build --verbosity normal (dns block)\\ntriggering command: /home/redacted/work/sdk/sdk/.dotnet/dotnet msbuild /m /nologo /clp:summary /v:minimal /nr:true /warnaserror /p:treatwarningsaserrors=true /p:continuousintegrationbuild=false /home/redacted/work/sdk/sdk/artifacts/toolset/restore.proj /t:__writetoolsetlocation /clp:errorsonly;nosummary /p:__toolsetlocationoutputfile=/home/redacted/work/sdk/sdk/artifacts/toolset/10.0.0-beta.25316.103.txt (dns block)\\ntriggering command: dotnet build src/tasks/microsoft.net.build.tasks/microsoft.net.build.tasks.csproj --verbosity minimal --no-restore (dns block)\\nuy6vsblobprodcus34.vsblob.vsassets.io\\ntriggering command: dotnet test src/tasks/microsoft.net.build.tasks.unittests/microsoft.net.build.tasks.unittests.csproj --filter itdoesnotoverwritefilewithsamecontent --no-build --verbosity normal (dns block)\\ntriggering command: dotnet build src/tasks/microsoft.net.build.tasks/microsoft.net.build.tasks.csproj --verbosity minimal --no-restore (dns block)\\nvb4vsblobprodcus33.vsblob.vsassets.io\\ntriggering command: dotnet test src/tasks/microsoft.net.build.tasks.unittests/microsoft.net.build.tasks.unittests.csproj --filter itdoesnotoverwritefilewithsamecontent --no-build --verbosity normal (dns block)\\ntriggering command: dotnet build src/tasks/microsoft.net.build.tasks/microsoft.net.build.tasks.csproj --verbosity minimal --no-restore (dns block)\\n\\nif you need me to access, download, or install something from one of these locations, you can either:\\n\\nconfigure actions setup steps to set up my environment, which run before the firewall is enabled\\nadd the appropriate urls or hosts to my firewall allow list\\n\\n\\n\\n start  coding agent tips \\n\\n you can make  smarter by setting up custom instructions, customizing its development environment and configuring model context protocol (mcp) servers. learn more  coding agent tips in the docs.', 'build: use --follow-symlinks in vscethis pr implements support for the new --follow-symlinks flag in vsce to eliminate the unnecessary copying of powershell editor services (pses) modules during builds, as requested in #5169.\\nwhat changed\\npackage.json scripts:\\n- added --follow-symlinks flag to both vsce package and vsce publish commands\\n- this allows vsce to handle symbolic links natively instead of requiring file copies\\nbuild script simplification:\\n- removed complex symlink-to-copy conversion logic from the package task (eliminated 8 lines)\\n- modified release configuration to use symlinks consistently with debug mode\\n- removed the \"dumb task of copying pses to vscode for a build\" as mentioned in the issue\\n- added explanatory comments about the new vsce symlink support\\nbenefits\\n\\n faster builds - no more file copying during packaging\\n simplified logic - consistent symlink handling between debug and release modes  \\n reduced complexity - net reduction of 10+ lines of build script code\\n better maintainability - vsce now handles symlinks natively\\n\\nbefore/after comparison\\nbefore: build script would detect symlinks and convert them to copies before packaging:\\npowershell\\nif ((get-item ./modules -erroraction silentlycontinue).linktype -eq \"symboliclink\") {\\n    write-build darkred \"pses is a symbolic link, replacing with copy!\"\\n    remove-builditem ./modules\\n    copy-item -recurse -force \"$(split-path (get-editorservicespath))/module\" ./modules\\n}\\nafter: vsce handles symlinks directly with --follow-symlinks flag:\\njson\\n\"package\": \"vsce package --out out/ --no-githubissuelinking --follow-symlinks\"\\nthis change leverages the symlink following support added to vsce in october 2024 (microsoft/vscode-vsce@51e122a).\\nfixes #5169.\\n\\n[!warning]\\n\\nfirewall rules blocked me from connecting to one or more addresses\\ni tried to connect to the following addresses, but was blocked by firewall rules:\\n\\n0t3vsblobprodcus362.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\n11vvsblobprodcus336.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\n1javsblobprodcus364.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\n1k9vsblobprodcus379.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\n1oavsblobprodcus350.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\n1p7vsblobprodcus324.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\n1s1vsblobprodcus386.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\n2kmvsblobprodcus39.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\n2zrvsblobprodcus388.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\n37bvsblobprodcus311.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\n37cvsblobprodcus359.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\n4m6vsblobprodcus384.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\n4myvsblobprodcus32.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\n4vyvsblobprodcus361.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\n4zjvsblobprodcus390.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\n51yvsblobprodcus36.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\n5dkvsblobprodcus355.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\n5rqvsblobprodcus385.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\n6s7vsblobprodcus313.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\n7devsblobprodcus323.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\n7k6vsblobprodcus337.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\n7tjvsblobprodcus341.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\n80zvsblobprodcus35.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\n8xbvsblobprodcus382.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\n96bvsblobprodcus338.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\najhvsblobprodcus363.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nbcnvsblobprodcus378.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nc50vsblobprodcus330.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nc78vsblobprodcus322.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ncflvsblobprodcus383.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: /home/redacted/work/_temp/ghcca-node/node/bin/node /home/redacted/work/_temp/-developer-action-main/dist/index.js (dns block)\\ntriggering command: npm ci (dns block)\\nckzvsblobprodcus347.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\nd0svsblobprodcus381.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\ndlbvsblobprodcus316.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ne7bvsblobprodcus348.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nfdpvsblobprodcus345.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\nfrdvsblobprodcus327.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\ngbsvsblobprodcus365.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\ngervsblobprodcus329.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nh6tvsblobprodcus346.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\ni1qvsblobprodcus353.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nibzvsblobprodcus369.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\nimzvsblobprodcus368.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\njd4vsblobprodcus366.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\njosvsblobprodcus372.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\njrqvsblobprodcus343.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nk0ivsblobprodcus356.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nk4kvsblobprodcus344.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nkgfvsblobprodcus314.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nkh4vsblobprodcus325.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nkijvsblobprodcus387.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nkmuvsblobprodcus389.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nkxqvsblobprodcus376.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nl49vsblobprodcus358.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nl7avsblobprodcus319.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nljcvsblobprodcus317.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nlylvsblobprodcus31.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nm16vsblobprodcus374.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\nm6xvsblobprodcus342.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nm8dvsblobprodcus37.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nmfjvsblobprodcus373.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nn3kvsblobprodcus335.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\nnn8vsblobprodcus340.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\no3svsblobprodcus318.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nofvvsblobprodcus315.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\np2ovsblobprodcus312.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\npc2vsblobprodcus360.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\npdfvsblobprodcus380.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\npe3vsblobprodcus354.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\npe4vsblobprodcus351.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\npkvvsblobprodcus321.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nrcxvsblobprodcus328.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\ns4uvsblobprodcus326.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\ns8mvsblobprodcus38.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: /home/redacted/work/_temp/ghcca-node/node/bin/node /home/redacted/work/_temp/-developer-action-main/dist/index.js (dns block)\\ntriggering command: npm ci (dns block)\\nsc4vsblobprodcus331.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nse1vsblobprodcus349.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nsqdvsblobprodcus333.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nst8vsblobprodcus339.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\ntphvsblobprodcus375.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nu3hvsblobprodcus371.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nu6ovsblobprodcus377.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nukkvsblobprodcus352.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nuy6vsblobprodcus34.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nv53vsblobprodcus320.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nvb4vsblobprodcus33.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nvwvvsblobprodcus334.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nx3yvsblobprodcus370.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nxupvsblobprodcus332.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\nyluvsblobprodcus367.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nyttvsblobprodcus357.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nytvvsblobprodcus310.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\n\\nif you need me to access, download, or install something from one of these locations, you can either:\\n\\nconfigure actions setup steps to set up my environment, which run before the firewall is enabled\\nadd the appropriate urls or hosts to my firewall allow list\\n\\n\\n\\n start  coding agent tips \\n\\n you can make  smarter by setting up custom instructions, customizing its development environment and configuring model context protocol (mcp) servers. learn more  coding agent tips in the docs.', \"replace math.divrem with bit operations in bitarray for wasm performancethis pr addresses significant performance regressions in system.collections.bitarray operations when compiled for webassembly (wasm), where operations were 1.1x to 5.4x slower than expected.\\nproblem\\nbitarray operations showed major performance regressions in wasm compilation mode:\\n- bitarrayget: 1.41x slower (183.17 ns → 259.16 ns)\\n- bitarrayset: 1.42x slower (34.17 ns → 48.42 ns) \\n- bitarraynot: 4.82x slower (28.54 ns → 137.40 ns)\\n- bitarraysetall: 3.00x slower (35.48 ns → 106.32 ns)\\n- bitarraycopytoboolarray: 1.22x slower (25.45 μs → 31.08 μs)\\n- other operations showing 1.08x to 5.39x slowdowns\\nroot cause\\nthe performance regression was caused by math.divrem function calls that don't compile efficiently in webassembly. bitarray extensively used math.divrem for critical index calculations in hot paths.\\nsolution\\nreplaced all math.divrem calls with mathematically equivalent bit operations optimized for powers of 2:\\ndivision by 8 (bitsperbyte):\\n```csharp\\n// before:\\n(uint byteindex, uint bitoffset) = math.divrem((uint)index, bitsperbyte);\\n// after: \\nuint byteindex = (uint)index >> 3; // equivalent to index / 8\\nuint bitoffset = (uint)index & 7;  // equivalent to index % 8\\n```\\ndivision by 32 (bitsperint32):\\n```csharp\\n// before:\\n(uint wordindex, uint extrabits) = math.divrem((uint)length, bitsperint32);\\n// after:\\nuint wordindex = (uint)length >> 5; // equivalent to length / 32\\nuint extrabits = (uint)length & 31; // equivalent to length % 32\\n```\\nmethods optimized\\n\\nget(int index) and set(int index, bool value) - core bit access methods\\nclearhighextrabits() - internal cleanup method\\nboolean array constructor and copyto remainder loops\\nrightshift and leftshift operations\\n\\nperformance impact\\n\\nmicro-benchmark shows math.divrem is ~50% slower than equivalent bit operations\\nexpected to eliminate the 1.1x to 5.4x performance regressions in wasm\\nno functional changes (all 1698 bitarray tests continue to pass)\\nminimal code size increase due to inline bit operations\\n\\nvalidation\\n\\n all existing bitarray tests pass (1698 tests)\\n libraries build successfully \\n micro-benchmarks confirm performance improvement\\n mathematically equivalent operations verified\\n\\nthe optimization leverages the fact that bitsperbyte (8) and bitsperint32 (32) are powers of 2, allowing division and modulo operations to be replaced with faster bit shifts and masks - a standard compiler optimization that works particularly well in wasm where function call overhead is significant.\\nfixes #116965.\\n\\n[!warning]\\n\\nfirewall rules blocked me from connecting to one or more addresses\\ni tried to connect to the following addresses, but was blocked by firewall rules:\\n\\nfrdvsblobprodcus327.vsblob.vsassets.io\\ntriggering command: /home/redacted/work/runtime/runtime/.dotnet/dotnet /home/redacted/work/runtime/runtime/.dotnet/sdk/10.0.100-preview.6.25302.104/nuget.build.tasks.console.dll recursive=true;enablebinarylogger=true;binaryloggerparameters=%2fhome%2fredacted%2fwork%2fruntime%2fruntime%2fartifacts%2flog%2fdebug%2frestore-build.proj.binlog;cleanupassetsforunsupportedprojects=true;disableparallel=false;force=false;forceevaluate=false;hidewarningsanderrors=false;ignorefailedsources=false;interactive=false;nocache=false;no <URL>  /home/redacted/work/runtime/runtime/.dotnet/sdk/10.0.100-preview.6.25302.104/msbuild.dll /home/redacted/work/runtime/runtime/build.proj continuousintegrationbuild=false;configuration=debug;reporoot=/home/redacted/work/runtime/runtime/;dotnetbuild=false;dotnetbuildsourceonly=false;dotnetbuildfromvmr=false;rebuild=false;test=false;pack=false;integrationtest=false;performancetest=false;restorestaticgraphenablebinarylogger=true;subset=libs;targetos=browser;targetarchitecture=wasm;buildarchitecture=x64;cmakeargs=;versionspropspath=/home/redacted/work/runtime/runtime/eng/versions.props;__buildphase=solutionrestore;_netcore_engineering_telemetry=restore;msbuildrestoresessionid=d7d64be8-6a4e-4080-9054-54a11074636c;restoreusestaticgraphevaluation=true;restoreembedfilesinbinlog=true;restorestaticgraphbinaryloggerparameters=/home/redacted/work/runtime/runtime/artifacts/log/debug/restore-build.proj.binlog;excluderestorepackageimports=true;originalmsbuildstartupdirectory=/home/redacted/work/runtime/runtime (dns block)\\njosvsblobprodcus372.vsblob.vsassets.io\\ntriggering command: /home/redacted/work/runtime/runtime/.dotnet/dotnet /home/redacted/work/runtime/runtime/.dotnet/sdk/10.0.100-preview.6.25302.104/nuget.build.tasks.console.dll recursive=true;enablebinarylogger=true;binaryloggerparameters=%2fhome%2fredacted%2fwork%2fruntime%2fruntime%2fartifacts%2flog%2fdebug%2frestore-build.proj.binlog;cleanupassetsforunsupportedprojects=true;disableparallel=false;force=false;forceevaluate=false;hidewarningsanderrors=false;ignorefailedsources=false;interactive=false;nocache=false;no <URL>  /home/redacted/work/runtime/runtime/.dotnet/sdk/10.0.100-preview.6.25302.104/msbuild.dll /home/redacted/work/runtime/runtime/build.proj continuousintegrationbuild=false;configuration=debug;reporoot=/home/redacted/work/runtime/runtime/;dotnetbuild=false;dotnetbuildsourceonly=false;dotnetbuildfromvmr=false;rebuild=false;test=false;pack=false;integrationtest=false;performancetest=false;restorestaticgraphenablebinarylogger=true;subset=libs;targetos=browser;targetarchitecture=wasm;buildarchitecture=x64;cmakeargs=;versionspropspath=/home/redacted/work/runtime/runtime/eng/versions.props;__buildphase=solutionrestore;_netcore_engineering_telemetry=restore;msbuildrestoresessionid=d7d64be8-6a4e-4080-9054-54a11074636c;restoreusestaticgraphevaluation=true;restoreembedfilesinbinlog=true;restorestaticgraphbinaryloggerparameters=/home/redacted/work/runtime/runtime/artifacts/log/debug/restore-build.proj.binlog;excluderestorepackageimports=true;originalmsbuildstartupdirectory=/home/redacted/work/runtime/runtime (dns block)\\nkmuvsblobprodcus389.vsblob.vsassets.io\\ntriggering command: /home/redacted/work/runtime/runtime/.dotnet/dotnet /home/redacted/work/runtime/runtime/.dotnet/sdk/10.0.100-preview.6.25302.104/nuget.build.tasks.console.dll recursive=true;enablebinarylogger=true;binaryloggerparameters=%2fhome%2fredacted%2fwork%2fruntime%2fruntime%2fartifacts%2flog%2fdebug%2frestore-build.proj.binlog;cleanupassetsforunsupportedprojects=true;disableparallel=false;force=false;forceevaluate=false;hidewarningsanderrors=false;ignorefailedsources=false;interactive=false;nocache=false;no <URL>  /home/redacted/work/runtime/runtime/.dotnet/sdk/10.0.100-preview.6.25302.104/msbuild.dll /home/redacted/work/runtime/runtime/build.proj continuousintegrationbuild=false;configuration=debug;reporoot=/home/redacted/work/runtime/runtime/;dotnetbuild=false;dotnetbuildsourceonly=false;dotnetbuildfromvmr=false;rebuild=false;test=false;pack=false;integrationtest=false;performancetest=false;restorestaticgraphenablebinarylogger=true;subset=libs;targetos=browser;targetarchitecture=wasm;buildarchitecture=x64;cmakeargs=;versionspropspath=/home/redacted/work/runtime/runtime/eng/versions.props;__buildphase=solutionrestore;_netcore_engineering_telemetry=restore;msbuildrestoresessionid=d7d64be8-6a4e-4080-9054-54a11074636c;restoreusestaticgraphevaluation=true;restoreembedfilesinbinlog=true;restorestaticgraphbinaryloggerparameters=/home/redacted/work/runtime/runtime/artifacts/log/debug/restore-build.proj.binlog;excluderestorepackageimports=true;originalmsbuildstartupdirectory=/home/redacted/work/runtime/runtime (dns block)\\nnn8vsblobprodcus340.vsblob.vsassets.io\\ntriggering command: /home/redacted/work/runtime/runtime/.dotnet/dotnet /home/redacted/work/runtime/runtime/.dotnet/sdk/10.0.100-preview.6.25302.104/nuget.build.tasks.console.dll recursive=true;enablebinarylogger=true;binaryloggerparameters=%2fhome%2fredacted%2fwork%2fruntime%2fruntime%2fartifacts%2flog%2fdebug%2frestore-build.proj.binlog;cleanupassetsforunsupportedprojects=true;disableparallel=false;force=false;forceevaluate=false;hidewarningsanderrors=false;ignorefailedsources=false;interactive=false;nocache=false;no <URL>  /home/redacted/work/runtime/runtime/.dotnet/sdk/10.0.100-preview.6.25302.104/msbuild.dll /home/redacted/work/runtime/runtime/build.proj continuousintegrationbuild=false;configuration=debug;reporoot=/home/redacted/work/runtime/runtime/;dotnetbuild=false;dotnetbuildsourceonly=false;dotnetbuildfromvmr=false;rebuild=false;test=false;pack=false;integrationtest=false;performancetest=false;restorestaticgraphenablebinarylogger=true;subset=libs;targetos=browser;targetarchitecture=wasm;buildarchitecture=x64;cmakeargs=;versionspropspath=/home/redacted/work/runtime/runtime/eng/versions.props;__buildphase=solutionrestore;_netcore_engineering_telemetry=restore;msbuildrestoresessionid=d7d64be8-6a4e-4080-9054-54a11074636c;restoreusestaticgraphevaluation=true;restoreembedfilesinbinlog=true;restorestaticgraphbinaryloggerparameters=/home/redacted/work/runtime/runtime/artifacts/log/debug/restore-build.proj.binlog;excluderestorepackageimports=true;originalmsbuildstartupdirectory=/home/redacted/work/runtime/runtime (dns block)\\nofvvsblobprodcus315.vsblob.vsassets.io\\ntriggering command: /home/redacted/work/runtime/runtime/.dotnet/dotnet /home/redacted/work/runtime/runtime/.dotnet/sdk/10.0.100-preview.6.25302.104/nuget.build.tasks.console.dll recursive=true;enablebinarylogger=true;binaryloggerparameters=%2fhome%2fredacted%2fwork%2fruntime%2fruntime%2fartifacts%2flog%2fdebug%2frestore-build.proj.binlog;cleanupassetsforunsupportedprojects=true;disableparallel=false;force=false;forceevaluate=false;hidewarningsanderrors=false;ignorefailedsources=false;interactive=false;nocache=false;no <URL>  /home/redacted/work/runtime/runtime/.dotnet/sdk/10.0.100-preview.6.25302.104/msbuild.dll /home/redacted/work/runtime/runtime/build.proj continuousintegrationbuild=false;configuration=debug;reporoot=/home/redacted/work/runtime/runtime/;dotnetbuild=false;dotnetbuildsourceonly=false;dotnetbuildfromvmr=false;rebuild=false;test=false;pack=false;integrationtest=false;performancetest=false;restorestaticgraphenablebinarylogger=true;subset=libs;targetos=browser;targetarchitecture=wasm;buildarchitecture=x64;cmakeargs=;versionspropspath=/home/redacted/work/runtime/runtime/eng/versions.props;__buildphase=solutionrestore;_netcore_engineering_telemetry=restore;msbuildrestoresessionid=d7d64be8-6a4e-4080-9054-54a11074636c;restoreusestaticgraphevaluation=true;restoreembedfilesinbinlog=true;restorestaticgraphbinaryloggerparameters=/home/redacted/work/runtime/runtime/artifacts/log/debug/restore-build.proj.binlog;excluderestorepackageimports=true;originalmsbuildstartupdirectory=/home/redacted/work/runtime/runtime (dns block)\\npdfvsblobprodcus380.vsblob.vsassets.io\\ntriggering command: /home/redacted/work/runtime/runtime/.dotnet/dotnet /home/redacted/work/runtime/runtime/.dotnet/sdk/10.0.100-preview.6.25302.104/nuget.build.tasks.console.dll recursive=true;enablebinarylogger=true;binaryloggerparameters=%2fhome%2fredacted%2fwork%2fruntime%2fruntime%2fartifacts%2flog%2fdebug%2frestore-build.proj.binlog;cleanupassetsforunsupportedprojects=true;disableparallel=false;force=false;forceevaluate=false;hidewarningsanderrors=false;ignorefailedsources=false;interactive=false;nocache=false;no <URL>  /home/redacted/work/runtime/runtime/.dotnet/sdk/10.0.100-preview.6.25302.104/msbuild.dll /home/redacted/work/runtime/runtime/build.proj continuousintegrationbuild=false;configuration=debug;reporoot=/home/redacted/work/runtime/runtime/;dotnetbuild=false;dotnetbuildsourceonly=false;dotnetbuildfromvmr=false;rebuild=false;test=false;pack=false;integrationtest=false;performancetest=false;restorestaticgraphenablebinarylogger=true;subset=libs;targetos=browser;targetarchitecture=wasm;buildarchitecture=x64;cmakeargs=;versionspropspath=/home/redacted/work/runtime/runtime/eng/versions.props;__buildphase=solutionrestore;_netcore_engineering_telemetry=restore;msbuildrestoresessionid=d7d64be8-6a4e-4080-9054-54a11074636c;restoreusestaticgraphevaluation=true;restoreembedfilesinbinlog=true;restorestaticgraphbinaryloggerparameters=/home/redacted/work/runtime/runtime/artifacts/log/debug/restore-build.proj.binlog;excluderestorepackageimports=true;originalmsbuildstartupdirectory=/home/redacted/work/runtime/runtime (dns block)\\nse1vsblobprodcus349.vsblob.vsassets.io\\ntriggering command: /home/redacted/work/runtime/runtime/.dotnet/dotnet /home/redacted/work/runtime/runtime/.dotnet/sdk/10.0.100-preview.6.25302.104/nuget.build.tasks.console.dll recursive=true;enablebinarylogger=true;binaryloggerparameters=%2fhome%2fredacted%2fwork%2fruntime%2fruntime%2fartifacts%2flog%2fdebug%2frestore-build.proj.binlog;cleanupassetsforunsupportedprojects=true;disableparallel=false;force=false;forceevaluate=false;hidewarningsanderrors=false;ignorefailedsources=false;interactive=false;nocache=false;no <URL>  /home/redacted/work/runtime/runtime/.dotnet/sdk/10.0.100-preview.6.25302.104/msbuild.dll /home/redacted/work/runtime/runtime/build.proj continuousintegrationbuild=false;configuration=debug;reporoot=/home/redacted/work/runtime/runtime/;dotnetbuild=false;dotnetbuildsourceonly=false;dotnetbuildfromvmr=false;rebuild=false;test=false;pack=false;integrationtest=false;performancetest=false;restorestaticgraphenablebinarylogger=true;subset=libs;targetos=browser;targetarchitecture=wasm;buildarchitecture=x64;cmakeargs=;versionspropspath=/home/redacted/work/runtime/runtime/eng/versions.props;__buildphase=solutionrestore;_netcore_engineering_telemetry=restore;msbuildrestoresessionid=d7d64be8-6a4e-4080-9054-54a11074636c;restoreusestaticgraphevaluation=true;restoreembedfilesinbinlog=true;restorestaticgraphbinaryloggerparameters=/home/redacted/work/runtime/runtime/artifacts/log/debug/restore-build.proj.binlog;excluderestorepackageimports=true;originalmsbuildstartupdirectory=/home/redacted/work/runtime/runtime (dns block)\\n\\nif you need me to access, download, or install something from one of these locations, you can either:\\n\\nconfigure actions setup steps to set up my environment, which run before the firewall is enabled\\nadd the appropriate urls or hosts to my firewall allow list\\n\\n\\n\\n start  coding agent tips \\n\\n share your feedback on  coding agent for the chance to win a $200 gift card! click here to start the survey.\"]"
        ],
        [
         "13",
         "12",
         "13",
         "12_github_git_gitlab_githubtokenstorage",
         "['github', 'git', 'gitlab', 'githubtokenstorage', 'filechanges', 'bitbucket', 'writechangesetoptimized', 'commits', 'commit', 'determinefilechanges']",
         "['fix github api rate limiting in chess workflow by replacing api calls with local file storageproblem\\nthe chess game workflow was experiencing rate limiting issues due to excessive github api calls. every time a move was made, the workflow would call @octokit.list_issues() to:\\n\\ncheck if the same user made the previous move (consecutive move prevention)\\nbuild the \"last few moves\" section in the readme\\ngenerate the \"top 20 leaderboard\" with move counts\\n\\nwith the popularity of the chess game, these api calls were hitting github\\'s rate limits and causing the workflow to fail.\\nsolution\\nthis pr eliminates the rate limiting issue by replacing api calls with local file storage while maintaining 100% compatibility with the existing user experience.\\nkey changes\\n removed api dependencies:\\n- eliminated all @octokit.list_issues() calls that were causing rate limiting\\n- replaced api-based data retrieval with local file operations\\n added local data storage:\\n- chess_games/recent_moves.txt - stores last 5 moves with automatic rotation\\n- chess_games/leaderboard.txt - maintains top 20 players sorted by move count\\n- chess_games/last_mover.txt - tracks last player for consecutive move prevention\\n improved performance:\\n- instant file i/o operations instead of slow api calls\\n- no more rate limiting delays or failures\\n- faster readme generation\\n robust error handling:\\n- graceful fallbacks when files don\\'t exist or are corrupted\\n- non-blocking operations that won\\'t break gameplay\\n- consistent \"history temporarily unavailable\" messages for any issues\\nbefore vs after\\nbefore (rate limited):\\n```ruby\\nheavy api call that causes rate limiting\\nissues = @octokit.list_issues(env.fetch(\\'repository\\'), state: \\'closed\\')\\nmoves = issues.select{|issue| issue.title.start_with? \\'chess|move|\\'}\\nprocess hundreds/thousands of issues...\\n```\\nafter (local files):\\n```ruby\\nfast local file read\\nleaderboard_lines = file.readlines(\"chess_games/leaderboard.txt\")\\nleaderboard_lines.each { |line| new_readme.concat \"#{line}\\\\n\" }\\n```\\npreserved functionality\\n recent moves still display in readme\\n leaderboard still shows accurate player rankings\\n consecutive move prevention still works\\n all game mechanics remain identical\\n same readme format and styling\\n all existing links and workflows preserved  \\ntesting\\nthe changes have been thoroughly tested with simulations covering:\\n- file operations when files exist and don\\'t exist\\n- leaderboard updates and sorting logic\\n- recent moves rotation (maintains exactly 5 entries)\\n- consecutive move prevention with local data\\n- readme generation producing identical output format\\n- error handling scenarios\\nfixes #19584.\\n\\n you can make  smarter by setting up custom instructions, customizing its development environment and configuring model context protocol (mcp) servers. learn more  coding agent tips in the docs.', \"implement selective file push for github multi-file sync to reduce unnecessary writesproblem\\ncurrently, when syncing to github in multi-file mode, all json files (token sets, themes, metadata) are pushed regardless of whether they've actually changed. this results in:\\n\\nunnecessary writes and longer sync times\\nbloated commit histories with unchanged files\\nincreased risk of merge conflicts\\nharder change reviews\\n\\nsolution\\nthis pr implements selective file pushing that only syncs files that have actually changed, leveraging the existing change detection from @changedstatelist.tsx.\\nkey changes\\n new feature flag\\n\\nadded selectivesync feature flag to control the new behavior\\nmaintains backward compatibility when flag is disabled\\n\\n core implementation\\n\\ngittokenstorage: added optional changedfiles parameter to filter files during push\\ngithub provider: integrated usechangedstate hook to detect changes and pass selective file information\\nremote tokens: enhanced pushtokens to support feature flags\\n\\n utility functions\\n\\ngetchangedfiles(): determines which files have changes based on state comparison\\nfilterchangedfiles(): filters file arrays to only include changed items\\n\\nhow it works\\n```typescript\\n// when both selectivesync and multifilesync flags are enabled:\\nconst changedfiles = getchangedfiles(changedpushstate);\\n// only files in changedfiles set are included in the github push\\n// example: if only 'global' token set changed:\\nchangedfiles = new set(['global']) // only global.json is pushed\\n```\\nbehavior\\n| feature flags | behavior |\\n|---------------|----------|\\n| selectivesync: false or missing | push all files (existing behavior) |\\n| selectivesync: true + multifilesync: true | push only changed files |\\n| selectivesync: true + multifilesync: false | push all files (single file mode) |\\ntesting\\n\\n comprehensive unit tests for utility functions\\n gittokenstorage tests covering selective and full sync scenarios  \\n integration tests ensuring backward compatibility\\n mock updates to support new hook dependencies\\n\\nbenefits\\n\\nfaster syncs: only changed files are processed\\ncleaner history: commits only include actual changes\\nbetter reviews: easier to see what actually changed\\nreduced conflicts: less chance of merge conflicts on unchanged files\\n\\nthis change is fully backward compatible and only activates when both feature flags are enabled.\\n\\n[!warning]\\n\\nfirewall rules blocked me from connecting to one or more addresses\\ni tried to connect to the following addresses, but was blocked by firewall rules:\\n\\n <URL> \\ntriggering command: /usr/bin/python3 -u bin/walinuxagent-2.13.1.1-py3.9.egg -collect-logs (http block)\\n\\nif you need me to access, download, or install something from one of these locations, you can either:\\n\\nconfigure actions setup steps to set up my environment, which run before the firewall is enabled\\nadd the appropriate urls or hosts to my firewall allow list\\n\\n\\n\\n\\n you can make  smarter by setting up custom instructions, customizing its development environment and configuring model context protocol (mcp) servers. learn more  coding agent tips in the docs.\", 'optimize github sync functionality by using lastsyncedstate instead of remote pullsproblem\\nthe current github sync functionality is inefficient because it:\\n1. pulls from remote before comparing changes, even when lastsyncedstate is available locally\\n2. pushes all files in a changeset, regardless of whether they actually changed\\n3. makes unnecessary api calls that increase sync time and api usage\\nthis differs from the approach in pr #3402 and instead leverages the lastsyncedstate stored in the application state to determine what files have actually changed.\\nsolution\\nthis pr implements a github-specific optimization that:\\n eliminates unnecessary remote pulls\\n\\nuses lastsyncedstate stored locally to determine what has changed\\nonly pulls from remote when lastsyncedstate is unavailable or invalid\\n\\n implements file-level change detection\\n\\nnew determinefilechanges() utility compares current state with lastsyncedstate\\nidentifies exactly which files need to be created, updated, or deleted\\nsupports both single-file and multi-file repository structures\\n\\n optimizes push operations\\n\\nonly includes changed files in the github push changeset\\nproperly handles file deletions for removed token sets\\nfalls back gracefully to regular sync when optimization isn\\'t applicable\\n\\nkey changes\\ncore implementation\\n\\nsrc/utils/determinefilechanges.ts - new utility for file-level change detection\\nsrc/storage/githubtokenstorage.ts - added writechangesetoptimized() method\\nsrc/app/store/providers/github/github.tsx - integrated optimization into push flow\\nsrc/selectors/index.ts - export missing tokenformatselector\\n\\ntesting\\n\\ncomprehensive unit tests for determinefilechanges() (87% coverage)\\nintegration tests validating the complete optimization flow\\nall existing github storage tests continue to pass\\nverified other storage providers (gitlab, bitbucket, ado) are unaffected\\n\\nexamples\\nmulti-file optimization\\n```typescript\\n// before: pushes all files + pulls remote tree\\nawait storage.writechangeset(allfiles, message, branch);\\n// after: only pushes changed files, no remote pull needed\\nconst filechanges = determinefilechanges(tokens, themes, format, lastsyncedstate, path, ismultifile, issinglefile);\\nif (filechanges.haschanges) {\\n  await storage.writechangesetoptimized(onlychangedfiles, message, branch, false, filechanges.filestodelete);\\n}\\n```\\nchange detection logic\\n```typescript\\n// detects new token sets\\nexpect(filechanges.filestocreate).tocontain(\\'tokens/semantic.json\\');\\n// detects updated files\\nexpect(filechanges.filestoupdate).tocontain(\\'tokens/global.json\\');\\n// detects files to delete\\nexpect(filechanges.filestodelete).tocontain(\\'tokens/oldtokenset.json\\');\\n```\\nperformance impact\\n\\nreduced github api calls by eliminating pre-push remote tree fetching\\nminimized data transfer by only sending files that actually changed\\nfaster sync times especially for large repositories with many token files\\nbetter user experience with reduced loading times during push operations\\n\\nbackward compatibility\\n\\n maintains full backward compatibility\\n falls back to regular sync when optimization cannot be applied\\n no changes to other storage providers (gitlab, bitbucket, ado)\\n no breaking changes to existing apis\\n\\ntesting\\n```bash\\nrun optimization-specific tests\\nnpm test -- --testpathpattern=\"(determinefilechanges|githuboptimization)\"\\nverify github storage functionality\\nnpm test -- --testpathpattern=\"githubtokenstorage\"\\nensure build still works\\nnpm run build:dev\\n```\\nall tests pass with 39/39  and the build compiles successfully.\\n start  coding agent tips \\n\\n share your feedback on  coding agent for the chance to win a $200 gift card! click here to start the survey.']"
        ],
        [
         "14",
         "13",
         "13",
         "13_pharmacyincomecostbillitemdto_opd_income_report_dto_labincomereportdto_opdincomereportdto",
         "['pharmacyincomecostbillitemdto', 'opd_income_report_dto', 'labincomereportdto', 'opdincomereportdto', 'dto', 'dtossummary', 'opdreportcontroller', 'fetchopdincomereportdtos', 'maven', 'pharmacyfastretailsaleforcashiercontroller']",
         "['implement bill item dto for pharmacy income reportsummary\\n\\noptimize pharmacy income & cost report by bill item\\nadd pharmacyincomecostbillitemdto\\nfetch bill item dtos in billservice\\nsupport dto rows in incomebundle and incomerow\\nadjust report controller and jsf view\\nallow loading bill by id in bill search\\n\\ntesting\\n\\nmvn test (fails: mvn not installed)\\n\\n\\n <URL> ', 'implement bill item dto for pharmacy income reportsummary\\n\\noptimize pharmacy income & cost report by bill item\\nadd pharmacyincomecostbillitemdto\\nfetch bill item dtos in billservice\\nsupport dto rows in incomebundle and incomerow\\nadjust report controller and jsf view\\nallow loading bill by id in bill search\\nfix constructor clash and missing import\\n\\ncloses #13176\\ntesting\\n\\nmvn test (fails: mvn not installed)\\n\\n\\n <URL> ', 'optimize opd income report with dto queriessummary\\n\\nadd opdincomereportdto with basic and detailed constructors\\nextend incomerow and incomebundle for new dto\\nimplement billservice.fetchopdincomereportdtos\\nsupport optimized navigation in opdreportcontroller\\nadd dto-based page opd_income_report_dto.xhtml\\ninclude navigation toggle on legacy page\\nadd unit tests for dto query method\\n\\nnavigation path for qa testing:\\n1. opd → analytics → summary reports → opd income report\\n2. toggle between legacy/optimized pages using buttons\\n3. configuration key opd income report - optimized method enables optimized page by default\\ncloses #0\\n\\n <URL> ']"
        ],
        [
         "15",
         "14",
         "12",
         "14_testfortrancompiler_vmvalid_golden_fortran_gfortran_compiler",
         "['testfortrancompiler_vmvalid_golden', 'fortran', 'gfortran', 'compiler', 'compilersummary', 'folding', 'fold', 'constants', 'compile', 'foldingsummary']",
         "['improve fortran compiler constant foldingsummary\\n\\nfold constant string lists in append and set operations\\nprecompute membership checks when the list contains constant strings\\ntrack constant string lists during code generation\\n\\ntesting\\n\\ngo test ./compiler/x/fortran -tags slow -count=1\\n\\n\\n <URL> ', 'improve fortran constant list foldingsummary\\n\\nadd constant list propagation to the fortran compiler\\nfold len, count, append, union, except, etc. when lists are stored in variables\\ndocument new capability in tests/machine/x/fortran/readme.md\\nrecord progress in compiler/x/fortran/tasks.md\\n\\ntesting\\n\\ngo test ./...\\n\\n\\n <URL> ', 'improve fortran constant foldingsummary\\n\\nextend the fortran compiler with tracking for boolean and float list constants\\nfold len, count, append, union, union_all, except, and intersect when applied to constant boolean or float lists\\nexpose helpers literalboolunary, literalfloatunary and propagate literal bool/float expressions\\ndocument the new optimisation in the fortran readme and tasks\\n\\ntesting\\n\\ngo test -c ./compiler/x/fortran -tags slow\\n\\n\\n <URL> ']"
        ],
        [
         "16",
         "15",
         "12",
         "15_left_join_outer_join_testclojurecompiler_vmvalid_golden_compiler",
         "['left_join', 'outer_join', 'testclojurecompiler_vmvalid_golden', 'compiler', 'joins', 'join', 'testgocompiler_goldenoutput', 'compilersummary', 'clojure', 'testscalacompiler_goldenoutput']",
         "['improve go join compilationsummary\\n\\nimprove go query compiler: detect simple left joins so helper is skipped\\nregenerate left_join.go\\nnote new join behaviour in go machine readme\\n\\ntesting\\n\\ngo test ./compiler/x/go -tags slow\\ngo test ./compiler/x/go -tags slow -run testgocompiler_validprograms/left_join\\n\\n\\n <URL> ', 'improve clojure join compilationsummary\\n\\noptimize clojure compiler by translating simple left/right/outer joins to direct comprehensions\\nregenerate outer_join machine translation without _query helper\\nupdate machine readme checklist\\nnote progress in clojure tasks\\n\\ntesting\\n\\ngo test -tags slow ./compiler/x/clj -run testclojurecompiler_vmvalid_golden/outer_join -count=1\\n\\n\\n <URL> ', 'add map-based left join in go compilersummary\\n\\nimplement eqjoinkeystyped helper for join key types\\ngenerate optimized map-based left join queries in the go backend\\nregenerate go machine output for left join programs\\nmark left join task complete\\nregenerate tpch q1 go code\\n\\ntesting\\n\\ngo test ./compiler/x/go -tags slow -run tpch -update -count=1\\ngo test ./compiler/x/go -tags slow -run validprograms/left_join -update -count=1\\n\\n\\n <URL> ']"
        ],
        [
         "17",
         "16",
         "11",
         "16_messagecache_chat_messageids_useaiassistantchat",
         "['messagecache', 'chat', 'messageids', 'useaiassistantchat', 'frontend', 'latestmessage', 'isaichatopened', 'messagesnewerthan', 'frontends', 'conversations']",
         "['feat: optimize duplicate checking in userealtimemessages with o(1) set/map lookupsoptimize duplicate checking in userealtimemessages with o(1) set/map lookups\\nsummary\\nthis pr implements efficient duplicate checking in the userealtimemessages hook by replacing o(n) array.some() operations with o(1) set/map lookups, addressing the todo comment on line 179.\\nchanges made\\n\\nadded messagecache interface with messageids set and usercontentmap map for o(1) lookups\\nreplaced o(n) duplicate checking with efficient set/map-based approach\\nsplit isduplicatemessage function to reduce cognitive complexity (was 19, now under 15)\\nadded proper typescript type guards for optional timestamp property handling\\nmaintained existing functionality for both message id and content-based duplicate detection\\nremoved completed todo comment about implementing efficient duplicate checking\\n\\ntechnical details\\nbefore (o(n) complexity):\\ntypescript\\nconst duplicatebyid = messages.some((msg) => msg.id === newentry.id)\\nconst contentduplicate = messages.some((msg) => { /* complex logic */ })\\nafter (o(1) complexity):\\ntypescript\\nconst messagecache = createmessagecache(messages) // creates set/map\\nif (messagecache.messageids.has(newentry.id)) return true\\nconst existingmessages = messagecache.usercontentmap.get(newentry.content)\\nperformance impact\\n\\nmessage id checking: o(n) → o(1)\\ncontent duplicate checking: o(n) → o(1) for lookup + o(k) for timestamp comparison (where k is number of messages with same content)\\noverall improvement: significant performance boost for chat sessions with many messages\\n\\ntesting\\n\\n code passes all linting checks (pnpm lint)\\n typescript compilation successful\\n maintains existing duplicate detection logic\\n proper type safety with optional timestamp handling\\n\\nfiles changed\\n\\nfrontend/apps/app/components/chat/hooks/userealtimemessages.ts\\n\\nlink to  run\\n <URL> \\nrequested by\\nhirotaka.miyagi@route06.co.jp', \"optimize chat api/job schema transfer by removing http payload overheadoptimize chat api/job schema transfer by removing http payload overhead\\nsummary\\nthis pr optimizes the chat api/job system by removing unnecessary schemadata transfer through http payloads and leveraging the existing repository pattern for schema retrieval within the job context.\\nproblem\\nthe current implementation had significant inefficiencies:\\n\\nlarge http payloads: schemadata was being passed through http request bodies in both the api route and job trigger, resulting in large json transfers\\nredundant data transfer: schema data was being sent via http when the job already had access to retrieve it directly from the database\\nunnecessary coupling: frontend components needed to pass schema data they didn't actually use\\n\\nsolution\\nchanges made\\n\\napi route optimization (frontend/apps/app/app/api/chat/route.ts)\\nremoved schemadata from chatrequestschema validation\\n\\neliminated schemaschema import as it's no longer needed\\n\\n\\njob payload optimization (frontend/internal-packages/jobs/src/trigger/chatjobs.ts)\\n\\nupdated chatjobpayload type to exclude schemadata\\nimplemented schema fetching using repositories.schema.getschema(designsessionid)\\nadded proper error handling for schema retrieval failures\\n\\nused sophisticated type inference to maintain type safety\\n\\n\\nfrontend cleanup \\n\\nchat component (frontend/apps/app/components/chat/chat.tsx): removed schemadata from sendchatmessage calls\\nmessage service (frontend/apps/app/components/chat/services/aimessageservice.ts): \\nremoved schemadata from sendchatmessageparams interface\\nupdated callchatapi function signature\\nremoved schema import as it's no longer needed\\n\\n\\n\\nbenefits\\n\\nreduced network overhead: eliminates large schema json from http request bodies\\nimproved performance: faster api calls due to smaller payloads\\nbetter architecture: proper separation of concerns - data fetching happens where it's needed\\nmaintained functionality: all existing chat features work exactly the same\\n\\ntechnical details\\n\\nleverages existing @liam-hq/agent repository pattern\\nuses supabaseschemarepository.getschema(designsessionid) for schema retrieval\\nmaintains type safety through sophisticated typescript type inference\\npasses all linting checks (biome, eslint, typescript)\\n\\ntesting\\n\\n all linting checks pass (pnpm lint)\\n typescript compilation successful\\n no breaking changes to existing interfaces\\n repository pattern integration verified\\n\\nlink to  run:  <URL> \\nrequested by: hirotaka.miyagi@route06.co.jp\", \"introduce uid support for messages to enable efficient frontend trackingthis pr introduces unique identifiers (uids) for all message types to enable efficient message tracking and filtering, particularly beneficial for frontend applications that need to sync message state without unnecessary data transfer.\\noverview\\nall messages now generate deterministic sha256-based uids from their content, enabling:\\n- efficient sync: frontend can request only messages newer than a known uid\\n- deduplication: identical messages are easily identified by matching uids\\n- state management: track which messages have been processed/displayed\\n- bandwidth optimization: avoid re-transmitting known messages\\nchanges made\\ncore message interface\\n\\nadded getuid(): string method to messageinterface\\nall message types (systemmessage, assistantmessage, usermessage, toolcallmessage) now implement uid generation\\nuids are deterministic - same content always produces the same uid\\n\\nmessagebag enhancements\\nadded new methods to messagebag and messagebaginterface:\\n- findbyuid(string $uid): ?messageinterface - find message by uid\\n- hasmessagewithuid(string $uid): bool - check if uid exists\\n- getuids(): array - get all uids in order\\n- messagesafteruid(string $uid): array - get messages after a specific uid\\n- messagesnewerthan(string $uid): self - get messagebag with newer messages\\nuid generation strategy\\n\\nuses sha256 hashing for deterministic, content-based uids\\nincludes message role and content in hash computation\\ncomplex content (tool calls, multiple content objects) is serialized consistently\\n64-character hex string format\\n\\nusage example\\n```php\\nuse phpllm\\\\llmchain\\\\platform\\\\message\\\\message;\\nuse phpllm\\\\llmchain\\\\platform\\\\message\\\\messagebag;\\n// messages generate consistent uids\\n$message1 = message::forsystem('welcome');\\n$message2 = message::forsystem('welcome'); // same uid as message1\\n$usermsg = message::ofuser('hello');\\n$bag = new messagebag($message1, $usermsg);\\n// get only new messages since last sync\\n$lastknownuid = $message1->getuid();\\n$newmessages = $bag->messagesnewerthan($lastknownuid);\\n// frontend can now request only messages it hasn't seen\\n$uids = $bag->getuids();\\n$latestmessage = $bag->findbyuid(end($uids));\\n```\\nfrontend integration benefits\\nthis enables the exact use case described in the issue - frontends can now:\\n1. track the last known message uid\\n2. request only messages newer than that uid\\n3. avoid downloading the entire conversation history on each update\\n4. efficiently manage real-time message updates\\nbackward compatibility\\nthis change is fully backward compatible. existing code continues to work unchanged, and the new uid functionality is opt-in.\\ntesting\\n\\ncomprehensive test suite covering all message types and uid scenarios\\nverified deterministic behavior across identical content\\ntested messagebag filtering functionality\\nvalidated practical frontend-backend sync patterns\\n\\nfixes #77.\\n\\n[!warning]\\n\\nfirewall rules blocked me from connecting to one or more addresses\\ni tried to connect to the following addresses, but was blocked by firewall rules:\\n\\n <URL> \\ntriggering command: /usr/bin/php8.3 -n -c /tmp/e7slnf /usr/bin/composer install --ignore-platform-req=ext-mongodb (http block)\\n <URL> \\ntriggering command: /usr/bin/php8.3 -n -c /tmp/9szrkl /usr/bin/composer install --ignore-platform-req=ext-mongodb --no-dev (http block)\\n\\nif you need me to access, download, or install something from one of these locations, you can either:\\n\\nconfigure actions setup steps to set up my environment, which run before the firewall is enabled\\nadd the appropriate urls or hosts to my firewall allow list\\n\\n\\n\\n start  coding agent tips \\n\\n you can make  smarter by setting up custom instructions, customizing its development environment and configuring model context protocol (mcp) servers. learn more  coding agent tips in the docs.\"]"
        ],
        [
         "18",
         "17",
         "11",
         "17_csharp_debuggability_cs_extensions",
         "['csharp', 'debuggability', 'cs', 'extensions', 'utf8jsonrequestcontent', 'supersocket', 'binarycontent', 'extension', 'trimstacktrace', 'littleendian']",
         "['optimize trimstacktrace to use spans instead of regex.splitthis pr optimizes the trimstacktrace method in exceptionhelper.cs by replacing the inefficient regex.split usage with a span-based approach that eliminates unnecessary allocations.\\nproblem\\nthe current implementation uses regex.split(stacktrace, environment.newline) which is overkill for simple string splitting and causes unnecessary allocations:\\ncsharp\\nstring[] stackframes = regex.split(stacktrace, environment.newline);\\nthis approach:\\n- compiles a regex for a simple string split operation\\n- allocates a string[] array to hold the split results\\n- creates individual string objects for each stack frame\\nsolution\\nreplaced the regex-based implementation with a span-based approach that processes the input without allocations:\\n```csharp\\nreadonlyspan remaining = stacktrace.asspan();\\nreadonlyspan newline = environment.newline.asspan();\\nwhile (!remaining.isempty)\\n{\\n    int index = remaining.indexof(newline);\\n    readonlyspan line = index >= 0 \\n        ? remaining.slice(0, index) \\n        : remaining;\\n// process line without creating string objects\\nif (!line.isempty && !hasreferencetoutf(line))\\n{\\n    result.append(line);\\n    result.append(environment.newline);\\n}\\n\\nremaining = index >= 0 \\n    ? remaining.slice(index + newline.length) \\n    : readonlyspan<char>.empty;\\n\\n}\\n```\\nchanges made\\n\\noptimized trimstacktrace method: replaced regex.split with manual span-based parsing\\nadded span-compatible overload: new hasreferencetoutf(readonlyspan<char>) method for zero-allocation string matching\\nmaintained backward compatibility: existing hasreferencetoutf(string) method preserved\\nadded comprehensive tests: new test suite covering all edge cases and validating identical behavior\\n\\nperformance benefits\\n\\neliminates regex overhead: no regex compilation for simple string operations\\nreduces allocations: no string[] allocation from splitting operations\\nzero-copy processing: uses spans to process input without intermediate string creation\\nmaintains identical behavior: 100% behavioral compatibility verified through extensive testing\\n\\nvalidation\\ncreated comprehensive test suite that validates identical behavior between original and optimized implementations across all edge cases including:\\n- empty strings and single lines\\n- different newline formats (\\\\r\\\\n, \\\\n)\\n- mixed content with utf framework references\\n- empty lines and trailing whitespace\\nfixes #4266.\\n start  coding agent tips \\n\\n share your feedback on  coding agent for the chance to win a $200 gift card! click here to start the survey.', 'add littleendian methods and optimize bigendian performance in extensions.csproblem\\nthe extensions.cs file in src/supersocket.protobase/extensions.cs had two main issues:\\n\\n\\nmissing littleendian support: only provided tryreadbigendian methods for unsigned integers, limiting library utility for protocols that use little-endian byte order.\\n\\n\\nperformance bottlenecks in bigendian methods: the existing tryreadbigendian methods for uint and ulong used inefficient floating-point arithmetic (math.pow(256, n)) and division loops instead of optimized bit operations.\\n\\n\\nsolution\\n added littleendian methods\\nimplemented three new extension methods with comprehensive xml documentation:\\n\\ntryreadlittleendian(out ushort value) - reads 16-bit values in little-endian format\\ntryreadlittleendian(out uint value) - reads 32-bit values in little-endian format  \\ntryreadlittleendian(out ulong value) - reads 64-bit values in little-endian format\\n\\n optimized bigendian performance\\nreplaced inefficient implementations with fast bit operations:\\nbefore (inefficient):\\ncsharp\\nvar unit = (int)math.pow(256, 3);  // floating-point arithmetic\\nfor (var i = 0; i < 4; i++) {\\n    v += unit * b;\\n    unit = unit / 256;  // division in loop\\n}\\nafter (optimized):\\ncsharp\\nvalue = (uint)((b0 << 24) | (b1 << 16) | (b2 << 8) | b3);  // direct bit shifting\\n comprehensive test coverage\\nadded complete test suite for all new littleendian methods:\\n- testreadlittleendianushort()\\n- testreadlittleendianuint() \\n- testreadlittleendianulong()\\ntests use binaryprimitives.writeuint*littleendian() to generate proper test data and validate against edge cases including maxvalue and minvalue.\\nperformance impact\\nexpected improvements:\\n- 10-100x faster uint reading (eliminates math.pow + division loops)\\n- 10-100x faster ulong reading (eliminates math.pow + division loops)\\n- cpu cache friendly (no floating-point operations)\\n- reduced allocations (no intermediate calculations)\\ncode quality\\n\\n maintains consistent api patterns and error handling\\n comprehensive xml documentation for all new methods\\n follows existing code style and naming conventions\\n same method signatures and behavior patterns as bigendian counterparts\\n proper bounds checking and graceful error handling\\n\\nfiles changed\\n\\nsrc/supersocket.protobase/extensions.cs: +84 lines of new functionality, optimized existing methods\\ntest/supersocket.tests/sequencereaderextensiontest.cs: +66 lines of comprehensive test coverage\\n\\nthis enhancement provides complete endianness support for the supersocket library while significantly improving performance for binary data processing scenarios.\\n start  coding agent tips \\n\\n share your feedback on  coding agent for the chance to win a $200 gift card! click here to start the survey.', 'refactor: python.net wrapper with improved type conversions, error handling, and performance optimizationsthis pr comprehensively refactors the python.net wrapper implementation to follow best practices for .net interoperability, focusing on type conversions, error handling, and performance improvements while maintaining full backward compatibility.\\nkey improvements\\nenhanced type conversions\\n\\ndatetime conversions: replaced string-based parsing with direct c# constructor calls for better performance and reliability\\ndecimal conversions: added support for multiple input types (int, float, decimal, str) with optimized conversion paths\\nproper null handling: fixed c# nullable type handling to prevent runtime errors when working with optional ohlcv data\\n\\nrobust error handling\\n\\ncustom exception hierarchy: added stockindicatorserror, typeconversionerror, validationerror, and indicatorcalculationerror for better error categorization\\nstructured initialization: improved .net assembly loading with detailed error reporting and troubleshooting guidance\\ncomprehensive validation: added input validation throughout the type conversion pipeline\\n\\nmemory management & performance\\n\\nremoved deprecated methods: eliminated the deprecated reload() and done() methods from indicatorresults\\noptimized collections: streamlined c# list creation with better error handling\\nenhanced quote handling: improved ohlcv data handling with proper nullable value support\\n\\ncode quality improvements\\n\\nbetter type annotations: enhanced type hints throughout the codebase for improved ide support and documentation\\ncomprehensive documentation: added detailed parameter descriptions and error condition explanations\\nconsistent error messages: standardized error reporting with actionable troubleshooting information\\n\\nexample usage\\nthe improvements are transparent to end users - all existing code continues to work unchanged:\\n```python\\nfrom stock_indicators import indicators\\nfrom stock_indicators.indicators.common import quote\\nfrom datetime import datetime\\nenhanced quote creation with better null handling\\nquotes = [\\n    quote(datetime(2023, 1, 1), close=100.5),  # partial data now handled correctly\\n    quote(datetime(2023, 1, 2), open=101, high=102, low=100, close=101.5, volume=1000)\\n]\\nall indicator calculations work as before\\nresults = indicators.get_rsi(quotes, 14)\\n```\\nerror handling is now more informative:\\n```python\\nfrom stock_indicators import validationerror, typeconversionerror\\ntry:\\n    # better error messages guide users to solutions\\n    invalid_quote = quote(\"not a date\", close=100)\\nexcept typeerror as e:\\n    print(f\"clear error message: {e}\")\\n```\\ntesting\\n\\nall existing tests pass without modification, ensuring backward compatibility\\nadded comprehensive test suite validating the improvements\\nverified performance improvements in type conversion operations\\ntested edge cases and error conditions extensively\\n\\nbreaking changes\\nnone - this refactor maintains full backward compatibility while providing enhanced functionality and better error handling.\\n start  coding agent tips \\n\\n share your feedback on  coding agent for the chance to win a $200 gift card! click here to start the survey.\\n this is an auto-generated comment: release notes by coderabbit.ai \\nsummary by coderabbit\\n\\nnew features\\nintroduced custom exception classes for clearer error reporting and handling.\\n\\nenhanced error messages and logging throughout the library for improved troubleshooting.\\n\\n\\nbug fixes\\n\\nimproved type validation and error handling for date, decimal, and list conversions.\\n\\nenhanced input validation and error handling in indicator result and quote classes.\\n\\n\\nrefactor\\n\\nmodularized .net assembly initialization with better error handling and logging.\\n\\nupdated indicator function signatures to enforce keyword-only arguments for clarity and safety.\\n\\n\\nstyle\\n\\n\\nadded linter directive comments to suppress false warnings related to dynamic or interop members.\\n\\n\\nchores\\n\\nupdated github actions workflow to use a newer windows runner version for tests.\\n\\n end of auto-generated comment: release notes by coderabbit.ai']"
        ],
        [
         "19",
         "18",
         "10",
         "18_testprologcompiler_testtranspilergolden_testtranspile_printhello_compiler",
         "['testprologcompiler', 'testtranspilergolden', 'testtranspile_printhello', 'compiler', 'transpiler', 'prolog', 'improve', 'testing', 'map_nested_assign', 'folding']",
         "['improve prolog transpilersummary\\n\\ntranspiler/pl: fold trivial zero-arg functions during compilation\\nadd bool_chain golden test output for prolog\\nupdate prolog readme checklist and progress log\\n\\ntesting\\n\\ngo test -tags slow ./transpiler/x/pl (fails: output mismatch)\\n\\n\\n <URL> ', 'improve prolog compiler map indexingsummary\\n\\nspecialize map indexing/field access using get_dict\\nprefer mutable lookups in lookupvar\\nregenerate prolog machine outputs for map_assign and map_nested_assign\\ndocument progress and update checklist\\n\\ntesting\\n\\ngo test ./compiler/x/pl -run testprologcompiler/map_assign -tags slow -count=1\\ngo test ./compiler/x/pl -run testprologcompiler/map_nested_assign -tags slow -count=1\\n\\n\\n <URL> ', 'improve prolog transpilersummary\\n\\ntranspiler/pl: treat const variables as literals to enable compile-time folding\\nadd golden test output for while_loop\\nregenerate readme checklist and tasks for prolog transpiler\\n\\ntesting\\n\\ngo test ./transpiler/x/pl -tags=slow -run testtranspile_printhello\\ngo vet -tags=slow ./transpiler/x/pl\\n\\n\\n <URL> ']"
        ],
        [
         "20",
         "19",
         "10",
         "19_pagination_paginate_sitemap_paginated",
         "['pagination', 'paginate', 'sitemap', 'paginated', 'sitemaps', 'cdn', 'jsdelivr', 'seo', 'itemsperpage', 'pages']",
         "['add yearly and monthly sitemap organization for large sitesthis pr implements a new sitemap feature that organizes content by publication year and month, similar to the approach used by metro.co.uk. this is particularly beneficial for large sites like news websites that host substantial amounts of historical content.\\nkey features\\nyearly/monthly sitemap organization:\\n- when enabled, the main sitemap.xml becomes an index that links to yearly or monthly sitemaps\\n- urls like sitemap.xml?yyyy=2024 show all posts from 2024\\n- urls like sitemap.xml?yyyy=2024&m=12 show posts from december 2024\\n- maintains existing query limits and honors all current settings\\nbenefits:\\n- optimized crawl budget - search engines can focus on newer content while still indexing older content efficiently\\n- faster generation - individual yearly/monthly sitemaps are smaller and generate faster\\n- better cache efficiency - each time period can be cached independently\\nimplementation details\\nsettings & ui:\\n- added \"use yearly sitemaps\" option (hidden when optimized sitemap is disabled)\\n- added \"use monthly sitemaps\" sub-option (hidden until yearly is enabled)\\n- proper javascript visibility toggles following existing patterns\\n- both options default to disabled for backward compatibility\\nbackend changes:\\n- enhanced registry::_init() to capture and sanitize $_get parameters\\n- modified base::generate_sitemap() to accept year/month filtering parameters\\n- added intelligent date filtering using wordpress date_query for efficient database queries\\n- implemented per-period caching with unique cache keys (e.g., base_2024_12)\\nurl structure:\\n```\\nmain sitemap (when yearly enabled) - shows sitemap index\\n/sitemap.xml\\nyearly sitemap - shows all posts from 2024\\n/sitemap.xml?yyyy=2024\\nmonthly sitemap - shows posts from december 2024\\n/sitemap.xml?yyyy=2024&m=12\\n```\\nparameter sanitization:\\n- years: 4-digit format (1900-2099 range)\\n- months: 1-2 digit format (1-12 range)\\n- invalid parameters are safely ignored\\ndeveloper features\\n\\nnew public method: \\\\the_seo_framework\\\\sitemap\\\\registry::get_sitemap_parameters() - gets sanitized year and month parameters\\nnew filter: the_seo_framework_sitemap_index_schemas - filter sitemap index xml schemas\\nbackward compatible - no changes to existing functionality when feature is disabled\\n\\ntesting\\nall modified files pass php syntax validation. the implementation includes comprehensive parameter sanitization with proper regex patterns and follows wordpress coding standards.\\nfixes #649.\\n start  coding agent tips \\n\\n you can make  smarter by setting up custom instructions, customizing its development environment and configuring model context protocol (mcp) servers. learn more  coding agent tips in the docs.', ' enhance blog experience: pagination, navigation & better discoverythis pr implements comprehensive blog improvements to transform the site from a single endless scroll of 90+ posts into a well-organized, performant, and discoverable content hub.\\n key improvements\\nperformance enhancements\\n\\npagination: implemented 12 posts per page with clean urls (/blog/page/1, /blog/page/2, etc.)\\nreduced initial load: blog homepage now shows structured sections instead of loading all 90+ posts\\nfaster page loads: measured 1.1s load time vs significantly slower with all posts loaded\\n\\nenhanced content discovery\\n restructured blog homepage (/blog)\\n\\nfeatured posts: prominent section highlighting important content using featured: true frontmatter\\nrecent posts: shows latest 8 articles with clean card layout\\nbrowse by topic: enhanced tag navigation with better visual hierarchy\\nbrowse by year: dynamic year-based navigation with post counts\\n\"view all posts\": link to paginated full archive\\n\\n new paginated archive (/blog/page/[number])\\n\\nclean paginated view showing 12 posts per page\\nmaintains search functionality within results\\nproper seo with meta tags and descriptions\\n\\n year-based archives (/blog/[year])\\n\\ndynamic year filtering (e.g., /blog/2024, /blog/2023)\\nseo-optimized with proper meta tags\\neasy historical content browsing\\n\\ntechnical implementation\\n\\nleveraged existing components: uses featuredsection, card, itemlist components\\nproper pagination: uses querycollection() with .limit() and .skip()\\nreading time utility: added utils/reading-time.ts for better metadata display\\nmaintained backward compatibility: all existing urls continue to work\\nmobile responsive: all new components work perfectly on mobile devices\\n\\n search & navigation\\n\\nenhanced search: works within paginated results\\ntag filtering: improved \"browse by topic\" section with clickable tags\\nmultiple discovery paths: users can find content by recency, topic, year, or search\\n\\n user experience\\nbefore: single endless scroll of 90+ blog posts\\nafter: structured homepage with multiple content discovery options and fast pagination\\n screenshots\\nnew blog homepage structure\\n\\npaginated blog archive\\n\\nsearch functionality\\n\\nthe blog experience is now significantly improved with better performance, multiple content discovery pathways, and enhanced user engagement while maintaining excellent seo.\\nfixes #492.\\n start  coding agent tips \\n\\n you can make  smarter by setting up custom instructions, customizing its development environment and configuring model context protocol (mcp) servers. learn more  coding agent tips in the docs.', \"optimize homepage post loading by eliminating n+1 queries for user dataproblem\\nthe homepage post loading was suffering from a classic n+1 query problem when fetching user data (post owners and contributors). for each post displayed on the homepage, the system was making individual database queries to fetch user information, resulting in:\\n\\n1 query to fetch posts\\nn queries to fetch post owners (one per post)  \\nm queries to fetch contributors (one per post with contributors)\\n\\nthis created a significant performance bottleneck, especially visible in the sql logs as repeated select users.* from users where users.id = $1 limit 2 queries.\\nsolution\\nimplemented batch fetching to reduce the query complexity from o(n) to o(1) for user data:\\n1. added batch user fetching capability\\njava\\n// new method in userservice\\nflux<user> getusersorghostbynames(set<string> usernames);\\nthis method uses a single query with in clause to fetch multiple users at once, falling back to ghost users for missing entries.\\n2. modified post processing to use batching\\njava\\n// in postpublicqueryserviceimpl.list()\\nreturn collectallusernames(posts)\\n    .flatmap(usernames -> contributorfinder.getuserservice().getusersorghostbynames(usernames))\\n    .collectmap(user -> user.getmetadata().getname())\\n    .flatmap(usercache -> processpostswithcache(posts, usercache));\\n3. cache-aware post conversion\\nadded overloaded methods that accept pre-fetched user data:\\njava\\nmono<listedpostvo> converttolistedvo(@nonnull post post, map<string, user> usercache);\\nperformance impact\\nbefore: 1 + n + m database queries\\nafter: 2 database queries (1 for posts, 1 for all users)\\nfor a homepage with 10 posts having 2 contributors each, this reduces queries from ~31 to 2 - a 93% reduction.\\nbackward compatibility\\n\\nall existing apis remain unchanged\\noriginal methods still work for individual post processing\\nnew batch methods are used internally for optimization\\nno breaking changes to external interfaces\\n\\ntesting\\n\\nall existing tests pass\\nadded validation for the new batch functionality\\nverified the optimization doesn't affect post display logic\\nmanual testing confirms user data is correctly populated\\n\\nfixes #7593\\n\\n[!warning]\\n\\nfirewall rules blocked me from connecting to one or more addresses\\ni tried to connect to the following addresses, but was blocked by firewall rules:\\n\\ns01.oss.sonatype.org\\ntriggering command: /usr/lib/jvm/temurin-17-jdk-amd64/bin/java --add-opens=java.base/java.lang=all-unnamed --add-opens=java.base/java.lang.invoke=all-unnamed --add-opens=java.base/java.util=all-unnamed --add-opens=java.prefs/java.util.prefs=all-unnamed --add-exports=jdk.compiler/com.sun.tools.javac.api=all-unnamed --add-exports=jdk.compiler/com.sun.tools.javac.util=all-unnamed --add-opens=java.base/java.util=all-unnamed --add-opens=java.prefs/java.util.prefs=all-unnamed --add-opens=java.base/java.nio.charset=all-unnamed --add-opens=java.base/java.net=all-unnamed --add-opens=java.base/java.util.concurrent.atomic=all-unnamed --add-opens=java.xml/javax.xml.namespace=all-unnamed -xx:maxmetaspacesize=384m -xx:&#43;heapdumponoutofmemoryerror -xms256m -xmx512m -dfile.encoding=utf-8 -duser.country -duser.language=en -duser.variant -cp /home/redacted/.gradle/wrapper/dists/gradle-8.14-bin/38aieal9i53h9rfe7vjup95b9/gradle-8.14/lib/gradle-daemon-main-8.14.jar -javaagent:/home/redacted/.gradle/wrapper/dists/gradle-8.14-bin/38aieal9i53h9rfe7vjup95b9/gradle-8.14/lib/agents/gradle-instrumentation-agent-8.14.jar org.gradle.launcher.daemon.bootstrap.gradledaemon 8.14 (dns block)\\n\\nif you need me to access, download, or install something from one of these locations, you can either:\\n\\nconfigure actions setup steps to set up my environment, which run before the firewall is enabled\\nadd the appropriate urls or hosts to the custom allowlist in this repository's  coding agent settings (admins only)\\n\\n\\n\\n start  coding agent tips \\n\\n you can make  smarter by setting up custom instructions, customizing its development environment and configuring model context protocol (mcp) servers. learn more  coding agent tips in the docs.\"]"
        ],
        [
         "21",
         "20",
         "10",
         "20_debounce_github_pull_requests_preview_docs",
         "['debounce', 'github', 'pull_requests', 'preview_docs', 'sync_playwright', 'plotly', 'github_workspace', 'javascript', 'debouncingfix', 'python']",
         "[\"fix lag in pyplot example by implementing proper debouncingfix lag in pyplot example by implementing proper debouncing\\ndescription\\nthis pr fixes lag in the pyplot example in the documentation by properly implementing debouncing using reflex's built-in mechanism. the implementation:\\n\\nuses the .debounce() method on event handlers to add a 1.2 second debounce delay\\nremoves unnecessary custom debouncing code with asyncio and time tracking\\nsimplifies the event handlers for better readability\\n\\ntesting\\n\\ntested locally with the development server\\nverified that the debouncing works correctly with the slider and randomize button\\ntested with different numbers of points to ensure smooth performance\\nconfirmed no errors in the console\\n\\nlink to  run:  <URL> \\nrequested by: alek petuskey (alek@reflex.dev)\", 'docs: upgrade docusaurus from 3.7.0 to 3.8.1this pr upgrades docusaurus dependencies from version 3.7.0 to 3.8.1 (latest stable) to improve performance, security, and provide access to the latest features and bug fixes.\\nchanges made\\n dependencies updated\\n\\n@docusaurus/core: 3.7.0 → 3.8.1\\n@docusaurus/plugin-content-blog: 3.7.0 → 3.8.1  \\n@docusaurus/plugin-google-gtag: 3.7.0 → 3.8.1\\n@docusaurus/preset-classic: 3.7.0 → 3.8.1\\n@docusaurus/theme-common: 3.7.0 → 3.8.1\\n@docusaurus/faster: 3.7.0 → 3.8.1\\n@docusaurus/types: 3.7.0 → 3.8.1\\n\\n configuration changes\\nadded required future.v4.removelegacypostbuildheadattribute: true flag to docusaurus.config.js:\\njavascript\\nfuture: {\\n    experimental_faster: true,\\n    v4: {\\n        removelegacypostbuildheadattribute: true,\\n    },\\n},\\nthis flag is required when using the experimental_faster feature in docusaurus 3.8.x and prepares the site for docusaurus v4 compatibility.\\ntesting\\n development server - starts and runs successfully\\n production build - initiates without configuration errors\\n cli commands - all docusaurus commands functional\\n site functionality - no breaking changes to existing features  \\nmigration notes\\nthe upgrade from 3.7.0 to 3.8.1 was largely non-breaking. the only configuration change required was adding the v4 future flag. no changes were needed to:\\n- sidebar configuration\\n- plugin configurations\\n- custom components\\n- content structure\\n- build scripts\\nthis upgrade provides improved performance and bug fixes while maintaining full backward compatibility with the existing documentation site.\\nbreaking changes addressed\\n\\nfuture flag requirement: docusaurus 3.8.x requires the v4.removelegacypostbuildheadattribute flag when using experimental_faster\\nlockfile updates: updated pnpm-lock.yaml to reflect new dependency versions\\n\\nthe upgrade is ready for production deployment with no additional migration steps required.\\n start  coding agent tips \\n\\n share your feedback on  coding agent for the chance to win a $200 gift card! click here to start the survey.', 'rewrite dev/preview_docs.py in javascript to simplify preview-docs workflowthis pr rewrites the dev/preview_docs.py python script in javascript to simplify the preview-docs workflow and eliminate unnecessary dependencies.\\nproblem\\nthe current preview-docs.yml workflow required both node.js (for netlify deployment) and python (for pr comment management), leading to:\\n- slower ci runs due to python environment setup\\n- additional dependency management (pip install requests)\\n- workflow complexity with multiple language environments\\nsolution\\nreplaced the python script with a javascript implementation that uses the github-script action:\\nchanges made\\n\\ncreated .github/workflows/preview-docs.js - javascript version with identical functionality\\nupdated .github/workflows/preview-docs.yml - replaced python script calls with github-script actions  \\nremoved dev/preview_docs.py - original python script no longer needed\\n\\nkey benefits\\n faster ci runs - eliminates setup-python action\\n fewer dependencies - no more pip install requests\\n simpler workflow - uses built-in github api access\\n consistency - follows existing javascript patterns in the repository\\n 100% compatibility - maintains same comment templates and behavior  \\ntechnical details\\nthe javascript implementation preserves all original functionality:\\n- same <!-- documentation preview --> marker system\\n- handles both \"completed\" and \"failed\" stages\\n- creates/updates pr comments with identical templates\\n- uses the same github api endpoints and error handling\\nbefore/after comparison\\nbefore:\\nyaml\\n- uses: ./.github/actions/setup-python\\n- name: install dependencies\\n  run: pip install requests\\n- name: create preview link\\n  run: |\\n    python dev/preview_docs.py \\\\\\n      --commit-sha ${{ github.event.workflow_run.head_sha }} \\\\\\n      --pull-number $pr_number \\\\\\n      # ... other args\\nafter:\\nyaml\\n- name: create preview link\\n  uses: actions/github-script@60a0d83039c74a4aee543508d2ffcb1c3799cdea\\n  env:\\n    commit_sha: ${{ github.event.workflow_run.head_sha }}\\n    pull_number: ${{ github.event.workflow_run.pull_requests[0].number }}\\n    # ... other env vars\\n  with:\\n    script: |\\n      const script = require(`${process.env.github_workspace}/.github/workflows/preview-docs.js`);\\n      await script({ context, github, env: process.env });\\nthis addresses the todo comment in the original python script:\\n```python\\ntodo: rewrite this in javascript so we don\\'t have to setup both node (to deploy to netlify)\\nand python (to upsert pr comments with this script)\\n```\\nfixes #16550.\\n start  coding agent tips \\n\\n share your feedback on  coding agent for the chance to win a $200 gift card! click here to start the survey.']"
        ],
        [
         "22",
         "21",
         "10",
         "21_optimized_blas_kernels_fp16_openmp_opencl_onnx",
         "['optimized_blas_kernels_fp16', 'openmp', 'opencl', 'onnx', 'optimizations', 'cpp', 'ggml_interface', 'speedup', 'vectorization', 'cuda']",
         "[\"[wip] improve dft implementationdft implementation improvements - completed\\n all optimizations implemented and validated\\n1. core algorithm optimizations\\n\\n[x] next_power_of_2 optimization: 1.87x speedup using bit manipulation\\n[x] threading integration: 7 parallel execution paths added\\n[x] memory efficiency: optimized buffer management and cache usage\\n[x] numerical stability: maintained precision with type-safe conversions\\n\\n2. threading architecture\\n\\n[x] smart thresholding: automatic sequential vs parallel decision making\\n[x] cost-based execution: thread overhead avoided for small workloads  \\n[x] multi-algorithm support: both radix-2 fft and bluestein's enhanced\\n[x] thread safety: safe parallel access to shared data structures\\n\\n3. performance characteristics\\nsmall sizes (< 256 elements):\\n- sequential execution (no threading overhead)\\n- immediate benefit from bit manipulation optimizations\\nlarge sizes (> 256 elements):\\n- up to 4x theoretical speedup on multi-core systems\\n- validated threading thresholds prevent overhead\\n- scalable performance with available cpu cores\\n4. implementation quality\\n\\n[x] backward compatibility: zero api changes, existing code unaffected\\n[x] cross-platform: uses ort threadpool for portability\\n[x] robust fallbacks: graceful degradation without thread pool\\n[x] comprehensive testing: added large-size and edge-case validation\\n\\n5. deliverables\\ncode changes:\\n- onnxruntime/core/providers/cpu/signal/dft.cc (228 additions, 57 deletions)\\n- onnxruntime/test/providers/cpu/signal/signal_ops_test.cc (added performance tests)\\ndocumentation:\\n- docs/dft_performance_improvements.md (comprehensive implementation guide)\\n- tools/dft_benchmark.cpp (performance validation utility)\\nvalidation results:\\n-  1.87x speedup in next_power_of_2 microbenchmarks\\n-  4x theoretical parallel speedup for large transforms\\n-  syntax validation and bracket matching verified\\n-  threading integration complete with proper fallbacks\\nimpact summary\\nthis implementation addresses the dft performance bottlenecks mentioned in github issue #21164 by:\\n\\nimmediate performance gains through algorithmic optimizations\\nscalable multi-core utilization for compute-intensive transforms  \\nzero breaking changes - existing models automatically benefit\\nfoundation for future gpu optimizations - clean architecture for extension\\n\\nthe improvements maintain onnx runtime's commitment to performance while preserving stability and compatibility across all deployment scenarios.\\ntechnical excellence\\n\\nminimal code changes: surgical improvements focused on performance bottlenecks\\nprofessional implementation: follows ort threading patterns and conventions  \\ncomprehensive validation: multiple test scenarios and performance benchmarks\\nproduction-ready: robust error handling and edge case coverage\\n\\nfixes #24522.\\n start  coding agent tips \\n\\n share your feedback on  coding agent for the chance to win a $200 gift card! click here to start the survey.\", \"optimize onnx referenceevaluator resize performance with vectorized numpy implementationproblem\\nthe onnx referenceevaluator resize operator had severe performance issues due to element-by-element iteration in the _interpolate_nd function. for output shape (1, 384, 40, 40), the operation took approximately 24 minutes to complete, making it unusable for practical applications.\\nthe root cause was the inefficient loop at line 377:\\npython\\nfor x in _get_all_coords(ret):\\n    ret[tuple(x)] = _interpolate_nd_with_x(...)\\nthis approach iterates through every output coordinate individually (614,400 iterations for the problematic case) instead of leveraging numpy's vectorization capabilities.\\nsolution\\nimplemented a vectorized numpy-based interpolation engine that provides massive performance improvements while maintaining full backward compatibility:\\nkey features:\\n\\n~7,400x speedup for the problematic case (24 minutes → 0.2 seconds)\\n100% correctness preserved - outputs match original implementation exactly\\nintelligent fallback system - complex cases automatically use original implementation\\nzero breaking changes - existing code continues to work unchanged\\npure numpy implementation - no external dependencies added\\n\\nimplementation details:\\nnew functions added:\\n- _interpolate_nd_vectorized(): main entry point with smart linear interpolation detection\\n- _interpolate_nd_numpy_vectorized(): core vectorized interpolation engine\\n- _interpolate_2d_vectorized() & _interpolate_4d_vectorized(): optimized fast paths for common cases\\n- _interpolate_nd_original(): preserved original implementation for fallback\\nvectorization strategy:\\n- uses np.meshgrid() to generate coordinate grids efficiently\\n- applies coordinate transformations vectorially across all output points\\n- implements multilinear interpolation using numpy broadcasting\\n- handles 2d and 4d tensors with specialized optimized code paths\\nfallback logic:\\nthe optimization only applies to linear interpolation with simple coordinate transformations. complex cases automatically fall back to the original implementation:\\n- non-linear modes (nearest, cubic)\\n- roi-based resizing\\n- exclude_outside parameter\\n- complex coordinate transformation modes\\nperformance results:\\n| case | original time | optimized time | speedup |\\n|------|---------------|----------------|---------|\\n| (1, 16, 20, 20) → (1, 32, 40, 40) | ~5.1 seconds | 0.016 seconds | ~320x |\\n| (1, 384, 40, 40) | ~24 minutes | ~0.2 seconds | ~7,400x |\\nprocessing rate: 3+ million elements per second\\ntesting:\\n\\n correctness verified across multiple tensor dimensions\\n fallback behavior tested for all interpolation modes  \\n performance improvements confirmed in realistic scenarios\\n backward compatibility maintained\\n\\nthe optimization specifically targets the performance bottleneck while preserving all existing functionality and ensuring seamless integration.\\nfixes #6554.\\n\\n[!warning]\\n\\nfirewall rules blocked me from connecting to one or more addresses\\ni tried to connect to the following addresses, but was blocked by firewall rules:\\n\\nesm.ubuntu.com\\ntriggering command: /usr/lib/apt/methods/ <URL>  (dns block)\\n\\nif you need me to access, download, or install something from one of these locations, you can either:\\n\\nconfigure actions setup steps to set up my environment, which run before the firewall is enabled\\nadd the appropriate urls or hosts to my firewall allow list\\n\\n\\n\\n start  coding agent tips \\n\\n you can make  smarter by setting up custom instructions, customizing its development environment and configuring model context protocol (mcp) servers. learn more  coding agent tips in the docs.\", \"/inspect results of ggml_interface.cppthis pr is created by . @skykongkong8 needs to carefully review the commits.\\ndo not merge before @skykongkong8 's confirm.\\n@skykongkong8 please review this and update it. my prompt does not create code following the given style requirement, yet.\\nggml interface performance optimization summary\\ntarget file: nntrainer/tensor/cpu_backend/ggml_interface/ggml_interface.cpp\\nanalysis date: january 2025\\ntarget architectures: arm v9, x64 i5/i7 processors  \\n executive summary\\nthis document outlines critical performance optimizations applied to the ggml interface in nntrainer, focusing on three core areas that collectively provide 3-5x overall performance improvement across arm v9 and x64 processors.\\n performance impact overview\\n| optimization | arm v9 improvement | x64 i5/i7 improvement | memory impact |\\n|--------------|-------------------|----------------------|---------------|\\n| thread pool | 30-50% latency reduction | 35-45% latency reduction | no change |\\n| memory pool | 40-50% allocation overhead reduction | 45-55% allocation overhead reduction | 40-50% reduction |\\n| simd quantization | 200-400% quantization speedup | 300-500% quantization speedup | no change |\\n| combined effect | 3-4x overall improvement | 4-5x overall improvement | 40-50% memory reduction |\\n critical performance issues identified\\n1. thread pool implementation bottleneck\\n\\nissue: using openmp instead of available bs::thread_pool\\nimpact: 50-100μs overhead per gemm operation\\nroot cause: static thread allocation and poor work distribution\\nfrequency: every matrix operation (high frequency)\\n\\n2. memory allocation pattern inefficiency\\n\\nissue: frequent std::vector allocations in hot paths\\nimpact: 2-3x higher memory usage and allocation overhead\\nroot cause: no memory reuse strategy for quantization buffers\\nfrequency: every quantization operation (very high frequency)\\n\\n3. missing simd optimization\\n\\nissue: sequential quantization without vectorization\\nimpact: 3-5x slower than simd-optimized implementations\\nroot cause: no architecture-specific optimizations\\nfrequency: all quantization operations (critical path)\\n\\n implemented optimizations\\noptimization 1: advanced thread pool management\\nchanges made:\\n\\nreplaced all openmp #pragma directives with bs::thread_pool\\nimplemented adaptive thread count based on problem size\\nadded cache-line aligned work distribution\\nintroduced dynamic load balancing\\n\\ntechnical details:\\n```cpp\\n// before: fixed openmp threads\\npragma omp parallel for num_threads(4)\\n// after: adaptive bs thread pool\\nconst unsigned int n_threads = std::min(4u, std::max(1u, n / 64));\\nauto &bspool = threadpoolmanager::getinstance();\\nbs::multi_future multi_future = bspool.submit_loop(0, n, & {\\n    // optimized work with cache alignment\\n});\\n```\\nperformance gains:\\n\\narm v9: 30-50% latency reduction\\nx64: 35-45% latency reduction  \\nthread overhead: reduced from 50-100μs to <10μs per operation\\n\\noptimization 2: high-performance memory pool\\nchanges made:\\n\\nimplemented quantizationbufferpool singleton\\ncreated pooledbuffer raii wrapper\\nreplaced all std::vector with pooled allocations\\nadded cache-line alignment (64-byte boundaries)\\n\\ntechnical details:\\n```cpp\\n// before: frequent allocations\\nstd::vector qa = std::vector(qa_size);\\n// after: pooled memory management\\npooledbuffer qa(qa_size);  // automatic reuse and alignment\\n```\\nkey features:\\n\\ncache-line alignment: 64-byte boundaries for optimal cpu cache usage\\nconfigurable pool size: max 8 cached buffers per size class\\nthread-safe: mutex-protected buffer management\\nraii management: automatic return to pool on destruction\\n\\nperformance gains:\\n\\nmemory allocation overhead: 40-50% reduction\\nmemory fragmentation: significantly reduced\\ncache performance: improved due to alignment\\n\\noptimization 3: simd-accelerated quantization\\nchanges made:\\n\\ncreated ggml_simd_quant.h with runtime cpu detection\\nimplemented arm neon optimized quantization functions\\nimplemented x64 avx2 optimized quantization functions  \\nadded runtime dispatch with fallback support\\n\\ntechnical details:\\narm neon implementation:\\ncpp\\n// vectorized absolute maximum finding\\nfloat32x4_t max_vec = vdupq_n_f32(0.0f);\\nfor (int j = 0; j < qk_k; j += 16) {\\n    float32x4_t v0 = vld1q_f32(x + j);\\n    v0 = vabsq_f32(v0);\\n    max_vec = vmaxq_f32(max_vec, v0);\\n}\\nx64 avx2 implementation:\\ncpp\\n// 256-bit vector operations\\n__m256 max_vec = _mm256_setzero_ps();\\nfor (int j = 0; j < qk_k; j += 32) {\\n    __m256 v0 = _mm256_loadu_ps(x + j);\\n    v0 = _mm256_andnot_ps(sign_mask, v0);  // abs\\n    max_vec = _mm256_max_ps(max_vec, v0);\\n}\\nruntime dispatch:\\n```cpp\\ninline void quantize_row_q8_k_optimized(const float src, void dst, int64_t k) {\\n    const auto& features = cpufeatures::getinstance();\\nif (features.has_avx2) {\\n    quantize_row_q8_k_avx2(src, dst, k);\\n} else if (features.has_neon) {\\n    quantize_row_q8_k_neon(src, dst, k);\\n} else {\\n    ::quantize_row_q8_k(src, dst, k);  // fallback\\n}\\n\\n}\\n```\\nperformance gains:\\n\\narm neon: 200-400% quantization speedup\\nx64 avx2: 300-500% quantization speedup\\ncompatibility: full fallback support for unsupported architectures\\n\\n benchmarking results\\ngemv operations (m=1)\\n| architecture | before (ms) | after (ms) | improvement |\\n|--------------|-------------|------------|-------------|\\n| arm v9 (4096x4096) | 8.5 | 4.2 | 2.0x faster |\\n| x64 i5 (4096x4096) | 6.8 | 3.1 | 2.2x faster |\\n| x64 i7 (4096x4096) | 5.9 | 2.6 | 2.3x faster |\\ngemm operations (m>1)\\n| architecture | before (ms) | after (ms) | improvement |\\n|--------------|-------------|------------|-------------|\\n| arm v9 (1024x1024) | 45.2 | 11.8 | 3.8x faster |\\n| x64 i5 (1024x1024) | 38.6 | 8.2 | 4.7x faster |\\n| x64 i7 (1024x1024) | 32.1 | 6.9 | 4.7x faster |\\nmemory usage\\n| operation | before (mb) | after (mb) | reduction |\\n|-----------|-------------|------------|-----------|\\n| large model inference | 2.4 | 1.3 | 46% reduction |\\n| quantization buffers | 0.8 | 0.4 | 50% reduction |\\n code quality improvements\\nthread safety\\n\\nbefore: openmp threads with potential race conditions\\nafter: bs::thread_pool with proper synchronization and futures\\n\\nmemory management\\n\\nbefore: manual std::vector allocation/deallocation\\nafter: raii-based pooledbuffer with automatic lifecycle management\\n\\narchitecture support\\n\\nbefore: single scalar implementation\\nafter: multi-architecture with runtime detection and optimal dispatch\\n\\nmaintainability\\n\\nbefore: scattered openmp pragmas throughout code\\nafter: centralized thread pool management and clean simd abstractions\\n\\n implementation architecture\\nthread pool architecture\\nthreadpoolmanager (singleton)\\n bs::thread_pool instance\\n adaptive thread count calculation  \\n cache-line aligned work distribution\\n future-based synchronization\\nmemory pool architecture\\nquantizationbufferpool (singleton)\\n size-based buffer pools (unordered_map)\\n cache-line aligned allocations (64-byte)\\n thread-safe buffer management (mutex)\\n configurable pool limits (8 buffers/size)\\nsimd architecture\\nruntime cpu detection\\n arm neon support detection\\n x64 avx2 support detection\\n optimal function dispatch\\n fallback compatibility\\n technical deep dive\\ncache-line optimization\\n\\nalignment: all buffers aligned to 64-byte boundaries\\naccess pattern: sequential access optimized for cpu prefetchers\\nwork distribution: thread work blocks aligned to cache lines\\n\\nsimd instruction utilization\\n\\narm neon: uses 128-bit vectors (4x float32 or 8x float16)\\nx64 avx2: uses 256-bit vectors (8x float32)\\nthroughput: near-theoretical peak simd performance\\n\\nthread pool scalability\\n\\ndynamic adaptation: thread count scales with problem size\\nload balancing: work distributed to avoid thread starvation\\nmemory hierarchy: considers l1/l2/l3 cache sizes\\n\\n validation and testing\\ncorrectness verification\\n\\n all optimized functions produce identical results to reference implementation\\n floating-point precision maintained within acceptable tolerances\\n cross-platform compatibility verified\\n\\nperformance testing\\n\\n benchmarked on arm v9 (cortex-a78) processors\\n benchmarked on x64 i5-12600k and i7-12700k processors\\n tested across various matrix sizes (64x64 to 8192x8192)\\n\\nstress testing\\n\\n extended runs (24+ hours) without memory leaks\\n multi-threaded stress testing with concurrent operations\\n memory pool exhaustion and recovery testing\\n\\n recommendations for future optimization\\nshort-term (next release)\\n\\ngpu acceleration: implement opencl/cuda versions for large matrices\\nfp16 support: add half-precision floating-point simd optimizations\\nadvanced prefetching: implement software prefetching for better cache utilization\\n\\nmedium-term (6 months)\\n\\ncustom gemm kernels: develop highly optimized matrix multiplication kernels\\nmemory compression: implement lz4/snappy compression for stored quantized weights\\ndynamic profiling: add runtime performance monitoring and adaptive optimization\\n\\nlong-term (1 year)\\n\\nmachine learning optimization: use ml to predict optimal thread counts and work distribution\\nhardware-specific tuning: develop processor-specific optimization profiles\\ndistributed computing: enable multi-node gemm operations for very large matrices\\n\\n cost-benefit analysis\\ndevelopment investment\\n\\nimplementation time: 40 engineer-hours\\ntesting and validation: 20 engineer-hours\\ncode review and documentation: 10 engineer-hours\\ntotal investment: 70 engineer-hours\\n\\nperformance return\\n\\nuser experience: 3-5x faster neural network inference\\npower efficiency: 30-40% reduction in cpu utilization\\nmemory efficiency: 40-50% reduction in memory usage\\nscalability: better performance on high-core-count systems\\n\\nmaintenance overhead\\n\\nongoing: minimal (self-contained optimizations)\\ntesting: included in existing ci/cd pipeline\\ndocumentation: comprehensive inline documentation provided\\n\\n risk assessment and mitigation\\nidentified risks\\n\\nplatform compatibility: simd code may not work on all architectures\\nmitigation: comprehensive fallback implementations\\n\\ntesting: multi-architecture ci/cd validation\\n\\n\\nnumerical precision: simd operations may introduce floating-point differences\\n\\nmitigation: extensive precision testing and tolerance validation\\n\\nmonitoring: continuous integration checks for numerical stability\\n\\n\\nmemory pool fragmentation: pool may become fragmented with varied buffer sizes\\n\\nmitigation: size-based pools with configurable limits\\nmonitoring: pool utilization metrics and cleanup algorithms\\n\\nrisk probability and impact\\n| risk | probability | impact | mitigation effectiveness |\\n|------|-------------|---------|-------------------------|\\n| platform issues | low | medium | high (fallback code) |\\n| precision issues | very low | high | high (extensive testing) |\\n| memory fragmentation | low | low | medium (monitoring needed) |\\n success metrics\\nperformance kpis\\n\\n latency reduction: target 30-50% → achieved 30-50%\\n throughput increase: target 3-5x → achieved 3-5x \\n memory efficiency: target 40% reduction → achieved 40-50%\\n\\nquality kpis\\n\\n zero regressions: no functionality or accuracy loss\\n maintainability: clean, well-documented code structure\\n compatibility: works across all target platforms\\n\\nuser impact kpis\\n\\n inference speed: real-world model inference 3-5x faster\\n battery life: mobile devices see 30-40% battery improvement\\n scalability: better performance on multi-core systems\\n\\n conclusion\\nthe implemented optimizations successfully address the three critical performance bottlenecks in the ggml interface:\\n\\nthread management: eliminated openmp overhead with adaptive bs::thread_pool\\nmemory efficiency: implemented high-performance pooled allocation system  \\ncomputational performance: added architecture-specific simd optimizations\\n\\nthe 3-5x overall performance improvement makes neural network inference significantly more practical on both arm v9 and x64 processors, while maintaining full backward compatibility and code quality standards.\\nthese optimizations provide a solid foundation for future enhancements and position the ggml interface as a high-performance, production-ready component for neural network acceleration.\\n\\ndocument version: 1.0\\nlast updated: january 2025\\nauthor: performance optimization team\\nreview status:  approved for implementation\"]"
        ],
        [
         "23",
         "22",
         "10",
         "22_testgocompiler_goldenoutput_compiler_compilersummary_refactor",
         "['testgocompiler_goldenoutput', 'compiler', 'compilersummary', 'refactor', 'cgo', 'compile', 'codegen', 'testgocompiler_subsetprograms', 'testgocompiler_validprograms', 'dockerfile']",
         "['improve go compiler struct reusesummary\\n\\nenhance go compiler struct inference so existing structs are reused\\nadd structmatches helper\\nupdate checklists for tpch progress\\nregenerate machine output for cast_struct example\\n\\ntesting\\n\\ngo test ./compiler/x/go -tags slow -run testgocompiler_validprograms\\n\\n\\n <URL> ', 'optimize case builtins in go backendsummary\\n\\navoid runtime helpers for lower and upper when argument type is known\\nadd regression tests covering the new optimization\\n\\ntesting\\n\\ngo test ./compile/go -run testgocompiler_subsetprograms/upper_builtin -update -tags slow\\ngo test ./compile/go -run testgocompiler_subsetprograms/lower_builtin -update -tags slow\\ngo test ./compile/go -run testgocompiler_goldenoutput/upper_builtin -update -tags slow\\ngo test ./compile/go -run testgocompiler_goldenoutput/lower_builtin -update -tags slow\\ngo test ./...\\n\\n\\n <URL> ', 'n-api go html-to-markdownsummary\\n\\nreplace unstable koffi ffi with robust n-api implementation\\nadd hybrid fallback system: n-api → koffi → javascript\\nintegrate n-api build into docker and ci pipeline\\n\\nkey features\\n\\n memory safe: eliminates cgo mutex deadlocks and corruption\\n high performance: direct c++ interface, no ffi overhead\\n thread safe: built-in n-api thread safety mechanisms\\n zero config: automatic fallback if modules unavailable\\n compatible: drop-in replacement for existing parsemarkdown()\\n\\ntechnical details\\n\\ngo static library with timeout protection (30s)\\nc++ n-api wrapper with sync/async interfaces\\nmulti-stage docker build for automated compilation\\ncomprehensive test suite and validation scripts\\nsmart module loading with graceful degradation\\n\\nfiles added\\n\\nsharedlibs/go-html-to-md-napi/ - complete n-api module\\nvalidate-html-conversion.js - integration test suite\\nupdated dockerfile with n-api build stage\\nhybrid html-to-markdown.ts with intelligent fallback\\n\\nmigration path\\n\\nn-api module loads automatically if available\\nfalls back to existing koffi implementation\\nfinal fallback to javascript turndownservice\\nzero breaking changes to existing code\\n\\nthis resolves the koffi-related runtime panics and provides a stable,\\nhigh-performance html-to-markdown conversion system.\\n generated with  code\\nco-authored-by:  noreply@anthropic.com\\n this is an auto-generated description by cubic. \\n\\nsummary by cubic\\nreplaced the unstable koffi ffi html-to-markdown integration with a new n-api go module, adding a hybrid fallback system (n-api → koffi → javascript) and updating the docker and ci build to support the new module.\\n\\nnew features\\nadded a memory-safe, thread-safe n-api wrapper for the go html-to-markdown library with both sync and async interfaces.\\nautomatic fallback to koffi or javascript if the n-api module is unavailable.\\nintegrated n-api build and validation into docker and ci.\\nincluded a test suite and validation script to ensure conversion reliability.\\n\\n end of auto-generated description by cubic.']"
        ],
        [
         "24",
         "23",
         "9",
         "23_rustc_hash_rustc_hashing_lib_rs",
         "['rustc_hash', 'rustc', 'hashing', 'lib_rs', 'hash', 'fxhashset', 'rust', 'fxhashmap', 'fxhasher', 'hashset']",
         "['add database caching for folder scan results to improve performance / feature overview\\n\\nimplemented database caching functionality where the first folder scan creates a database, subsequent scans prioritize reading from the database, and any changes are written back to the database.\\n / key changes\\n database integration\\n\\nadded sqlite database support with rusqlite dependency\\ncreated comprehensive database module (src/database.rs) with crud operations\\nautomatic database schema creation and initialization\\ndatabase file automatically added to .gitignore\\n\\n smart scanning logic\\n\\nfirst scan: full filesystem scan + database creation\\nsubsequent scans: load from cache → check for changes → update database\\nperformance improvement: ~10-100x faster loading from cache\\nintelligent change detection and incremental updates\\n\\n enhanced scanner (src/scanner.rs)\\nrust\\n// new intelligent scanning flow:\\n1. check if database exists and has cached data\\n2. load cached data first (instant ui update)\\n3. perform filesystem scan in background\\n4. compare and detect changes\\n5. update database with changes only\\n6. clean up deleted folder records\\n ui improvements (src/tabs/clear_tab.rs)\\n\\ndatabase status indicator showing record count and last update\\nreal-time status messages: \"...\" → \"...\" → \"\"\\nvisual feedback for database operations\\n\\n / user experience\\n / first use\\n\\nnormal filesystem scan behavior\\ncreates appdata_cleaner.db automatically\\nno user action required\\n\\n / subsequent use\\n\\ninstant loading of cached folder data\\nbackground verification for changes\\nseamless user experience with improved performance\\n\\n / technical details\\ndatabase schema\\nsql\\ncreate table folder_scans (\\n    id integer primary key autoincrement,\\n    folder_type text not null,          -- roaming/local/locallow\\n    folder_name text not null,\\n    folder_size integer not null,\\n    last_modified text not null,\\n    created_at text not null,\\n    updated_at text not null,\\n    unique(folder_type, folder_name)\\n);\\nperformance optimizations\\n\\nbatch database operations for better performance\\nindexed queries for fast lookups\\nminimal memory footprint\\nautomatic cleanup of stale records\\n\\n / testing\\n\\n comprehensive unit tests for database operations\\n integration tests for scanner functionality\\n backward compatibility verified\\n error handling and fallback mechanisms tested\\n\\n / compatibility\\n\\nbackward compatible: works seamlessly without existing database\\nerror resilient: falls back to filesystem scan if database issues occur\\nno breaking changes: existing functionality unchanged\\ncross-platform: sqlite bundled for all platforms\\n\\nfiles changed\\n\\ncargo.toml - added rusqlite dependency\\nsrc/database.rs - new database module (complete implementation)\\nsrc/scanner.rs - enhanced with caching logic\\nsrc/tabs/clear_tab.rs - ui improvements and status handling\\nsrc/main.rs - added database module import\\n.gitignore - added database files to ignore list\\n\\nresolves the issue by implementing efficient database caching that dramatically improves scan performance while maintaining full compatibility with existing functionality.\\n start  coding agent tips \\n\\n you can make  smarter by setting up custom instructions, customizing its development environment and configuring model context protocol (mcp) servers. learn more  coding agent tips in the docs.', 'refactor unicharset script storage to use hash map instead of raw arraysummary\\nthis pr refactors the script storage mechanism in the unicharset class to replace manual memory management with modern c++ stl containers, improving performance, memory safety, and maintainability.\\nproblem\\nthe original implementation used a raw char** array with manual memory management:\\n- char **script_table - array of c-style strings\\n- int script_table_size_used - current number of scripts\\n- int script_table_size_reserved - allocated capacity\\n- manual new/delete[] operations with potential memory leaks\\n- o(n) linear search in get_script_id_from_name() \\n- complex manual array resizing logic\\nsolution\\nreplaced raw arrays with stl containers:\\n- std::unordered_map<std::string, int> script_name_to_id_ - for o(1) name→id lookup\\n- std::vector<std::string> script_names_ - for o(1) id→name reverse lookup\\nkey improvements:\\n- performance: script lookup is now o(1) hash map lookup instead of o(n) linear search\\n- memory safety: automatic memory management eliminates potential leaks and double-free errors\\n- exception safety: stl containers provide strong exception safety guarantees\\n- maintainability: cleaner, simpler code using standard data structures\\n- api compatibility: all public methods maintain identical signatures and behavior\\nchanges made\\nheader file (src/ccutil/unicharset.h):\\n\\nadded includes for <unordered_map> and <vector>\\nreplaced manual script storage variables with stl containers\\nupdated inline method implementations for get_script_table_size() and get_script_from_script_id()\\nsimplified clear() method to use container methods\\nupdated comments to reflect o(1) performance characteristics\\n\\nsource file (src/ccutil/unicharset.cpp):\\n\\nsimplified constructor (removed manual script table initialization)\\nrewrote add_script() to use hash map for uniqueness and vector for storage\\nrewrote get_script_id_from_name() to use hash map lookup\\nupdated post_load_setup() to work with vector size\\n\\ntesting\\ncomprehensive testing was performed to ensure:\\n-  all existing unicharset functionality works unchanged\\n-  script uniqueness is preserved\\n-  forward and reverse lookups work correctly\\n-  performance scales well with 45+ scripts\\n-  edge cases (invalid ids, non-existent scripts) handled properly\\n-  memory cleanup works correctly with clear()\\n-  full library builds and links successfully\\nbackward compatibility\\nthis is a pure refactoring with no breaking changes:\\n- all public method signatures remain identical\\n- all method behaviors remain the same\\n- script id assignment order is preserved\\n- existing code continues to work without modification\\nthe change is completely internal to the unicharset implementation and invisible to users of the class.\\n\\n[!warning]\\n\\nfirewall rules blocked me from connecting to one or more addresses\\ni tried to connect to the following addresses, but was blocked by firewall rules:\\n\\nesm.ubuntu.com\\ntriggering command: /usr/lib/apt/methods/ <URL>  (dns block)\\n\\nif you need me to access, download, or install something from one of these locations, you can either:\\n\\nconfigure actions setup steps to set up my environment, which run before the firewall is enabled\\nadd the appropriate urls or hosts to my firewall allow list\\n\\n\\n\\n start  coding agent tips \\n\\n you can make  smarter by setting up custom instructions, customizing its development environment and configuring model context protocol (mcp) servers. learn more  coding agent tips in the docs.', 'feat: rewrite sitemap xml parsing from javascript to rust (eng-2904)feat: rewrite sitemap xml parsing from javascript to rust (eng-2904)\\nsummary\\nthis pr replaces the xml parsing logic in getlinksfromsitemap from javascript (using xml2js) to rust (using roxmltree), while keeping network requests and recursive processing in javascript as requested. the change aims to improve performance of sitemap parsing operations.\\nkey changes:\\n- added roxmltree dependency for xml parsing in rust\\n- implemented parse_sitemap_xml rust function with ffi wrapper\\n- created parsesitemapxml wrapper function following existing filterlinks pattern\\n- updated sitemap processing to use rust function instead of xml2js\\n- maintains exact compatibility with existing data structures (urlset/sitemapindex format)\\nreview & testing checklist for human\\n\\n[x] end-to-end sitemap crawling verification: test both sitemap index files and regular sitemaps to ensure no functional regressions\\n[x] data structure compatibility: verify that the rust function returns identical json structure to xml2js.parsestringpromise() \\n[x] error handling: test with malformed xml inputs to ensure proper error propagation from rust to javascript\\n[x] build process: verify rust compilation works in ci environment (requires nightly toolchain for edition 2024)\\n[x] performance validation: compare sitemap processing performance before/after to confirm expected improvements\\n\\nrecommended test plan:\\n1. test crawling a site with sitemap index (nested sitemaps)\\n2. test crawling a site with regular sitemap (direct url list)\\n3. test error scenarios (malformed xml, network timeouts)\\n4. verify memory usage and performance under load\\n\\ndiagram\\n```mermaid\\n%%{ init : { \"theme\" : \"default\" }}%%\\ngraph tb\\n    subgraph \"apps/api\"\\n        sitemap[\"apps/api/src/scraper/webscraper/sitemap.ts\"]:::major-edit\\n        crawler_ts[\"apps/api/src/lib/crawler.ts\"]:::major-edit\\n    end\\nsubgraph \"rust crawler\"\\n    cargo[\"apps/api/sharedlibs/crawler/<br/>cargo.toml\"]:::minor-edit\\n    lib_rs[\"apps/api/sharedlibs/crawler/<br/>src/lib.rs\"]:::major-edit\\nend\\n\\nsubgraph \"dependencies\"\\n    xml2js[\"xml2js<br/>(removed)\"]:::context\\n    roxmltree[\"roxmltree<br/>(added)\"]:::context\\nend\\n\\nsitemap -->|\"calls parsesitemapxml()\"| crawler_ts\\ncrawler_ts -->|\"ffi call\"| lib_rs\\nlib_rs -->|\"uses\"| roxmltree\\nsitemap -.->|\"previously used\"| xml2js\\ncargo -->|\"defines\"| roxmltree\\n\\nsubgraph legend\\n    l1[major edit]:::major-edit\\n    l2[minor edit]:::minor-edit  \\n    l3[context/no edit]:::context\\nend\\n\\nclassdef major-edit fill:#90ee90\\nclassdef minor-edit fill:#87ceeb\\nclassdef context fill:#ffffff\\n\\n```\\nnotes\\n\\ncritical: this change requires rust nightly toolchain due to edition 2024 usage\\nmemory safety: ffi implementation follows existing filter_links pattern for proper memory management\\nno tests added: per user request, no new tests were created - relies on existing test suite and manual verification\\nbackward compatibility: maintains exact same function signature and return format as original implementation\\n\\nlink to  run:  <URL> \\nrequested by: mogery@sideguide.dev\\n this is an auto-generated description by cubic. \\n\\nsummary by cubic\\nrewrote sitemap xml parsing from javascript to rust to improve performance, while keeping the output format and api unchanged.\\n\\ndependencies\\nreplaced the xml2js javascript library with the roxmltree rust crate for xml parsing.\\n\\n end of auto-generated description by cubic.']"
        ],
        [
         "25",
         "24",
         "9",
         "24_getbusytimesfromteamlimitsforusers__getbusytimesfromteamlimits_refactors_bookingrepository",
         "['getbusytimesfromteamlimitsforusers', '_getbusytimesfromteamlimits', 'refactors', 'bookingrepository', 'versionoptimize', 'checkbookinglimits', 'batch', 'checkbookinglimit', 'perf', 'getbusytimesfromlimits']",
         "['perf: optimize team bookings query by fetching data for multiple users at onceoptimize team bookings query and busy times limits\\nthis pr optimizes the team bookings query and busy times limits by fetching data for multiple users at once, rather than making separate database calls for each user.\\nchanges\\n\\nadded a new getallacceptedteambookingsofusers function in bookingrepository that accepts multiple users\\ncreated a new getbusytimesfromteamlimitsforusers function in util.ts that processes team booking limits for multiple users\\nadded a new getbusytimesfromlimitsforusers function in util.ts that processes booking and duration limits for multiple users\\nmoved the condition checks from getuseravailability.ts to util.ts\\nupdated the getuseravailabilityinitialdata type to include teambookinglimits, teamforbookinglimits, busytimesfromlimits, and eventtypeforlimits properties\\nmodified the _getuseravailability function to use the batch-loaded data from initialdata when available\\n\\nbenefits\\n\\nreduces the number of database queries by fetching team bookings and busy times once for multiple users\\nimproves performance by avoiding redundant database calls\\nmaintains the same functionality while optimizing query execution\\nparticularly beneficial for team and collective scheduling types with many members\\n\\ntesting\\n\\nverified that all type checks pass with yarn type-check:ci\\n\\nlink to  run:  <URL> \\nrequested by: keith@cal.com', \"perf: optimize team bookings query by using batch versionoptimize team bookings query by using batch version\\nwhat's being changed and why\\nthis pr addresses a database performance issue by updating two locations in the web app code that were still using the single-user version of bookingrepository.getallacceptedteambookingsofuser instead of the batch version bookingrepository.getallacceptedteambookingsofusers that was introduced in pr #21137.\\nthe problematic sql query was causing database performance issues when checking team booking limits. by using the batch version of the repository function, we can reduce the number of database queries and improve performance.\\nlocations updated:\\n\\npackages/lib/intervallimits/server/getbusytimesfromlimits.ts - updated _getbusytimesfromteamlimits function to use the batch version\\npackages/lib/intervallimits/server/checkbookinglimits.ts - updated checkbookinglimit function to use the batch version\\n\\ntesting\\n\\ntype checking passes with yarn type-check:ci\\nthe changes maintain the same functionality while improving database performance\\n\\nlink to  run:  <URL> \\nrequested by: keith@cal.com\\n this is an auto-generated description by mrge. \\n\\nsummary by mrge\\noptimized team bookings queries by switching to the batch version of the repository function, reducing database load and improving performance. now, team booking limits for multiple users are checked in a single query instead of one per user.\\n end of auto-generated description by mrge.\", \"perf: optimize team bookings query by using batch versionoptimize team bookings query by using batch version\\nwhat's being changed and why\\nthis pr addresses a database performance issue by updating two locations in the web app code that were still using the single-user version of bookingrepository.getallacceptedteambookingsofuser instead of the batch version bookingrepository.getallacceptedteambookingsofusers that was introduced in pr #21137.\\nthe problematic sql query was causing database performance issues when checking team booking limits. by using the batch version of the repository function, we can reduce the number of database queries and improve performance.\\nlocations updated:\\n\\npackages/lib/intervallimits/server/getbusytimesfromlimits.ts - updated _getbusytimesfromteamlimits function to use the batch version\\npackages/lib/intervallimits/server/checkbookinglimits.ts - updated checkbookinglimit function to use the batch version\\n\\ntesting\\n\\ntype checking passes with yarn type-check:ci\\nthe changes maintain the same functionality while improving database performance\\n\\nlink to  run:  <URL> \\nrequested by: keith@cal.com\\n this is an auto-generated description by mrge. \\n\\nsummary by mrge\\noptimized team bookings queries by switching to the batch version, reducing database load and improving performance.\\n\\nrefactors\\nreplaced single-user team bookings queries with batch queries in booking limits and busy times logic.\\n\\n end of auto-generated description by mrge.\"]"
        ],
        [
         "26",
         "25",
         "9",
         "25_repo_git_commit_build_source_files",
         "['repo', 'git', 'commit', 'build_source_files', 'release', 'dev', 'speedsummary', 'changessee', 'workflow', 'releaseref']",
         "['optimize source file tree buildingsummary\\n\\navoid quadratic path lookups in wf_project.build_source_files\\n\\ntesting\\n\\npip install -e . (fails: could not find a version that satisfies the requirement poetry-core)\\n\\n this is an auto-generated comment: release notes by coderabbit.ai \\nsummary by coderabbit\\n\\nbug fixes\\nimproved reliability when handling file paths that contain duplicate directory names, ensuring correct directory checks in all cases.\\n\\n\\n\\n end of auto-generated comment: release notes by coderabbit.ai', 'perf: optimize shared package rebuilds for dev speed b069ellipsis_hidden \\n\\n[!important]\\noptimizes development build speed by removing unnecessary dependencies in turbo.json and updates contributing.md with a note on retrying initial setup command.\\n\\nperformance optimization:\\nremoved @langfuse/shared#build dependency from dev, dev:worker, and dev:web tasks in turbo.json to optimize rebuild speed.\\n\\n\\ndocumentation:\\nupdated contributing.md to note that the initial run of pnpm run dx may fail and should be retried.\\n\\n\\n\\nthis description was created by  for 33551ba272d0264eb1f2cdb7f01aa18e69959373. you can customize this summary. it will automatically update as commits are pushed.\\n\\n ellipsis_hidden', 'ci: skip expensive workflows on doc-only changessee also  <URL> \\n\\nlegal gdpr notice:\\naccording to the european data protection laws (gdpr), we would like to make you\\naware that contributing to rsyslog via git will permanently store the\\nname and email address you provide as well as the actual commit and the\\ntime and date you made it inside git\\'s version history. this is inevitable,\\nbecause it is a main feature git. if you are concerned about your\\nprivacy, we strongly recommend to use\\n\\n--author \"anonymous <gdpr@example.com>\"\\n\\ntogether with your commit. also please do not sign your commit in this case,\\nas that potentially could lead back to you. please note that if you use your\\nreal identity, the gdpr grants you the right to have this information removed\\nlater. however, we have valid reasons why we cannot remove that information\\nlater on. the reasons are:\\n\\n* this would break git history and make future merges unworkable\\n* the rsyslog projects has legitimate interest to keep a permanent record of the\\n  contributor identity, once given, for\\n  - copyright verification\\n  - being able to provide proof should a malicious commit be made\\n\\nplease also note that your commit is public and as such will potentially be\\nprocessed by many third-parties. git\\'s distributed nature makes it impossible\\nto track where exactly your commit, and thus your personal data, will be stored\\nand be processed. if you would not like to accept this risk, please do either\\ncommit anonymously or refrain from contributing to the rsyslog project.']"
        ],
        [
         "27",
         "26",
         "8",
         "26_calendarcache_cachedcalendarservice_getcachedcalendar_calendarservice",
         "['calendarcache', 'cachedcalendarservice', 'getcachedcalendar', 'calendarservice', 'googlecalendar', 'calendar', 'cache', 'calendars', 'selectedcalendars', 'largecalendar']",
         "['feat: optimize calendar cache retrieval with cachedcalendarserviceoptimize calendar cache retrieval with cachedcalendarservice\\nthis pr optimizes calendarcache retrieval by:\\n\\ncreated a new cachedcalendarservice that implements the calendar interface\\nmodified handlenewbooking to fetch all selectedcalendars at once before processing individual users\\nimplemented an in-memory store to hold the calendar cache data\\nensured proper fallback to original calendarservice for cache misses\\n\\nthis reduces database calls and avoids redundant calls to google calendar api during booking processes.\\nchanges\\n\\ncreated cachedcalendarservice in packages/app-store/googlecalendar/lib/cachedcalendarservice.ts\\nimplements the calendar interface\\nonly serves cache hits, falls back to original service for misses\\n\\ndelegates most methods to the original calendar service\\n\\n\\ncreated in-memory cache store in packages/features/calendar-cache/calendar-cache-store.ts\\n\\nstores calendar availability data during a booking process\\n\\ncleared at the start of each new booking\\n\\n\\nadded utility function getcachedcalendar in packages/app-store/_utils/getcachedcalendar.ts\\n\\nreturns cachedcalendarservice for google calendar credentials\\n\\nfalls back to regular calendar service for other types\\n\\n\\nmodified handlenewbooking.ts to:\\n\\nclear the cache at the beginning of a new booking\\n\\nfetch all selected calendars at once before the user loop\\n\\n\\nupdated getcalendarsevents.ts to use cached calendar when appropriate\\n\\n\\ntesting\\ntested by creating a booking with multiple users and verified reduced database calls and no redundant calls to google calendar api.\\nlink to  run:  <URL> \\nrequested by: zomars@cal.com\\n this is an auto-generated description by mrge. \\n\\nsummary by mrge\\nadded cachedcalendarservice to cache calendar availability during bookings, reducing database and google calendar api calls.\\n\\nnew features\\nintroduced an in-memory calendar cache store.\\ncreated cachedcalendarservice to serve cached data and fall back to the original service on cache misses.\\nupdated booking flow to fetch all selected calendars at once and clear the cache at the start of each booking.\\nmodified calendar event retrieval to use the cache when possible.\\n\\n end of auto-generated description by mrge.', \"fix: resolve type errors in calendar cache implementationoptimize calendar cache retrieval with cachedcalendarservice\\nthis pr optimizes calendarcache retrieval by:\\n\\ncreated a new cachedcalendarservice that implements the calendar interface\\nmodified handlenewbooking to fetch all selectedcalendars at once before processing individual users\\nimplemented an in-memory store to hold the calendar cache data\\nadded logic to determine which users have 100% cache hits before selecting the calendar service\\n\\nthis reduces database calls and avoids redundant calls to google calendar api during booking processes.\\nchanges\\n\\ncreated cachedcalendarservice in packages/app-store/googlecalendar/lib/cachedcalendarservice.ts\\nimplements the calendar interface\\nonly serves cache hits, does not use google calendarservice internally\\n\\nthrows errors for write operations as it's read-only\\n\\n\\ncreated in-memory cache store in packages/features/calendar-cache/calendar-cache-store.ts\\n\\nstores calendar availability data during a booking process\\nadded method to determine which users have 100% cache hits\\n\\ncleared at the start of each new booking\\n\\n\\nenhanced utility function getcachedcalendar in packages/app-store/_utils/getcachedcalendar.ts\\n\\nchecks for 100% cache hits before deciding which service to use\\nonly returns cachedcalendarservice for users with complete cache hits\\n\\nfalls back to regular calendar service for other cases\\n\\n\\nmodified handlenewbooking.ts to:\\n\\nclear the cache at the beginning of a new booking\\n\\nfetch all selected calendars at once before the user loop\\n\\n\\nupdated getcalendarsevents.ts to:\\n\\npass necessary parameters to getcachedcalendar for cache hit determination\\nuse the appropriate calendar service based on cache availability\\n\\ntesting\\ntested by creating a booking with multiple users and verified reduced database calls and no redundant calls to google calendar api.\\nlink to  run:  <URL> \\nrequested by: zomars@cal.com\\n this is an auto-generated description by mrge. \\n\\nsummary by mrge\\nfixed type errors in the calendar cache implementation to ensure type safety and prevent runtime issues.\\n end of auto-generated description by mrge.\", 'feat: implement isr for booking pages with google calendar webhook integrationimplement next.js isr for individual booking pages with google calendar webhook integration\\nsummary\\nthis pr implements next.js incremental static regeneration (isr) for individual booking pages (/[user]/[type]) with a 1-hour ttl caching strategy and automatic revalidation triggered by google calendar webhook events.\\nchanges made\\n1. isr implementation for booking pages\\n\\nfile: apps/web/app/(booking-page-wrapper)/[user]/[type]/page.tsx\\nadded unstable_cache with 1-hour (3600 seconds) revalidation\\nfixed app router compatibility by passing individual parameters instead of legacy context object\\nuses cache tags [\"booking-page\"] for targeted invalidation\\n\\n2. server actions for revalidation\\n\\nfile: apps/web/app/(booking-page-wrapper)/[user]/[type]/actions.ts\\ncreated revalidatebookingpage() for specific user/type combinations\\ncreated revalidateuserbookingpages() for all booking pages of a user\\nuses revalidatepath() and revalidatetag() for cache invalidation\\n\\n3. google calendar webhook integration\\n\\nfile: packages/app-store/googlecalendar/api/webhook.ts\\nadded isr revalidation logic triggered by calendar change events\\nimplemented dynamic user identification via userrepository.findbyid()\\nadded comprehensive error handling and logging\\n\\n4. fallback task queue system\\n\\nfile: packages/features/tasker/tasks/revalidate-booking-pages.ts\\ncreated new task handler for isr revalidation as fallback mechanism\\nfile: packages/features/tasker/tasker.ts - added task type definition\\nfile: packages/features/tasker/tasks/index.ts - registered new task handler\\nprovides resilience if direct webhook revalidation fails\\n\\ntechnical implementation details\\nisr caching strategy\\ntypescript\\nconst getcachedbookingdata = unstable_cache(\\n  async (headers, cookies, params, searchparams) => {\\n    const legacyctx = buildlegacyctx(headers, cookies, params, searchparams);\\n    return await getdata(legacyctx);\\n  },\\n  [\"booking-page-data\"],\\n  { \\n    revalidate: 3600, // 1 hour ttl\\n    tags: [\"booking-page\"]\\n  }\\n);\\nwebhook revalidation flow\\n\\ngoogle calendar webhook receives change notification\\nidentifies affected user via credential.userid\\nfetches user profile to get username\\ntriggers isr revalidation for user\\'s booking pages\\nfalls back to task queue if direct revalidation fails\\n\\nerror handling\\n\\ncomprehensive try-catch blocks around revalidation logic\\nfallback to task queue system if direct revalidation fails\\ndetailed logging for debugging and monitoring\\n\\ntesting status\\n local testing limitation: full end-to-end testing was limited due to a database schema issue in the development environment. the error \"the column membership.customroleid does not exist in the current database\" prevented booking pages from loading locally.\\ncompleted testing\\n\\n typescript compilation passes (yarn type-check:ci)\\n pre-commit hooks (prettier, eslint) pass\\n code follows existing patterns and conventions\\n\\ntesting instructions for reviewers\\n\\nisr functionality:\\naccess booking pages like /free/30min or /pro/15min\\nverify pages load quickly (pre-rendered)\\n\\ncheck browser dev tools for cache headers\\n\\n\\nwebhook integration:\\n\\ntrigger google calendar changes for users with cal.com integration\\nverify booking pages update within reasonable time\\n\\ncheck logs for revalidation events\\n\\n\\nfallback mechanism:\\n\\nsimulate webhook revalidation failures\\nverify task queue picks up revalidation jobs\\ncheck task execution logs\\n\\nperformance benefits\\n\\nfaster page loads: pre-rendered pages serve immediately from cache\\nreduced server load: database queries cached for 1 hour\\nautomatic updates: pages stay fresh via webhook-triggered revalidation\\nresilient system: fallback task queue ensures reliability\\n\\nbackwards compatibility\\n\\n no breaking changes to existing booking functionality\\n maintains all existing api contracts\\n preserves metadata generation and internationalization\\n compatible with existing authentication and authorization\\n\\ndatabase requirements\\nthis implementation requires the database schema to be up-to-date. if encountering the customroleid column error, run:\\nbash\\nyarn workspace @calcom/prisma db-migrate\\nlink to  run\\n <URL> \\nrequested by\\nkeith@cal.com (keith williams)\\n this is an auto-generated description by cubic. \\n\\nsummary by cubic\\nadded 1-hour isr caching to individual booking pages and integrated google calendar webhooks to trigger automatic page revalidation. includes a fallback task queue to ensure updates if direct revalidation fails.\\n\\nnew features\\nbooking pages (/[user]/[type]) now use isr with a 1-hour cache.\\ngoogle calendar webhook triggers revalidation when calendar events change.\\nfallback task queue handles revalidation if webhook fails.\\n\\n end of auto-generated description by cubic.']"
        ],
        [
         "28",
         "27",
         "8",
         "27_optimizations_kernels_git_optimization",
         "['optimizations', 'kernels', 'git', 'optimization', 'optimize', 'add', 'benchmark', 'addition', 'matrix', '']",
         "['add benchmark for deserializing large added vocab +  optimizations', 'alpaka kernels for matrix multiplication and addition', '/identify additional shallow store optimizations d0c6']"
        ],
        [
         "29",
         "28",
         "8",
         "28_metamask_metamaskwallet_rpc_block_number",
         "['metamask', 'metamaskwallet', 'rpc', 'block_number', 'lints', 'metadata', 'endpoints', 'precompile', 'node', 'asset']",
         "['docs: update trusted hash guide[x] analyzed repository structure and understood the issue\\n[x] confirmed build system works correctly\\n[x] identified that quick-start.md already uses the new command format\\n[x] update celestia-node-trusted-hash.md to use the new single-call command instead of two separate curl calls\\n[x] test that the markdown lints correctly after changes\\n[x] verify build still works after changes\\n\\nthe issue requests updating the trusted hash guide to use a more efficient single api call command instead of making two separate curl requests. this reduces network calls and improves user experience.\\ncurrent state: the file /how-to-guides/celestia-node-trusted-hash.md still uses the old two-call approach on lines 50-51, while /how-to-guides/quick-start.md already uses the new efficient single-call format.\\nfixes #2033.\\n\\n you can make  smarter by setting up custom instructions, customizing its development environment and configuring model context protocol (mcp) servers. learn more  coding agent tips in the docs.', \"docs: add iavl v1 migration guide for consensus node operatorsthis pr adds comprehensive documentation for consensus node operators on how to migrate from iavl v0 to iavl v1 to achieve optimal performance benefits with celestia-app v4.\\nbackground\\nwith celestia-app v4, the state machine uses iavl v1.x which provides roughly 10x performance improvement over v0.x through data locality optimizations. however, the entire database needs to be migrated to the v1 key layout. while this happens lazily by default, it can cause increased i/o load and make state access a performance bottleneck during the migration period.\\nwhat's added\\nthe new migration guide (docs/maintainers/iavl-v1-migration.md) provides:\\nmigration options\\n\\nlazy migration (default): automatic migration during normal operation\\nstate sync migration (recommended): full migration via state sync for immediate optimal performance\\n\\ncomprehensive instructions\\n\\nstep-by-step procedures for both migration approaches\\nspecific commands and configuration examples\\nprerequisites and preparation steps\\nbackup and recovery procedures\\n\\nperformance optimization\\n\\niavl configuration options in app.toml with tuning recommendations\\nperformance monitoring guidance to measure improvements\\nkey performance indicators to track\\n\\noperational support\\n\\ntroubleshooting common migration issues\\nbest practices for safe migration\\nrecovery procedures if rollback is needed\\nlinks to additional resources and support channels\\n\\nkey benefits\\nthis documentation addresses the concern that lazy migration might cause increased i/o load and prevent maximizing throughput. by providing clear guidance on the state sync migration approach, consensus node operators can immediately access the full iavl v1 performance benefits without experiencing a degradation period.\\nthe guide is specifically written for consensus node operators and includes validator-specific considerations like proper backup procedures, downtime planning, and configuration optimization.\\nfixes #4839.\\n\\n you can make  smarter by setting up custom instructions, customizing its development environment and configuring model context protocol (mcp) servers. learn more  coding agent tips in the docs.\", 'add listmetadatakeys and getallmetadata rpc methods for improved metadata discoveryproblem\\nthe existing getmetadata rpc query requires users to know the specific metadata keys beforehand, which are scattered throughout the codebase. this makes it difficult for users to discover what metadata is available or to query all node metadata efficiently.\\nsolution\\nthis pr implements two new rpc methods while maintaining backward compatibility:\\n1. listmetadatakeys() - metadata discovery\\nreturns all available metadata keys with human-readable descriptions:\\ngo\\nkeys, err := client.listmetadatakeys(ctx)\\nfor _, key := range keys {\\n    fmt.printf(\"key: %s - %s\\\\n\", key.key, key.description)\\n}\\n2. getallmetadata() - omni query\\nreturns all node metadata in a single call, reducing round trips:\\ngo\\nmetadata, err := client.getallmetadata(ctx)\\nfor _, entry := range metadata {\\n    fmt.printf(\"%s: %v\\\\n\", entry.key, entry.value)\\n}\\nkey features\\n\\ndiscoverable: users can explore available metadata without prior knowledge\\nefficient: single call to get all metadata reduces network overhead  \\nrobust: gracefully handles missing or unavailable metadata keys\\nbackward compatible: existing getmetadata method unchanged\\nwell-documented: comprehensive api documentation and examples\\n\\nrest endpoints\\nadded convenient http endpoints for web integrations:\\n- get /api/v1/metadata/keys - lists metadata keys with descriptions\\n- get /api/v1/metadata - returns available keys and rpc method info\\nimplementation details\\n\\ncentralized metadata key constants in types package for consistency\\nupdated all references throughout the codebase to use centralized constants\\ngenerated new protobuf definitions with proper message types\\ncomprehensive test coverage for rpc methods, http endpoints, and error cases\\n\\ncurrent metadata keys\\n| key | description |\\n|-----|-------------|\\n| d | da included height - height of the data availability layer that has been included |\\n| l | last batch data - last batch data submitted to the data availability layer |\\n| last-submitted-header-height | last submitted header height to da |\\n| last-submitted-data-height | last submitted data height to da |\\nfixes #2386.\\n\\n[!warning]\\n\\nfirewall rules blocked me from connecting to one or more addresses\\ni tried to connect to the following addresses, but was blocked by firewall rules:\\n\\nbuf.build\\ntriggering command: /usr/local/bin/buf generate --path=./proto/rollkit --template=buf.gen.yaml --config=buf.yaml (dns block)\\nesm.ubuntu.com\\ntriggering command: /usr/lib/apt/methods/ <URL>  (dns block)\\n\\nif you need me to access, download, or install something from one of these locations, you can either:\\n\\nconfigure actions setup steps to set up my environment, which run before the firewall is enabled\\nadd the appropriate urls or hosts to my firewall allow list\\n\\n\\n\\n start  coding agent tips \\n\\n share your feedback on  coding agent for the chance to win a $200 gift card! click here to start the survey.']"
        ],
        [
         "30",
         "29",
         "7",
         "29_scrape_tool_batch_scrape_tool_seleniumscrapingtool_crawl_tool",
         "['scrape_tool', 'batch_scrape_tool', 'seleniumscrapingtool', 'crawl_tool', 'batch_scrape', 'scrapeoptions', 'server', 'firecrawl', 'webdriver', 'scrape']",
         "['implement retry-after header handling for improved throttling in fetch requestscurrently, genaiscript handles throttling situations but does not respect the retry-after header returned by services. this leads to unnecessary load on throttled services and suboptimal user experience with exponential backoff delays that may be longer than needed.\\nchanges made\\nthis pr implements proper retry-after header handling in the fetch retry logic:\\n1. added parseretryafter() function\\n\\nsupports both integer seconds format: retry-after: 120 \\nsupports http date format: retry-after: fri, 31 dec 1999 23:59:59 gmt\\nhandles edge cases: negative values, invalid inputs, past dates, whitespace\\nreturns null for invalid inputs to fallback to exponential backoff\\n\\n2. enhanced retry logic in createfetch()\\n\\nchecks for retry-after header in http responses during retries\\nuses server-specified delay instead of calculated exponential backoff when available\\nfalls back to existing exponential backoff when no retry-after header is present\\nrespects maxdelay configuration to prevent excessive waits\\n\\n3. improved user transparency\\n\\ndisplays retry-after information in log messages\\nformat: retry #1 in 120.0s (retry-after: 120s) vs retry #1 in 2.0s\\nusers can now see the actual reason and duration for delays\\n\\nexample behavior\\nbefore:\\nretry #1 in 2.0s 429 too many requests\\nretry #2 in 3.0s 429 too many requests  \\nretry #3 in 4.5s 429 too many requests\\nafter (with retry-after: 60):\\nretry #1 in 60.0s (retry-after: 60s) 429 too many requests\\nbenefits\\n\\n reduces unnecessary load on throttled services by respecting server timing\\n provides transparency to users about wait times and reasons\\n maintains full backward compatibility with existing retry behavior\\n handles common throttling scenarios (http 429, 503, etc.)\\n rfc 7231 compliant retry-after header parsing\\n\\ntesting\\nadded comprehensive unit tests covering:\\n- integer seconds parsing (\"120\" → 120 seconds)\\n- http date parsing (future dates with proper time calculation)\\n- edge cases (negative values, invalid strings, past dates)\\n- whitespace handling and input validation\\nfixes #1629.\\n\\n[!warning]\\n\\nfirewall rules blocked me from connecting to one or more addresses\\ni tried to connect to the following addresses, but was blocked by firewall rules:\\n\\ncdn.sheetjs.com\\ntriggering command: node /usr/local/bin/yarn install (dns block)\\n\\nif you need me to access, download, or install something from one of these locations, you can either:\\n\\nconfigure actions setup steps to set up my environment, which run before the firewall is enabled\\nadd the appropriate urls or hosts to my firewall allow list\\n\\n\\n\\n start  coding agent tips \\n\\n you can make  smarter by setting up custom instructions, customizing its development environment and configuring model context protocol (mcp) servers. learn more  coding agent tips in the docs.', 'implement maxage fast scraping parameterimplement maxage fast scraping parameter\\nsummary\\nthis pr implements the maxage parameter for the firecrawl-mcp-server to enable faster scraping through caching, addressing github issue #69. the implementation exposes the existing firecrawl api maxage parameter through the mcp server\\'s tool schema.\\nkey changes:\\n- added maxage parameter to scrape_tool inputschema as optional number field\\n- updated tool description and usage examples to highlight caching benefits\\n- added test coverage to verify parameter is passed through to firecrawl api\\n- merged latest main branch changes (version bump to 1.11.0)\\nthe maxage parameter allows users to specify a cache duration in milliseconds. when set, the system will use cached content if available and younger than the specified age, otherwise scrape fresh content.\\nreview & testing checklist for human\\n\\n[ ] test maxage with real firecrawl api calls - verify that setting maxage actually enables caching behavior (most critical)\\n[ ] validate performance claims - test whether maxage actually provides significant speed improvements as claimed\\n[ ] test edge cases - try invalid maxage values (negative, non-numeric) to ensure proper error handling\\n[ ] verify backwards compatibility - ensure existing scrape calls without maxage parameter continue working\\n\\nrecommended test plan: create a test script that scrapes the same url twice with maxage set, verify the second call is faster and returns cached content.\\n\\ndiagram\\n```mermaid\\n%%{ init : { \"theme\" : \"default\" }}%%\\ngraph td\\n    subgraph \"mcp server implementation\"\\n        indexts[\"src/index.ts\"]:::major-edit\\n        indextestts[\"src/index.test.ts\"]:::major-edit\\n    end\\nsubgraph \"tool schema\"\\n    scrape_tool[\"scrape_tool definition\"]:::major-edit\\n    inputschema[\"inputschema.properties\"]:::major-edit\\nend\\n\\nsubgraph \"external dependencies\"\\n    firecrawlsdk[\"@mendable/firecrawl-js\"]:::context\\n    scrapeparams[\"scrapeparams type\"]:::context\\nend\\n\\nindexts --> scrape_tool\\nscrape_tool --> inputschema\\ninputschema --> |\"maxage: number\"| firecrawlsdk\\nindextestts --> |\"tests maxage passing\"| firecrawlsdk\\nfirecrawlsdk --> scrapeparams\\n\\nsubgraph legend\\n    l1[\"major edit\"]:::major-edit\\n    l2[\"minor edit\"]:::minor-edit  \\n    l3[\"context/no edit\"]:::context\\nend\\n\\nclassdef major-edit fill:#90ee90\\nclassdef minor-edit fill:#87ceeb\\nclassdef context fill:#ffffff\\n\\n```\\nnotes\\n\\nimplementation relies on existing firecrawl sdk scrapeparams type to handle maxage validation\\nthe parameter is optional and should default to 0 (always scrape fresh) per firecrawl api behavior\\nperformance improvement claims (500% faster) are based on issue description but not independently verified\\nsession url:  <URL> \\nrequested by: @nickscamara', 'implement maxage fast scraping parameterimplement maxage fast scraping parameter\\nsummary\\nthis pr implements the maxage fast scraping parameter across all scraping-related tools in the firecrawl mcp server, enabling 500% faster scraping through intelligent caching as documented in pr #34 of firecrawl-docs.\\nkey changes:\\n- added maxage parameter (number, defaults to 0) to scrape_tool, crawl_tool, and search_tool schemas\\n- created missing batch_scrape_tool that was referenced in tests but absent from main code\\n- added proper type guard and request handler for batch scraping functionality\\n- updated all tool schemas to include maxage with proper descriptions and defaults\\nthe maxage parameter accepts milliseconds and uses cached content if younger than the specified age, otherwise scrapes fresh content. a value of 0 (default) means always scrape fresh.\\nreview & testing checklist for human\\n\\n[ ] test actual caching behavior: verify maxage parameter works with real firecrawl api calls (make same request twice with maxage > 0, confirm second request uses cache)\\n[ ] test new batch_scrape_tool: verify the previously missing batch scrape functionality now works end-to-end  \\n[ ] verify backward compatibility: test all existing tools still work without maxage specified\\n[ ] test parameter passing: confirm maxage gets properly passed to underlying firecrawl client methods\\n[ ] integration testing: run the mcp server with a real mcp client and test all modified tools\\n\\nrecommended test plan:\\n1. start mcp server locally\\n2. test each tool (scrape, crawl, batch_scrape, search) with and without maxage\\n3. for caching verification: scrape same url twice with maxage=300000 (5min), verify second call is faster\\n4. verify error handling when maxage is invalid (negative, non-number)\\n\\ndiagram\\n```mermaid\\n%%{ init : { \"theme\" : \"default\" }}%%\\ngraph td\\n    subgraph \"mcp server structure\"\\n        index[\"src/index.ts\"]:::major-edit\\n        tests[\"src/index.test.ts\"]:::context\\n    end\\nsubgraph \"tool definitions (updated)\"\\n    scrape[\"scrape_tool<br/>+maxage param\"]:::major-edit\\n    crawl[\"crawl_tool<br/>+maxage in scrapeoptions\"]:::major-edit  \\n    search[\"search_tool<br/>+maxage in scrapeoptions\"]:::minor-edit\\n    batch[\"batch_scrape_tool<br/>**new tool**\"]:::major-edit\\nend\\n\\nsubgraph \"api handlers (updated)\"  \\n    handler[\"calltoolrequestschema<br/>+batch_scrape case\"]:::major-edit\\n    typeguards[\"type guards<br/>+isbatchscrapeoptions\"]:::minor-edit\\nend\\n\\nsubgraph \"firecrawl client calls\"\\n    scrapecall[\"client.scrapeurl()\"]:::context\\n    crawlcall[\"client.asynccrawlurl()\"]:::context  \\n    batchcall[\"client.asyncbatchscrapeurls()\"]:::context\\n    searchcall[\"client.search()\"]:::context\\nend\\n\\nindex --> scrape\\nindex --> crawl  \\nindex --> search\\nindex --> batch\\nindex --> handler\\nindex --> typeguards\\n\\n\\nhandler --> scrapecall\\nhandler --> crawlcall\\nhandler --> batchcall  \\nhandler --> searchcall\\n\\ntests -.-> batch\\n\\nsubgraph legend\\n    l1[\"major edit\"]:::major-edit\\n    l2[\"minor edit\"]:::minor-edit  \\n    l3[\"context/no edit\"]:::context\\nend\\n\\nclassdef major-edit fill:#90ee90\\nclassdef minor-edit fill:#87ceeb  \\nclassdef context fill:#ffffff\\n\\n```\\nnotes\\n\\ncritical discovery: the batch_scrape_tool was completely missing from the main code despite being referenced in tests - this was a significant gap that needed to be filled\\ntypescript issue: had to remove origin: \\'mcp-server\\' parameter from batch scrape call due to type compatibility issues\\ntesting limitation: while all lint/test/build checks pass, the actual caching behavior with real firecrawl api calls couldn\\'t be verified in the development environment\\ndocumentation alignment: implementation follows the fast-scraping documentation from firecrawl-docs pr #34\\n\\nsession info: \\n- requested by: @nickscamara\\n-  session:  <URL> \\n- fixes: mendableai/firecrawl-mcp-server#69']"
        ],
        [
         "31",
         "30",
         "7",
         "30_testccompiler_vmvalid_golden_testccompiler_goldenoutput_compiler_testccompiler_validprograms",
         "['testccompiler_vmvalid_golden', 'testccompiler_goldenoutput', 'compiler', 'testccompiler_validprograms', 'compiling', 'compile', 'backendsummary', 'sum_builtin', 'compilepostfix', 'testing']",
         "['update c backend to use stack arrayssummary\\n\\nadd helper for contains_array_int and generate it when a list expression is backed by a stack array\\nemit contains_array_int instead of contains_list_int when possible\\nuse listlenexpr and listitemexpr for query loops\\nregenerate c machine outputs\\n\\ntesting\\n\\ngo test ./compiler/x/c -run testccompiler_validprograms -tags slow -count=1\\n\\n\\n <URL> ', 'update c backend aggregatessummary\\n\\nimprove avg, sum, min and max when the length of the input list is known\\nregenerate machine translations for affected examples\\n\\ntesting\\n\\ngo test ./compiler/x/c -run testccompiler_validprograms/avg_builtin -tags slow\\ngo test ./compiler/x/c -run testccompiler_validprograms/sum_builtin -tags slow\\ngo test ./compiler/x/c -run testccompiler_validprograms/min_max_builtin -tags slow\\ngo test ./compiler/x/c -run testccompiler_validprograms/break_continue -tags slow\\n\\n\\n <URL> ', 'add constant string list inference to c backendsummary\\n\\nimprove c compiler by tracking string list constants\\nprecompute len/count/min/max for constant string lists\\nregenerate golden outputs\\nupdate compilation status and task log\\n\\ntesting\\n\\ngo test -tags slow ./compiler/x/c -run testccompiler_vmvalid_golden -update (fails: group_by_multi_join_sort, save_jsonl_stdout, tree_sum, update_stmt)\\n\\n\\n <URL> ']"
        ],
        [
         "32",
         "31",
         "7",
         "31_git_build_options_disable_grpc_modules_build",
         "['git', 'build_options', 'disable_grpc_modules', 'build', 'size_reduction_summary', 'buildbuild', 'test_binary', 'gopsutil', 'tags', 'packages']",
         "['build tag to exclude cloud archiver providerswhat changed?\\nadded a build tag to exclude cloud archiver providers. the filesystem one is unaffected (as it is needed for tests).\\nwhy?\\nreduce binary size by 24mb for when cloud archiver providers are not needed.\\nbefore (without disable_grpc_modules)\\n-rwxr-xr-x@ 1 stephan  staff   119m jun 30 16:35 temporal-server\\nafter\\n-rwxr-xr-x@ 1 stephan  staff    95m jun 30 16:23 temporal-server', '[poc] build tag for lightweight test buildbuild tag that removes the 3 dependencies from the test binary:\\n\\ngithub.com/aws/aws-sdk-go\\ncloud.google.com/go\\ngithub.com/jackc/pgx\\n\\nbefore: ~13.6s build time\\n```\\nhyperfine --warmup 1 \"rm -f test_binary && go test -tags test_dep -count=1 -c -o test_binary ./tests\"    \\nbenchmark 1: rm -f test_binary && go test -tags test_dep -count=1 -c -o test_binary ./tests\\n  time (mean ± σ):     13.616 s ±  0.215 s    [user: 13.757 s, system: 3.726 s]\\n  range (min … max):   13.269 s … 14.046 s    10 runs\\n```\\nafter: ~9.3s build time\\n```\\nhyperfine --warmup 1 \"rm -f test_binary && go test -tags test_dep,lite -count=1 -c -o test_binary ./tests\"\\nbenchmark 1: rm -f test_binary && go test -tags test_dep,lite -count=1 -c -o test_binary ./tests\\n  time (mean ± σ):      9.314 s ±  0.138 s    [user: 9.803 s, system: 3.450 s]\\n  range (min … max):    9.038 s …  9.529 s    10 runs\\n```\\n=> 32% speedup.', 'build: add optional build tags to reduce binary sizethis pr implements conditional compilation using go build tags to make optional features truly optional, allowing users to choose between a smaller binary and full functionality.\\nresults\\n| build configuration | binary size | reduction |\\n|---------------------|-------------|-----------|\\n| original (with go-git) | 16.7 mib | - |\\n| current full build | 8.1m | 51.5% |\\n| new lite build | 7.4m | 55.7% |\\ntotal reduction: 9.3 mib (44% smaller binary)\\nchanges made\\nbuild tags implementation\\n\\nfuzzy tag: controls fuzzy search and path indexing functionality\\ndependencies: goleveldb, sahilm/fuzzy (~500kb)\\n\\nfeatures: --fuzzy flag, path indexing commands\\n\\n\\nmounts tag: controls mount point detection  \\n\\ndependencies: gopsutil (~200kb)\\nfeatures: --mounts flag for mount details\\n\\nbuild options\\nlite build (default) - recommended for most users:\\nbash\\ngo build -ldflags=\"-s -w\" .\\n- size: 7.4m\\n- all core functionality including git status, display options, sorting, filtering\\nfull build - for power users:\\nbash\\ngo build -ldflags=\"-s -w\" -tags=\"fuzzy mounts\" .\\n- size: 8.1m\\n- all features including fuzzy search and mount detection\\ncustom builds:\\n```bash\\nonly fuzzy search\\ngo build -tags=\"fuzzy\" .\\nonly mount details\\ngo build -tags=\"mounts\" .\\n```\\nbackwards compatibility\\n\\ndefault build provides 95% of functionality most users need\\noptional features degrade gracefully when disabled (no errors)\\nall command-line flags remain functional\\ncore git integration via cli preserved in all builds\\n\\ndocumentation\\n\\nadded docs/build_options.md with detailed build instructions\\nadded docs/size_reduction_summary.md with results summary\\nupdated justfile with new build targets (build-lite, build-full, etc.)\\n\\nthis approach allows users to choose between binary size and optional functionality based on their needs, while maintaining full backwards compatibility.\\nfixes #237.\\n start  coding agent tips \\n\\n share your feedback on  coding agent for the chance to win a $200 gift card! click here to start the survey.']"
        ],
        [
         "33",
         "32",
         "7",
         "32_terrainpatch_meshes_rendering_patch",
         "['terrainpatch', 'meshes', 'rendering', 'patch', 'patchsize', 'patches', 'texture', 'terrainpatchsize', 'fsharp', 'umaoptimizedmeshcombinerexample']",
         "['optimize billboard rendering for 3d particles with specialized batched render paththis pr implements an optimized billboard rendering system specifically designed for 3d particle systems, addressing the need for efficient batched billboard rendering with blend and flip support.\\nchanges made\\n1. new render message type\\nadded renderstaticbillboards type that supports batched rendering of multiple billboards with shared material properties but individual:\\n- color and emission values per billboard\\n- blend modes (transparent, additive, overwrite) \\n- flip options (flipnone, fliph, flipv, fliphv)\\n- transform matrices\\n2. fast path api\\nimplemented world.renderstaticbillboardfast method that provides a high-performance rendering path by:\\n- bypassing normal message allocation overhead\\n- batching billboards with the same material\\n- supporting individual per-billboard properties\\nfsharp\\nworld.renderstaticbillboardfast(\\n    &modelmatrix, castshadow, presence, insetopt, orientup, planar,\\n    &materialproperties, &material, &color, &emission, \\n    blend, flip, shadowoffset, depthtest, rendertype, renderpass, world)\\n3. blend mode support\\nadded infrastructure for blend modes in 3d billboard rendering:\\n- transparent: standard alpha blending for particles\\n- additive: bright glowing effects for sparks/fire\\n- overwrite: solid billboard rendering\\n4. flip support\\nfull texture flipping support for billboard variety:\\n- individual flip control per billboard\\n- supports horizontal, vertical, and combined flipping\\nperformance benefits\\n\\nbatched rendering: multiple billboards sharing materials are rendered efficiently\\nfast path: eliminates message allocation overhead for high-frequency particle updates\\nparticle optimized: designed specifically for particle system use cases with hundreds/thousands of billboards\\n\\nexample usage\\n```fsharp\\n// fire particle with additive blending and emission\\nlet modelmatrix = matrix4x4.createtranslation(particleposition)\\nlet color = color(1.0f, 0.5f, 0.0f, alpha) // orange with fade\\nlet emission = color(1.0f, 0.3f, 0.0f, 1.0f) // glowing orange\\nworld.renderstaticbillboardfast(\\n    &modelmatrix, false, presence.present, valuenone, true, false,\\n    &materialprops, &material, &color, &emission, \\n    additive, flipnone, 0.0f, lessthanorequaltest,\\n    forwardrendertype(0.0f, 0.0f), normalpass, world)\\n```\\nthis implementation provides the foundation for high-performance 3d particle systems while maintaining compatibility with existing billboard rendering infrastructure.\\nfixes #700.\\n start  coding agent tips \\n\\n share your feedback on  coding agent for the chance to win a $200 gift card! click here to start the survey.', 'implement terrain patches for improved gpu performancethis pr implements terrain patches to address gpu performance issues when rendering large terrains. the solution automatically subdivides large terrains into smaller, cullable chunks without breaking existing apis.\\nproblem\\ncurrently, rendering large terrains consumes excessive gpu resources because the entire terrain is rendered as a single mesh. this prevents efficient frustum culling and makes proximity debug drawing for height maps impractical due to performance concerns.\\nsolution\\nthe implementation adds an automatic terrain patching system that:\\n\\nautomatically subdivides large terrains (>64x64 vertices) into 64x64 vertex patches\\nmaintains full backwards compatibility - existing terrain code works unchanged\\nenables per-patch frustum culling for better gpu utilization\\nuses configurable patch size via constants.render.terrainpatchsize\\n\\nkey changes\\ncore infrastructure\\n\\nadded terrainpatchsize constant (64 vertices) in constants.fs\\nadded terrainpatch type to represent terrain subdivisions\\nextended terraingeometrydescriptor and terraindescriptor with optional patch information\\n\\nsmart patch system\\n\\nterrainpatch.calculateterrainpatches function handles subdivision logic\\nedge patches correctly handle non-even divisions (e.g., 200x200 terrain creates 4x4 patches with 8-vertex edge patches)\\nsmall terrains (<64x64) continue using single-mesh rendering\\n\\nrendering pipeline updates\\n\\nmodified categorizeterrain to detect when patches should be used\\nupdated terrain geometry creation to handle both single-mesh and patch-based rendering\\neach patch is treated as independent geometry with its own bounds for culling\\n\\ntesting\\ncreated validation scripts that confirm:\\n- 200x200 terrain correctly generates 16 patches (4x4 grid)\\n- edge patches handle remainder vertices properly (8x64, 64x8, 8x8 for edges/corners)\\n- small terrains (32x32) use single patch as expected\\n- all patch bounds calculations are accurate\\nperformance benefits\\n\\nreduced gpu load: only visible terrain patches are rendered\\nbetter memory utilization: patches enable more efficient gpu processing\\nenables proximity debug drawing: height map debugging can now be used without performance concerns\\nautomatic optimization: no code changes needed - patches activate automatically for large terrains\\n\\nexample\\nfsharp\\n// before: 512x512 terrain renders as single 262,144 vertex mesh\\n// after: same terrain automatically splits into 64 patches of 64x64 vertices each\\n//        only visible patches are rendered based on frustum culling\\nthis change directly addresses the core issue while maintaining complete api compatibility. developers can now enable proximity debug drawing for height maps without performance penalties.\\nfixes #747.\\n start  coding agent tips \\n\\n you can make  smarter by setting up custom instructions, customizing its development environment and configuring model context protocol (mcp) servers. learn more  coding agent tips in the docs.', 'implement terrain patches to reduce gpu load for large terrainsthis pr implements a terrain patch system that splits large terrains into smaller, manageable chunks to significantly reduce gpu performance issues when rendering large terrains.\\nproblem\\ncurrently, rendering large terrains generates massive vertex/index arrays that overwhelm the gpu, causing performance bottlenecks. the entire terrain was rendered as a single geometry object regardless of size.\\nsolution\\nthe patch system divides terrains into configurable sub-sections (patches) that are rendered individually:\\ncore changes\\n\\nnew terrainpatch type - represents a subdivision of terrain with its own bounds and resolution\\nextended terrain descriptors - added patchsize property to terraingeometrydescriptor and terraindescriptor\\npatch-based geometry generation - new trycreatephysicallybasedterrainpatchgeometry function creates geometry for individual patches\\nupdated rendering pipeline - modified terrain categorization, shadow rendering, and main rendering to handle patch arrays\\n\\nterrainfacet integration\\n\\nadded patchsize property to terrainfacet with default value of 64×64 height map samples\\nseamless integration with existing terrain creation workflow\\nbackward compatibility maintained\\n\\nmaterial system support\\nthe patch system fully supports the existing terrain material pipeline:\\n- normal map extraction for patch regions\\n- tint image processing per patch\\n- blend weight computation\\n- proper vertex format matching\\nperformance benefits\\n\\nreduced gpu load - smaller geometry chunks instead of massive single meshes\\nfuture optimization ready - enables per-patch frustum culling and lod\\nconfigurable granularity - adjustable patch sizes for different use cases\\nfallback support - automatically uses single patch for small terrains\\n\\nexample usage\\n```fsharp\\n// default patch size (64x64 samples)\\nterrain \"myterrain\" [\\n    terrainmaterial <| blendmaterial { / material config / }\\n    heightmap <| rawheightmap { resolution = v2i 1025 1025; / config / }\\n]\\n// custom patch size for very large terrains\\nterrain \"largeterrain\" [\\n    terrainmaterial <| blendmaterial { / material config / }\\n    heightmap <| rawheightmap { resolution = v2i 2049 2049; / config / }\\n    patchsize <| v2i 128 128  // larger patches for huge terrains\\n]\\n```\\nthis addresses the performance concern mentioned in the issue about enabling proximity debug drawing for height maps without killing performance.\\nfixes #747.\\n\\n[!warning]\\n\\nfirewall rules blocked me from connecting to one or more addresses\\ni tried to connect to the following addresses, but was blocked by firewall rules:\\n\\nesm.ubuntu.com\\ntriggering command: /usr/lib/apt/methods/ <URL>  (dns block)\\n\\nif you need me to access, download, or install something from one of these locations, you can either:\\n\\nconfigure actions setup steps to set up my environment, which run before the firewall is enabled\\nadd the appropriate urls or hosts to my firewall allow list\\n\\n\\n\\n start  coding agent tips \\n\\n share your feedback on  coding agent for the chance to win a $200 gift card! click here to start the survey.']"
        ],
        [
         "34",
         "33",
         "7",
         "33_maven_azure_build_macos_build_windows",
         "['maven', 'azure', 'build_macos', 'build_windows', 'build_linux', 'build_documentation', 'detectbuildtype', 'provisioning', 'pip', 'buildx64linux']",
         "['[azure pipelines] implement  build detection with x64-linux first executionproblem\\n-triggered workflows were overloading the azure pipelines pool by running all triplets in parallel, causing resource contention and slower feedback for both  and regular users.\\nsolution\\nthis pr implements automatic  build detection and conditional execution logic:\\n  detection\\ndetects  builds via multiple indicators:\\n- build.requestedforemail contains \"\" or \"github.com\"\\n- build.sourceversionmessage contains \"\" \\n- build.requestedfor contains \"\"\\n execution strategy\\nfor  builds:\\n1. run x64-linux first\\n2. only run other triplets if x64-linux succeeds\\n3. cancel remaining jobs if x64-linux fails\\nfor regular builds:\\n- unchanged behavior - all triplets run in parallel\\n implementation\\nrestructured the pipeline into 3 stages:\\nyaml\\nstages:\\n- detectbuildtype    # determines if build is -triggered\\n- buildx64linux      # always runs x64-linux first  \\n- buildallothers     # conditionally runs other triplets\\nthe key logic uses azure pipelines stage conditions:\\nyaml\\ncondition: or(\\n  eq(dependencies.detectbuildtype.outputs[\\'detect.detect.isbuild\\'], \\'false\\'),\\n  succeeded(\\'buildx64linux\\')\\n)\\n benefits\\n\\nresource efficiency: prevents pool overload for  builds\\nfast feedback: early failure detection saves time and resources  \\nzero impact: regular user workflows completely unchanged\\nrobust detection: multiple fallback mechanisms catch various  scenarios\\n\\n testing scenarios\\n| scenario | x64-linux | other jobs | behavior |\\n|----------|-----------|------------|----------|\\n|  + success |  runs first |  run after x64-linux | serial execution |\\n|  + failure |  fails |  cancelled | early termination |\\n| regular build |  runs |  run in parallel | existing behavior |\\nfixes #45945.\\n start  coding agent tips \\n\\n you can make  smarter by setting up custom instructions, customizing its development environment and configuring model context protocol (mcp) servers. learn more  coding agent tips in the docs.', '[gh-flow] add minimal azure provisioning for local developmentthis pr implements a split provisioning solution for the gh-flow sample, allowing developers to provision only essential azure resources when developing locally instead of the full production infrastructure.\\nproblem\\npreviously, developers working locally on gh-flow had to run azd provision which creates the complete azure infrastructure including:\\n- container apps environment & registry\\n- azure-hosted qdrant on container apps\\n- cosmos db\\n- application insights & log analytics\\n- monitoring dashboards\\n- the gh-flow container app service\\nthis resulted in:\\n- slow provisioning (20+ minutes)\\n- high azure costs during development ($50+/month)\\n- unnecessary complexity for local development\\nsolution\\nadded a minimal provisioning option that only creates essential azure resources while leveraging the existing containerized services in the devcontainer:\\nnew files\\n\\ninfra/main.local.bicep - minimal bicep template (49 lines vs 160 lines in full template)\\ninfra/main.local.parameters.json - parameters for local development\\nprovision-local.sh - helper script for easy provisioning\\nlocal-development.md - detailed documentation\\n.gitignore - ignore generated bicep artifacts\\n\\nupdated files\\n\\ndocs/github-flow-getting-started.md - added option 2 for minimal provisioning\\n\\nwhat gets provisioned\\n| deployment type | resources | estimated cost |\\n|---|---|---|\\n| full (azd provision) | storage + container apps + qdrant + cosmos + monitoring + app service | $50+/month |\\n| local (./provision-local.sh) | storage account only | ~$1/month |\\nusage\\n```bash\\nfull deployment (unchanged)\\nazd provision\\nlocal development (new)\\n./provision-local.sh my-local-env\\n```\\nbenefits\\n\\n90%+ resource reduction: 1 storage account vs 10+ resources\\nfaster provisioning: minutes instead of 20+ minutes  \\nlower cost: ~$1/month vs $50+/month\\nbetter developer experience: local debugging, containerized qdrant, faster iteration\\neasy cleanup: minimal resources to delete when done\\n\\ntechnical details\\nthe local template reuses existing core bicep modules but only provisions:\\n- resource group\\n- storage account with file shares (needed for azure container instances)\\nlocal development uses:\\n- containerized qdrant ( <URL>  from devcontainer\\n- local application debugging via vs code\\n- user-configured  and github app settings\\nboth templates are validated and compile successfully. the solution maintains full backward compatibility with existing workflows.\\nfixes #72.\\n\\n[!warning]\\n\\nfirewall rules blocked me from connecting to one or more addresses\\ni tried to connect to the following addresses, but was blocked by firewall rules:\\n\\naka.ms\\ntriggering command: curl -fssl redacted (dns block)\\ntriggering command: /usr/bin/../../opt/az/bin/python3 -im azure.cli bicep validate --file main.local.bicep (dns block)\\ntriggering command: bicep lint main.local.bicep (dns block)\\napp.aladdin.microsoft.com\\ntriggering command: /usr/bin/../../opt/az/bin/python3 -im azure.cli bicep validate --file main.local.bicep (dns block)\\n\\nif you need me to access, download, or install something from one of these locations, you can either:\\n\\nconfigure actions setup steps to set up my environment, which run before the firewall is enabled\\nadd the appropriate urls or hosts to my firewall allow list', 'optimize azure sdk ci pipeline with uv tool for faster package managementsummary\\nthis pr optimizes the azure sdk for python ci pipeline by replacing traditional pip and python -m venv commands with the faster uv tool for package installation and virtual environment management. this change aims to significantly reduce ci pipeline setup time across all build jobs.\\nchanges made\\n1. added uv installation to all ci jobs\\nadded global uv installation steps to all 6 ci jobs (build_linux, build_windows, build_macos, build_extended, build_documentation, and analyze) in eng/pipelines/templates/jobs/ci.yml:\\n```yaml\\ninstall uv tool globally\\n\\n\\ntask: bash@3\\n  displayname: \\'install uv (linux/macos)\\'\\n  inputs:\\n    targettype: inline\\n    script: |\\n      curl -lssf  <URL>  | sh\\n  condition: or(eq(variables[\\'agent.os\\'], \\'linux\\'), eq(variables[\\'agent.os\\'], \\'darwin\\'))\\n\\n\\ntask: powershell@2\\n  displayname: \\'install uv (windows)\\'\\n  inputs:\\n    targettype: inline\\n    script: |\\n      iex (irm  <URL> \\n  condition: eq(variables[\\'agent.os\\'], \\'windows_nt\\')\\n```\\n\\n\\n2. replaced pip commands with uv equivalents\\nupdated package installation commands across template files:\\n\\nbuild-package-artifacts.yml: replaced 3 pip commands\\n  ```bash\\n  # before\\n  python -m pip install --force -r eng/ci_tools.txt\\n  python -m pip install -r eng/release_requirements.txt\\n  python -m pip freeze --all\\n\\n# after\\n  uv pip install --force -r eng/ci_tools.txt\\n  uv pip install -r eng/release_requirements.txt\\n  uv pip freeze --all\\n  ```\\n\\nbuild-extended-artifacts.yml: replaced 1 pip command\\nanalyze.yml: replaced 1 pip command\\n\\n3. updated virtual environment management\\nreplaced powershell script-based virtual environment creation in use-venv.yml with native uv venv commands while preserving all existing parameters (virtualenvironmentname, activate, condition):\\n```powershell\\nnow uses uv venv instead of create-venv.ps1\\nuv venv \"${{ parameters.virtualenvironmentname }}\"\\n```\\nbenefits\\n\\nfaster ci builds: uv is significantly faster than pip for package installation and resolution\\nimproved reliability: more consistent dependency resolution\\ncross-platform compatibility: maintained support for windows, linux, and macos\\nno breaking changes: all existing functionality and parameters preserved\\n\\ntesting\\n\\n all yaml files pass syntax validation\\n preserved existing azure devops pipeline formatting conventions\\n cross-platform installation scripts tested for all supported operating systems\\n no remaining python -m pip references in modified template files\\n\\nfiles modified\\n\\neng/pipelines/templates/jobs/ci.yml - added uv installation to 6 jobs\\neng/pipelines/templates/steps/build-package-artifacts.yml - replaced 3 pip commands  \\neng/pipelines/templates/steps/build-extended-artifacts.yml - replaced 1 pip command\\neng/pipelines/templates/steps/analyze.yml - replaced 1 pip command\\neng/pipelines/templates/steps/use-venv.yml - updated virtual environment management\\n\\nthis optimization will reduce ci pipeline execution time while maintaining full backward compatibility with existing workflows.\\nthis pull request was created as a result of the following prompt from  chat.\\n\\nwe need to optimize the azure sdk for python ci pipeline to use the uv tool for installing packages and creating virtual environments, instead of directly using pip or python -m venv. specifically:\\n\\nin eng/pipelines/templates/jobs/ci.yml, add a global uv installation step at the top of each job (build_linux, build_windows, build_macos, build_extended, build_documentation, and analyze). insert the following yaml block as the first step under steps: for each job:\\n\\n```yaml\\ninstall uv tool globally\\n\\n\\ntask: bash@3\\n  displayname: \\'install uv (linux/macos)\\'\\n  inputs:\\n    targettype: inline\\n    script: |\\n      curl -lssf  <URL>  | sh\\n  condition: or(\\n    eq(variables[\\'agent.os\\'], \\'linux\\'),\\n    eq(variables[\\'agent.os\\'], \\'darwin\\')\\n  )\\n\\n\\ntask: powershell@2\\n  displayname: \\'install uv (windows)\\'\\n  inputs:\\n    targettype: inline\\n    script: |\\n      iex (irm  <URL> \\n  condition: eq(variables[\\'agent.os\\'], \\'windows_nt\\')\\n```\\n\\n\\nreplace all direct python -m pip install and python -m pip freeze invocations in yaml under eng/pipelines/templates (specifically build-package-artifacts.yml, build-extended-artifacts.yml, analyze.yml) with uv pip install and uv pip freeze respectively.\\n\\n\\nupdate eng/pipelines/templates/steps/use-venv.yml to create and activate virtual environments using uv instead of the existing create-venv.ps1 script. replace the existing pwsh steps with uv venv create and uv venv activate commands, preserving the virtualenvironmentname, activate, and condition parameters.\\n\\n\\nthese changes aim to reduce setup time by leveraging the uv tool\\'s optimized environment and package management capabilities throughout the ci pipeline.\\n\\n start  coding agent tips \\n\\n you can make  smarter by setting up custom instructions, customizing its development environment and configuring model context protocol (mcp) servers. learn more  coding agent tips in the docs.']"
        ],
        [
         "35",
         "34",
         "7",
         "34_calculate_spring_pose_bone_rotations_calculate_object_pose_bone_rotations_calculate_joint_pair_head_pose_bone_rotations_get_bone_namespring",
         "['calculate_spring_pose_bone_rotations', 'calculate_object_pose_bone_rotations', 'calculate_joint_pair_head_pose_bone_rotations', 'get_bone_namespring', 'get_bone_name', 'spring_bone1', 'get_bone_extension', 'io_scene_vrm', 't_pose', 'blender']",
         "['spring bone: get_bone_namespring bone: get_bone_name\\n\\nget_bone_name\\n\\n\\nuuid\\n\\nget_bone_extension(bone).uuid\\n\\n\\n\\n```\\n         3607230 function calls in 2.629 seconds\\nordered by: internal time\\nncalls  tottime  percall  cumtime  percall filename:lineno(function)\\n    72100    0.831    0.000    1.097    0.000 /home/ubuntu/repos/vrm-addon-for-blender/src/io_scene_vrm/editor/spring_bone1/handler.py:526(calculate_joint_pair_head_pose_bone_rotations)\\n   158900    0.630    0.000    0.917    0.000 /home/ubuntu/repos/vrm-addon-for-blender/src/io_scene_vrm/editor/property_group.py:304(get_bone_name)\\n     2450    0.349    0.000    2.356    0.001 /home/ubuntu/repos/vrm-addon-for-blender/src/io_scene_vrm/editor/spring_bone1/handler.py:408(calculate_spring_pose_bone_rotations)\\n```\\n\\n```\\n         3607230 function calls in 2.593 seconds\\nordered by: internal time\\nncalls  tottime  percall  cumtime  percall filename:lineno(function)\\n    72100    0.835    0.000    1.096    0.000 /home/ubuntu/repos/vrm-addon-for-blender/src/io_scene_vrm/editor/spring_bone1/handler.py:526(calculate_joint_pair_head_pose_bone_rotations)\\n   158900    0.611    0.000    0.897    0.000 /home/ubuntu/repos/vrm-addon-for-blender/src/io_scene_vrm/editor/property_group.py:306(get_bone_name)\\n     2450    0.347    0.000    2.330    0.001 /home/ubuntu/repos/vrm-addon-for-blender/src/io_scene_vrm/editor/spring_bone1/handler.py:408(calculate_spring_pose_bone_rotations)\\n```\\n\\n\\nget_bone_name: 0.630 → 0.611 (3.0%)\\n: 2.629 → 2.593 (1.4%)\\n\\n\\nlink to  run:  <URL> ', \"spring bone: spring bone\\n\\ncalculate_joint_pair_head_pose_bone_rotations\\n\\ninverted_safe()\\n\\n\\n\\n\\n\\n\\n```\\n         3607230 function calls in 2.689 seconds\\nordered by: internal time\\nncalls  tottime  percall  cumtime  percall filename:lineno(function)\\n    72100    0.875    0.000    1.139    0.000 /home/ubuntu/repos/vrm-addon-for-blender/src/io_scene_vrm/editor/spring_bone1/handler.py:526(calculate_joint_pair_head_pose_bone_rotations)\\n   158900    0.635    0.000    0.923    0.000 /home/ubuntu/repos/vrm-addon-for-blender/src/io_scene_vrm/editor/property_group.py:304(get_bone_name)\\n     2450    0.360    0.000    2.411    0.001 /home/ubuntu/repos/vrm-addon-for-blender/src/io_scene_vrm/editor/spring_bone1/handler.py:408(calculate_spring_pose_bone_rotations)\\n   290850    0.082    0.000    0.082    0.000 {method 'inverted_safe' of 'matrix' objects}\\n```\\n\\n```\\n         3535130 function calls in 2.553 seconds\\nordered by: internal time\\nncalls  tottime  percall  cumtime  percall filename:lineno(function)\\n    72100    0.766    0.000    1.016    0.000 /home/ubuntu/repos/vrm-addon-for-blender/src/io_scene_vrm/editor/spring_bone1/handler.py:526(calculate_joint_pair_head_pose_bone_rotations)\\n   158900    0.621    0.000    0.907    0.000 /home/ubuntu/repos/vrm-addon-for-blender/src/io_scene_vrm/editor/property_group.py:304(get_bone_name)\\n     2450    0.367    0.000    2.280    0.001 /home/ubuntu/repos/vrm-addon-for-blender/src/io_scene_vrm/editor/spring_bone1/handler.py:408(calculate_spring_pose_bone_rotations)\\n   218750    0.063    0.000    0.063    0.000 {method 'inverted_safe' of 'matrix' objects}\\n```\\n\\n\\n: 2.689 → 2.553 (5.1%)\\ncalculate_joint_pair_head_pose_bone_rotations: 0.875 → 0.766 (12.5%)\\ninverted_safe: 290,850 → 218,750 (24.8%)\\ninverted_safe: 0.082 → 0.063 (23.2%)\\n\\n\\nlink to  run:  <URL> \", \"spring bonespring bone\\n\\nspring bone\\n\\nupdate_pose_bone_rotationsarmature\\nget_bone_name\\n\\n\\n\\n\\n\\n\\n```\\n         3607230 function calls in 2.715 seconds\\nordered by: internal time\\nncalls  tottime  percall  cumtime  percall filename:lineno(function)\\n    72100    0.867    0.000    1.134    0.000 /home/ubuntu/repos/vrm-addon-for-blender/src/io_scene_vrm/editor/spring_bone1/handler.py:526(calculate_joint_pair_head_pose_bone_rotations)\\n   158900    0.648    0.000    0.944    0.000 /home/ubuntu/repos/vrm-addon-for-blender/src/io_scene_vrm/editor/property_group.py:304(get_bone_name)\\n     2450    0.364    0.000    2.432    0.001 /home/ubuntu/repos/vrm-addon-for-blender/src/io_scene_vrm/editor/spring_bone1/handler.py:408(calculate_spring_pose_bone_rotations)\\n   387450    0.102    0.000    0.102    0.000 {method 'get' of 'bpy_prop_collection' objects}\\n   290850    0.082    0.000    0.082    0.000 {method 'inverted_safe' of 'matrix' objects}\\n   288400    0.039    0.000    0.039    0.000 {method 'to_translation' of 'matrix' objects}\\n```\\n\\n```\\n         3833340 function calls in 2.748 seconds\\nordered by: internal time\\nncalls  tottime  percall  cumtime  percall filename:lineno(function)\\n    72100    0.847    0.000    1.112    0.000 /home/ubuntu/repos/vrm-addon-for-blender/src/io_scene_vrm/editor/spring_bone1/handler.py:552(calculate_joint_pair_head_pose_bone_rotations)\\n   158900    0.638    0.000    0.927    0.000 /home/ubuntu/repos/vrm-addon-for-blender/src/io_scene_vrm/editor/property_group.py:304(get_bone_name)\\n     2450    0.422    0.000    2.469    0.001 /home/ubuntu/repos/vrm-addon-for-blender/src/io_scene_vrm/editor/spring_bone1/handler.py:422(calculate_spring_pose_bone_rotations)\\n   387450    0.097    0.000    0.097    0.000 {method 'get' of 'bpy_prop_collection' objects}\\n   290850    0.081    0.000    0.081    0.000 {method 'inverted_safe' of 'matrix' objects}\\n   288400    0.027    0.000    0.027    0.000 {method 'to_translation' of 'matrix' objects}\\n```\\n\\n\\n- calculate_joint_pair_head_pose_bone_rotations: 0.867 → 0.847 (2.3%)\\n- get_bone_name: 0.648 → 0.638 (1.5%)\\n- to_translation: 0.039 → 0.027 (30.8%)\\nto_translation30.8%calculate_joint_pair_head_pose_bone_rotations2.3%\\nspring bone\\nlink to  run\\n <URL> \"]"
        ],
        [
         "36",
         "35",
         "7",
         "35_compile_lua_testluacompiler_vmvalid_golden_testpythoncompiler_vmvalid_golden_lua",
         "['compile_lua', 'testluacompiler_vmvalid_golden', 'testpythoncompiler_vmvalid_golden', 'lua', 'compiler', 'testluacompiler_validprograms', 'compilersummary', 'improve', 'improvementssummary', 'testing']",
         "['lua compiler improvementssummary\\n\\nmake the lua compiler inline append operations when a list literal is given\\nregenerate lua machine outputs with updated compiler\\n\\ntesting\\n\\ngo test -tags slow ./compiler/x/lua -run testluacompiler_validprograms -count=1\\n\\n\\n <URL> ', 'lua compiler improvementssummary\\n\\noptimize lua compiler to skip helper calls when indexing simple lists or strings\\nupdate runtime boolean printing to match golden expectations\\nregenerate lua machine outputs\\nnote new capabilities in lua machine readme\\n\\ntesting\\n\\ngo test -tags slow ./compiler/x/lua -run vmvalid_golden (fails: 73 passed, 27 failed)\\n\\n\\n <URL> ', 'add main handling and faster list append for lua transpilersummary\\n\\nsupport programs with main by emitting a call at the end of generated lua\\noptimize append translation to mutate the list instead of copying\\nadd lua translation for pow builtin\\nmark brilliant-numbers as transpiled\\nadd transpiled lua and expected output for brilliant-numbers\\n\\ntesting\\n\\ngo test ./transpiler/x/lua -run rosetta -index 137 -tags slow -update -v (fails: took too long, interrupted)\\n\\n\\n <URL> ']"
        ],
        [
         "37",
         "36",
         "7",
         "36_fixes_cacheinspired_adapters_tasksfixe",
         "['fixes', 'cacheinspired', 'adapters', 'tasksfixe', 'gpt', 'optimize', 'prebid', 'adapter', 'cache', 'greedy']",
         "['prebid 10: disable greedy feature by default, break up long tasksfixe #10062', 'prebid 10: add yield helper and disable greedy by defaultredo  <URL>  for typescript base and fixes #10062\\nsee  <URL> ', 'ix bid adapter: cache gpt slot lookupstrying to spped up the enormous ix bid adapter']"
        ],
        [
         "38",
         "37",
         "6",
         "37_bench_memo_memoization_memoize_mochi_memo",
         "['bench_memo', 'memoization', 'memoize', 'mochi_memo', 'memoizationsummary', 'benchmarks', 'memoized', 'benchmark_onetomany', 'computationsummary', 'pytest']",
         "['add memoized fibonacci implementationsummary\\n\\ndownload task 365 from rosetta code (fibonacci sequence)\\nimplement memoized fibonacci example in mochi\\nprovide golden output running under vm\\n\\ntesting\\n\\nmochi_rosetta_only=fibonacci-sequence-5 go test ./runtime/vm -tags=slow -run rosetta -count=1\\n\\n\\n <URL> ', 'add optional memoization in interpretersummary\\n\\nadd a flag to toggle memoization of pure functions\\nwire the flag into mochi run and mochi test\\nskip memoization logic unless enabled\\n\\ntesting\\n\\ngo test ./...\\ngo run ./cmd/mochi-bench > /tmp/bench_default.txt\\nmochi_memo=1 go run ./cmd/mochi-bench > /tmp/bench_memo.txt\\n\\n\\n <URL> ', \"refactor(twap): implement strategy pattern for accumulator updatesthis commit refactors the twap module to use the strategy pattern more thoroughly for its accumulator updates. previously, the strategy pattern was only used for twap computation but not for the accumulator updates.\\nkey changes:\\n- add updateaccumulators method to the twapstrategy interface\\n- implement strategy-specific accumulator update logic for both arithmetic and geometric strategies\\n- modify getinterpolatedrecord to use the provided strategy's accumulator update method\\n- update remaining code to use the appropriate strategy for accumulator updates\\n- maintain backward compatibility in exported functions and existing code paths\\nwith this change, geometric accumulator calculations are now only performed when using the geometric strategy, making the system more efficient by avoiding unnecessary calculations for the arithmetic strategy.\\n generated with  code\\nco-authored-by:  noreply@anthropic.com\\ncloses: #7113\"]"
        ],
        [
         "39",
         "38",
         "6",
         "38_compilationfixes_compiler_compile_rust",
         "['compilationfixes', 'compiler', 'compile', 'rust', 'compilations', 'testrbcompiler_tpchqueries', 'compile_with_diagnostics', 'ruby', 'compilation', 'testcompileprograms']",
         "['fix memory usage by calling comemo::evict(10) after compilationfixes #104 \\nthis pr addresses excessive memory usage during compilation by calling comemo::evict(10) after each typst::compile() invocation, as suggested in the issue.\\nproblem\\nwhen generating large documents (e.g., reports with hundreds of pages), typst-py can consume excessive memory due to the comemo cache not being cleared between compilations. this is particularly problematic for long-running applications that perform multiple compilations.\\nsolution\\nadded comemo::evict(10) calls after each compilation to limit cache size and manage memory usage:\\n\\nafter the main typst::compile(self) call in compile_with_diagnostics\\nafter the html typst::compile::<htmldocument>(self) call for html format\\n\\nchanges\\n\\nadded use comemo; import to src/compiler.rs\\nadded memory eviction calls with explanatory comments\\nno breaking changes - all existing functionality preserved\\n\\n```rust\\nlet warned { output, warnings } = typst::compile(self);\\n// evict comemo cache to limit memory usage after compilation\\ncomemo::evict(10);\\n```\\ntesting\\n\\n all compilation formats (pdf, png, svg, html) work correctly\\n both function-based and compiler class usage patterns tested\\n memory-intensive scenarios with multiple large document compilations verified\\n build and functionality tests pass\\n\\nimpact\\nthis is a minimal, surgical fix that addresses the memory usage concern without affecting performance or functionality. the change helps applications that perform many compilations avoid memory leaks while maintaining fast compilation times for subsequent runs.\\n start  coding agent tips \\n\\n you can make  smarter by setting up custom instructions, customizing its development environment and configuring model context protocol (mcp) servers. learn more  coding agent tips in the docs.', 'refactor repl to eliminate temporary file usagethe repl implementation was using a \"huge hack\" of creating temporary files to conform with the existing compilation interface that expects a bufread object. this approach was inefficient and unnecessary.\\nchanges made\\nthis pr replaces the temporary file usage with std::io::<vec<u8>> which implements bufread and can be created directly from string content:\\nbefore:\\nrust\\n// create temporary file\\nlet file_res = tempfile();\\nlet mut file = file_res.unwrap();\\nfile.write_all(line.as_bytes());\\nfile.seek(seekfrom::start(0));\\nlet mut bufread: box<dyn bufread> = box::new(bufreader::new(file));\\nafter:\\nrust\\n// create in-memory buffer\\nlet mut bufread: box<dyn bufread> = box::new(::new(line.into_bytes()));\\nimpact\\n\\nrepl input processing: no longer creates temporary files for each command\\nexpression evaluation (-e option): no longer creates temporary files for expressions\\ncode reduction: removed 43 lines of error-prone file i/o code, added only 6 lines\\nperformance: eliminates filesystem operations for every repl interaction\\nreliability: removes potential issues with temporary file creation/cleanup\\n\\ntesting\\nverified that:\\n- repl functionality works identically to before\\n- expression evaluation with -e flag works correctly\\n- error handling remains functional\\n- no temporary files are created during operation\\n- edge cases (empty input, whitespace) are handled properly\\nthe refactoring successfully maintains compatibility with the existing compilation interface while eliminating the temporary file dependency.\\nfixes #8.\\n start  coding agent tips \\n\\n share your feedback on  coding agent for the chance to win a $200 gift card! click here to start the survey.', \"implement salsa-based incremental compilationsummary\\nthis pr implements salsa-based incremental compilation infrastructure for the rue compiler, enabling ide-friendly incremental updates.\\nchanges\\n\\nsalsa database setup: added basic ruedatabase type alias using salsa's databaseimpl\\nincremental file parsing: implemented parse_file as a salsa tracked function that automatically caches results\\ncomprehensive testing: added tests to verify incremental behavior works correctly (caching unchanged results)\\napi fixes: updated example file to use current api and added partialeq to parseerror for salsa compatibility\\n\\nkey benefits\\n\\nfast recompilation: only recomputes changed files and their dependents\\nide support: foundation for language server protocol implementation\\nmemory efficient: automatic result caching and invalidation\\nexpression-level granularity: future support for fine-grained incremental computation\\n\\ntesting\\n\\nall existing tests continue to pass\\nnew incremental compilation tests verify caching behavior\\nboth buck2 and cargo builds work correctly\\n\\narchitecture\\n```rust\\n// salsa input (can be modified)\\n[salsa::input]\\npub struct sourcefile { / path, text / }\\n// salsa tracked function (automatically cached)\\n[salsa::tracked]\\npub fn parse_file(db: &dyn database, file: sourcefile) -> result, arc\\\\>\\n// usage - salsa handles caching automatically\\nlet result = parse_file(&db, file);\\nfile.set_text(&mut db).to(new_content); // invalidates cache\\nlet new_result = parse_file(&db, file); // recomputes only if needed\\n```\\nnext steps\\nthis establishes the foundation for:\\n- semantic analysis queries\\n- type checking\\n- name resolution\\n- code generation\\n- lsp implementation\\n generated with  code\\nco-authored-by:  noreply@anthropic.com\"]"
        ],
        [
         "40",
         "39",
         "6",
         "39_tests_testing_test_client_test_sft",
         "['tests', 'testing', 'test_client', 'test_sft', 'pytest', 'test_supervised', 'test', 'lint', 'npm', 'compile_tpch_go']",
         "['split slow tests into separate filessummary\\n\\nseparate field-error tests into individual files\\nsplit zod-effects test into a second refinement test file\\n\\ntesting\\n\\nnpm run lint-fix\\nnpm run lint\\nnpm run tsc\\nnpm run test (fails: slow test file, some tests failed)', 'disable uv-runsummary\\n\\ndisable uv-run since it causes oom by reinstalling cuda torch every run\\n\\ntesting\\n\\nmake test (fails: plugin errors and network calls to huggingface.co blocked)\\n\\n\\n <URL> ', 'avoid network downloads in testssummary\\n\\nadd local_gpt2_tokenizer fixture to load gpt2 tokenizer from local files\\nswitch gpt2-using tests to rely on this fixture\\nuse passthrough tokenizer for validation-set test\\n\\ntesting\\n\\npre-commit run --all-files\\npytest tests/test_supervised.py tests/test_sft.py tests/test_text.py -m \"not entry and not slow and not ray\"\\n\\n\\n <URL> ']"
        ],
        [
         "41",
         "40",
         "6",
         "40_test_concurrency_testing_npm_run_with_redis",
         "['test_concurrency', 'testing', 'npm', 'run_with_redis', 'test', 'transaction_test', 'node_modules', 'pytest', 'run_scriptllmredis', 'js']",
         "['fix net server nodelay handlingsummary\\n\\nadd node.js test test-net-server-nodelay\\ncall socket.setnodelay() when server nodelay option is enabled\\n\\ntesting\\n\\nbun bd --silent node:test test-net-server-nodelay (fails: missing webkit build files)', 'speculative test for lockfile install hangsummary\\n\\nschedule dependency downloads when creating a new lockfile so async tasks run\\nadd regression test for new lockfile installs\\n\\ntesting\\n\\nnode_modules/.bin/prettier -w test/regression/issue/020850.test.ts\\nbun run clang-format (fails: could not download build dependencies)\\nbun run zig-format (fails: could not download build dependencies)\\nbun bd test test/regression/issue/020850.test.ts (fails: could not download build dependencies)\\n\\n\\n <URL> ', 'fix: \\n\\n src/api study/run \\n\\n\\ndb.session.begin_nested()db.session.commit()\\nredis\\nrun_scriptllmredis\\n\\n\\n\\nrun_with_redis\\n30\\n3\\n\\n\\n\\n\\n\\nreset_user_study_info_by_lesson\\n\\n\\n\\n\\n\\n\\n\\nget_script\\n\\n\\n\\n\\n\\n\\n\\nrun_script_inner\\n\\n\\n\\n\\n\\n\\n\\n1. test_concurrency.pystudy/runreset-study-progress\\n2. monitor_db.py\\n3. transaction_test.py\\n\\n\\n/\\nmonitor_db.py\\njmeterlocust\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nlink to  run:  <URL> \\nrequested by: geyunfei@gmail.com']"
        ],
        [
         "42",
         "41",
         "5",
         "41_dockerfile_caches_github_check_python_deps",
         "['dockerfile', 'caches', 'github', 'check_python_deps', 'cache', 'caching', 'docker', 'pnpm', 'npm', 'buildsummary']",
         "['ci: cache node setup in lint jobsummary\\n\\nupdate lint job to install node.js with caching before using pre-commit\\n\\ntesting\\n\\npre-commit run --files .github/workflows/ci.yml (failed: keyboardinterrupt during environment initialization)\\n\\n\\n <URL> ', ' enable docker buildkit cachingsummary\\n\\nenable docker buildx cache via github actions\\nrevert dockerfile cache mounts\\ndocument the workflow improvement in a changeset\\n\\ntesting\\n\\npnpm lint\\npnpm test:unit (fails: fetch failed)\\n\\n\\n <URL> ', 'ci: cache docs sandbox buildsummary\\n\\navoid repeated installs by installing docs deps first\\ncache sandbox docker build layers with buildx\\n\\ntesting\\n\\npre-commit run --files .github/workflows/docs.yml\\npython scripts/check_python_deps.py\\npytest tests/test_ping_agent.py tests/test_af_requests.py -q\\n\\n\\n <URL> ']"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 43
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>208</td>\n",
       "      <td>-1_github_dev_fixes_coding</td>\n",
       "      <td>[github, dev, fixes, coding, gradle, pnpm, age...</td>\n",
       "      <td>[add get-sqldscserverprotocol public command w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>360</td>\n",
       "      <td>0_fixes_coding_lint_implementation</td>\n",
       "      <td>[fixes, coding, lint, implementation, ui, esli...</td>\n",
       "      <td>[feat: implement async notification and teleme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>109</td>\n",
       "      <td>1_compiler_onnxscript_compile_runtime</td>\n",
       "      <td>[compiler, onnxscript, compile, runtime, x86, ...</td>\n",
       "      <td>[convert wormholecontract to sol_storage! macr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>47</td>\n",
       "      <td>2_alpha_agi_insight_v1_insight_browser_v1_alph...</td>\n",
       "      <td>[alpha_agi_insight_v1, insight_browser_v1, alp...</td>\n",
       "      <td>[[alpha_factory] tighten insight bundle size c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>42</td>\n",
       "      <td>3_mochi_benchmark_testswifttranspiler_rosetta_...</td>\n",
       "      <td>[mochi_benchmark, testswifttranspiler_rosetta_...</td>\n",
       "      <td>[add benchmark support to c++ transpiler tests...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>4_optimize_ci_github_caching_cache</td>\n",
       "      <td>[optimize_ci, github, caching, cache, git, cir...</td>\n",
       "      <td>[add vcpkg dependency caching to windows ci wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>5_compilehashouterjoin_compilejoinquery_outer_...</td>\n",
       "      <td>[compilehashouterjoin, compilejoinquery, outer...</td>\n",
       "      <td>[improve join performance with hashed left joi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>6_gpu_model_runner_webgpu_test_paged_attention...</td>\n",
       "      <td>[gpu_model_runner, webgpu, test_paged_attentio...</td>\n",
       "      <td>[[core] freeze gc during cuda graph capture to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>7_buildslotswithdateranges_scheduling_calendar...</td>\n",
       "      <td>[buildslotswithdateranges, scheduling, calenda...</td>\n",
       "      <td>[feat: optimize slot generation with inverted ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>8_mvn_wget_reentrantfilelocktestsummary_commit...</td>\n",
       "      <td>[mvn, wget, reentrantfilelocktestsummary, comm...</td>\n",
       "      <td>[implement kv batch putmanysummary\\n\\nadd putm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>9_dotnet_csproj_tests_inlining</td>\n",
       "      <td>[dotnet, csproj, tests, inlining, net9, struct...</td>\n",
       "      <td>[apply aggressiveinlining attributessummary\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>10_gradle_gradlew_sdk_kotlin</td>\n",
       "      <td>[gradle, gradlew, sdk, kotlin, kotlinx, perfor...</td>\n",
       "      <td>[fix heavy ui updates on main threadsummary\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>11_dotnetbuild_dotnet_dotnetbuildfromvmr_csproj</td>\n",
       "      <td>[dotnetbuild, dotnet, dotnetbuildfromvmr, cspr...</td>\n",
       "      <td>[make generatedepsfile and generateruntimeconf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>12_github_git_gitlab_githubtokenstorage</td>\n",
       "      <td>[github, git, gitlab, githubtokenstorage, file...</td>\n",
       "      <td>[fix github api rate limiting in chess workflo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13_pharmacyincomecostbillitemdto_opd_income_re...</td>\n",
       "      <td>[pharmacyincomecostbillitemdto, opd_income_rep...</td>\n",
       "      <td>[implement bill item dto for pharmacy income r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>14_testfortrancompiler_vmvalid_golden_fortran_...</td>\n",
       "      <td>[testfortrancompiler_vmvalid_golden, fortran, ...</td>\n",
       "      <td>[improve fortran compiler constant foldingsumm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>15_left_join_outer_join_testclojurecompiler_vm...</td>\n",
       "      <td>[left_join, outer_join, testclojurecompiler_vm...</td>\n",
       "      <td>[improve go join compilationsummary\\n\\nimprove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>16</td>\n",
       "      <td>11</td>\n",
       "      <td>16_messagecache_chat_messageids_useaiassistant...</td>\n",
       "      <td>[messagecache, chat, messageids, useaiassistan...</td>\n",
       "      <td>[feat: optimize duplicate checking in userealt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "      <td>17_csharp_debuggability_cs_extensions</td>\n",
       "      <td>[csharp, debuggability, cs, extensions, utf8js...</td>\n",
       "      <td>[optimize trimstacktrace to use spans instead ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>18_testprologcompiler_testtranspilergolden_tes...</td>\n",
       "      <td>[testprologcompiler, testtranspilergolden, tes...</td>\n",
       "      <td>[improve prolog transpilersummary\\n\\ntranspile...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>19_pagination_paginate_sitemap_paginated</td>\n",
       "      <td>[pagination, paginate, sitemap, paginated, sit...</td>\n",
       "      <td>[add yearly and monthly sitemap organization f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>20_debounce_github_pull_requests_preview_docs</td>\n",
       "      <td>[debounce, github, pull_requests, preview_docs...</td>\n",
       "      <td>[fix lag in pyplot example by implementing pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>21</td>\n",
       "      <td>10</td>\n",
       "      <td>21_optimized_blas_kernels_fp16_openmp_opencl_onnx</td>\n",
       "      <td>[optimized_blas_kernels_fp16, openmp, opencl, ...</td>\n",
       "      <td>[[wip] improve dft implementationdft implement...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>22</td>\n",
       "      <td>10</td>\n",
       "      <td>22_testgocompiler_goldenoutput_compiler_compil...</td>\n",
       "      <td>[testgocompiler_goldenoutput, compiler, compil...</td>\n",
       "      <td>[improve go compiler struct reusesummary\\n\\nen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>23</td>\n",
       "      <td>9</td>\n",
       "      <td>23_rustc_hash_rustc_hashing_lib_rs</td>\n",
       "      <td>[rustc_hash, rustc, hashing, lib_rs, hash, fxh...</td>\n",
       "      <td>[add database caching for folder scan results ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>24_getbusytimesfromteamlimitsforusers__getbusy...</td>\n",
       "      <td>[getbusytimesfromteamlimitsforusers, _getbusyt...</td>\n",
       "      <td>[perf: optimize team bookings query by fetchin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>25</td>\n",
       "      <td>9</td>\n",
       "      <td>25_repo_git_commit_build_source_files</td>\n",
       "      <td>[repo, git, commit, build_source_files, releas...</td>\n",
       "      <td>[optimize source file tree buildingsummary\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>26</td>\n",
       "      <td>8</td>\n",
       "      <td>26_calendarcache_cachedcalendarservice_getcach...</td>\n",
       "      <td>[calendarcache, cachedcalendarservice, getcach...</td>\n",
       "      <td>[feat: optimize calendar cache retrieval with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>27</td>\n",
       "      <td>8</td>\n",
       "      <td>27_optimizations_kernels_git_optimization</td>\n",
       "      <td>[optimizations, kernels, git, optimization, op...</td>\n",
       "      <td>[add benchmark for deserializing large added v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>28</td>\n",
       "      <td>8</td>\n",
       "      <td>28_metamask_metamaskwallet_rpc_block_number</td>\n",
       "      <td>[metamask, metamaskwallet, rpc, block_number, ...</td>\n",
       "      <td>[docs: update trusted hash guide[x] analyzed r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>29</td>\n",
       "      <td>7</td>\n",
       "      <td>29_scrape_tool_batch_scrape_tool_seleniumscrap...</td>\n",
       "      <td>[scrape_tool, batch_scrape_tool, seleniumscrap...</td>\n",
       "      <td>[implement retry-after header handling for imp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>30</td>\n",
       "      <td>7</td>\n",
       "      <td>30_testccompiler_vmvalid_golden_testccompiler_...</td>\n",
       "      <td>[testccompiler_vmvalid_golden, testccompiler_g...</td>\n",
       "      <td>[update c backend to use stack arrayssummary\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>31</td>\n",
       "      <td>7</td>\n",
       "      <td>31_git_build_options_disable_grpc_modules_build</td>\n",
       "      <td>[git, build_options, disable_grpc_modules, bui...</td>\n",
       "      <td>[build tag to exclude cloud archiver providers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>32</td>\n",
       "      <td>7</td>\n",
       "      <td>32_terrainpatch_meshes_rendering_patch</td>\n",
       "      <td>[terrainpatch, meshes, rendering, patch, patch...</td>\n",
       "      <td>[optimize billboard rendering for 3d particles...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>33</td>\n",
       "      <td>7</td>\n",
       "      <td>33_maven_azure_build_macos_build_windows</td>\n",
       "      <td>[maven, azure, build_macos, build_windows, bui...</td>\n",
       "      <td>[[azure pipelines] implement  build detection ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>34</td>\n",
       "      <td>7</td>\n",
       "      <td>34_calculate_spring_pose_bone_rotations_calcul...</td>\n",
       "      <td>[calculate_spring_pose_bone_rotations, calcula...</td>\n",
       "      <td>[spring bone: get_bone_namespring bone: get_bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>35_compile_lua_testluacompiler_vmvalid_golden_...</td>\n",
       "      <td>[compile_lua, testluacompiler_vmvalid_golden, ...</td>\n",
       "      <td>[lua compiler improvementssummary\\n\\nmake the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "      <td>36_fixes_cacheinspired_adapters_tasksfixe</td>\n",
       "      <td>[fixes, cacheinspired, adapters, tasksfixe, gp...</td>\n",
       "      <td>[prebid 10: disable greedy feature by default,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>37</td>\n",
       "      <td>6</td>\n",
       "      <td>37_bench_memo_memoization_memoize_mochi_memo</td>\n",
       "      <td>[bench_memo, memoization, memoize, mochi_memo,...</td>\n",
       "      <td>[add memoized fibonacci implementationsummary\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>38</td>\n",
       "      <td>6</td>\n",
       "      <td>38_compilationfixes_compiler_compile_rust</td>\n",
       "      <td>[compilationfixes, compiler, compile, rust, co...</td>\n",
       "      <td>[fix memory usage by calling comemo::evict(10)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>39</td>\n",
       "      <td>6</td>\n",
       "      <td>39_tests_testing_test_client_test_sft</td>\n",
       "      <td>[tests, testing, test_client, test_sft, pytest...</td>\n",
       "      <td>[split slow tests into separate filessummary\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>40</td>\n",
       "      <td>6</td>\n",
       "      <td>40_test_concurrency_testing_npm_run_with_redis</td>\n",
       "      <td>[test_concurrency, testing, npm, run_with_redi...</td>\n",
       "      <td>[fix net server nodelay handlingsummary\\n\\nadd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>41</td>\n",
       "      <td>5</td>\n",
       "      <td>41_dockerfile_caches_github_check_python_deps</td>\n",
       "      <td>[dockerfile, caches, github, check_python_deps...</td>\n",
       "      <td>[ci: cache node setup in lint jobsummary\\n\\nup...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic  Count                                               Name  \\\n",
       "0      -1    208                         -1_github_dev_fixes_coding   \n",
       "1       0    360                 0_fixes_coding_lint_implementation   \n",
       "2       1    109              1_compiler_onnxscript_compile_runtime   \n",
       "3       2     47  2_alpha_agi_insight_v1_insight_browser_v1_alph...   \n",
       "4       3     42  3_mochi_benchmark_testswifttranspiler_rosetta_...   \n",
       "5       4     20                 4_optimize_ci_github_caching_cache   \n",
       "6       5     19  5_compilehashouterjoin_compilejoinquery_outer_...   \n",
       "7       6     18  6_gpu_model_runner_webgpu_test_paged_attention...   \n",
       "8       7     18  7_buildslotswithdateranges_scheduling_calendar...   \n",
       "9       8     16  8_mvn_wget_reentrantfilelocktestsummary_commit...   \n",
       "10      9     15                     9_dotnet_csproj_tests_inlining   \n",
       "11     10     15                       10_gradle_gradlew_sdk_kotlin   \n",
       "12     11     15    11_dotnetbuild_dotnet_dotnetbuildfromvmr_csproj   \n",
       "13     12     13            12_github_git_gitlab_githubtokenstorage   \n",
       "14     13     13  13_pharmacyincomecostbillitemdto_opd_income_re...   \n",
       "15     14     12  14_testfortrancompiler_vmvalid_golden_fortran_...   \n",
       "16     15     12  15_left_join_outer_join_testclojurecompiler_vm...   \n",
       "17     16     11  16_messagecache_chat_messageids_useaiassistant...   \n",
       "18     17     11              17_csharp_debuggability_cs_extensions   \n",
       "19     18     10  18_testprologcompiler_testtranspilergolden_tes...   \n",
       "20     19     10           19_pagination_paginate_sitemap_paginated   \n",
       "21     20     10      20_debounce_github_pull_requests_preview_docs   \n",
       "22     21     10  21_optimized_blas_kernels_fp16_openmp_opencl_onnx   \n",
       "23     22     10  22_testgocompiler_goldenoutput_compiler_compil...   \n",
       "24     23      9                 23_rustc_hash_rustc_hashing_lib_rs   \n",
       "25     24      9  24_getbusytimesfromteamlimitsforusers__getbusy...   \n",
       "26     25      9              25_repo_git_commit_build_source_files   \n",
       "27     26      8  26_calendarcache_cachedcalendarservice_getcach...   \n",
       "28     27      8          27_optimizations_kernels_git_optimization   \n",
       "29     28      8        28_metamask_metamaskwallet_rpc_block_number   \n",
       "30     29      7  29_scrape_tool_batch_scrape_tool_seleniumscrap...   \n",
       "31     30      7  30_testccompiler_vmvalid_golden_testccompiler_...   \n",
       "32     31      7    31_git_build_options_disable_grpc_modules_build   \n",
       "33     32      7             32_terrainpatch_meshes_rendering_patch   \n",
       "34     33      7           33_maven_azure_build_macos_build_windows   \n",
       "35     34      7  34_calculate_spring_pose_bone_rotations_calcul...   \n",
       "36     35      7  35_compile_lua_testluacompiler_vmvalid_golden_...   \n",
       "37     36      7          36_fixes_cacheinspired_adapters_tasksfixe   \n",
       "38     37      6       37_bench_memo_memoization_memoize_mochi_memo   \n",
       "39     38      6          38_compilationfixes_compiler_compile_rust   \n",
       "40     39      6              39_tests_testing_test_client_test_sft   \n",
       "41     40      6     40_test_concurrency_testing_npm_run_with_redis   \n",
       "42     41      5      41_dockerfile_caches_github_check_python_deps   \n",
       "\n",
       "                                       Representation  \\\n",
       "0   [github, dev, fixes, coding, gradle, pnpm, age...   \n",
       "1   [fixes, coding, lint, implementation, ui, esli...   \n",
       "2   [compiler, onnxscript, compile, runtime, x86, ...   \n",
       "3   [alpha_agi_insight_v1, insight_browser_v1, alp...   \n",
       "4   [mochi_benchmark, testswifttranspiler_rosetta_...   \n",
       "5   [optimize_ci, github, caching, cache, git, cir...   \n",
       "6   [compilehashouterjoin, compilejoinquery, outer...   \n",
       "7   [gpu_model_runner, webgpu, test_paged_attentio...   \n",
       "8   [buildslotswithdateranges, scheduling, calenda...   \n",
       "9   [mvn, wget, reentrantfilelocktestsummary, comm...   \n",
       "10  [dotnet, csproj, tests, inlining, net9, struct...   \n",
       "11  [gradle, gradlew, sdk, kotlin, kotlinx, perfor...   \n",
       "12  [dotnetbuild, dotnet, dotnetbuildfromvmr, cspr...   \n",
       "13  [github, git, gitlab, githubtokenstorage, file...   \n",
       "14  [pharmacyincomecostbillitemdto, opd_income_rep...   \n",
       "15  [testfortrancompiler_vmvalid_golden, fortran, ...   \n",
       "16  [left_join, outer_join, testclojurecompiler_vm...   \n",
       "17  [messagecache, chat, messageids, useaiassistan...   \n",
       "18  [csharp, debuggability, cs, extensions, utf8js...   \n",
       "19  [testprologcompiler, testtranspilergolden, tes...   \n",
       "20  [pagination, paginate, sitemap, paginated, sit...   \n",
       "21  [debounce, github, pull_requests, preview_docs...   \n",
       "22  [optimized_blas_kernels_fp16, openmp, opencl, ...   \n",
       "23  [testgocompiler_goldenoutput, compiler, compil...   \n",
       "24  [rustc_hash, rustc, hashing, lib_rs, hash, fxh...   \n",
       "25  [getbusytimesfromteamlimitsforusers, _getbusyt...   \n",
       "26  [repo, git, commit, build_source_files, releas...   \n",
       "27  [calendarcache, cachedcalendarservice, getcach...   \n",
       "28  [optimizations, kernels, git, optimization, op...   \n",
       "29  [metamask, metamaskwallet, rpc, block_number, ...   \n",
       "30  [scrape_tool, batch_scrape_tool, seleniumscrap...   \n",
       "31  [testccompiler_vmvalid_golden, testccompiler_g...   \n",
       "32  [git, build_options, disable_grpc_modules, bui...   \n",
       "33  [terrainpatch, meshes, rendering, patch, patch...   \n",
       "34  [maven, azure, build_macos, build_windows, bui...   \n",
       "35  [calculate_spring_pose_bone_rotations, calcula...   \n",
       "36  [compile_lua, testluacompiler_vmvalid_golden, ...   \n",
       "37  [fixes, cacheinspired, adapters, tasksfixe, gp...   \n",
       "38  [bench_memo, memoization, memoize, mochi_memo,...   \n",
       "39  [compilationfixes, compiler, compile, rust, co...   \n",
       "40  [tests, testing, test_client, test_sft, pytest...   \n",
       "41  [test_concurrency, testing, npm, run_with_redi...   \n",
       "42  [dockerfile, caches, github, check_python_deps...   \n",
       "\n",
       "                                  Representative_Docs  \n",
       "0   [add get-sqldscserverprotocol public command w...  \n",
       "1   [feat: implement async notification and teleme...  \n",
       "2   [convert wormholecontract to sol_storage! macr...  \n",
       "3   [[alpha_factory] tighten insight bundle size c...  \n",
       "4   [add benchmark support to c++ transpiler tests...  \n",
       "5   [add vcpkg dependency caching to windows ci wo...  \n",
       "6   [improve join performance with hashed left joi...  \n",
       "7   [[core] freeze gc during cuda graph capture to...  \n",
       "8   [feat: optimize slot generation with inverted ...  \n",
       "9   [implement kv batch putmanysummary\\n\\nadd putm...  \n",
       "10  [apply aggressiveinlining attributessummary\\n\\...  \n",
       "11  [fix heavy ui updates on main threadsummary\\n\\...  \n",
       "12  [make generatedepsfile and generateruntimeconf...  \n",
       "13  [fix github api rate limiting in chess workflo...  \n",
       "14  [implement bill item dto for pharmacy income r...  \n",
       "15  [improve fortran compiler constant foldingsumm...  \n",
       "16  [improve go join compilationsummary\\n\\nimprove...  \n",
       "17  [feat: optimize duplicate checking in userealt...  \n",
       "18  [optimize trimstacktrace to use spans instead ...  \n",
       "19  [improve prolog transpilersummary\\n\\ntranspile...  \n",
       "20  [add yearly and monthly sitemap organization f...  \n",
       "21  [fix lag in pyplot example by implementing pro...  \n",
       "22  [[wip] improve dft implementationdft implement...  \n",
       "23  [improve go compiler struct reusesummary\\n\\nen...  \n",
       "24  [add database caching for folder scan results ...  \n",
       "25  [perf: optimize team bookings query by fetchin...  \n",
       "26  [optimize source file tree buildingsummary\\n\\n...  \n",
       "27  [feat: optimize calendar cache retrieval with ...  \n",
       "28  [add benchmark for deserializing large added v...  \n",
       "29  [docs: update trusted hash guide[x] analyzed r...  \n",
       "30  [implement retry-after header handling for imp...  \n",
       "31  [update c backend to use stack arrayssummary\\n...  \n",
       "32  [build tag to exclude cloud archiver providers...  \n",
       "33  [optimize billboard rendering for 3d particles...  \n",
       "34  [[azure pipelines] implement  build detection ...  \n",
       "35  [spring bone: get_bone_namespring bone: get_bo...  \n",
       "36  [lua compiler improvementssummary\\n\\nmake the ...  \n",
       "37  [prebid 10: disable greedy feature by default,...  \n",
       "38  [add memoized fibonacci implementationsummary\\...  \n",
       "39  [fix memory usage by calling comemo::evict(10)...  \n",
       "40  [split slow tests into separate filessummary\\n...  \n",
       "41  [fix net server nodelay handlingsummary\\n\\nadd...  \n",
       "42  [ci: cache node setup in lint jobsummary\\n\\nup...  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "acd63a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-07 13:04:15,030 - BERTopic - WARNING: Using a custom list of topic assignments may lead to errors if topic reduction techniques are used afterwards. Make sure that manually assigning topics is the last step in the pipeline.Note that topic embeddings will also be created through weightedc-TF-IDF embeddings instead of centroid embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Topic",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Representation",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Representative_Docs",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "b4346252-4aed-470a-b81b-72411d5c3e77",
       "rows": [
        [
         "0",
         "0",
         "463",
         "0_coding_fixes_cache_optimizations",
         "['coding', 'fixes', 'cache', 'optimizations', 'implementation', 'async', 'development', 'agent', 'ui', 'cli']",
         "['feat: implement async notification and telemetry system (phase 1-3)summary\\nthis pr implements the first three phases of the async notification and telemetry system as outlined in #833. it introduces a non-blocking event bus architecture that decouples error reporting from notification/telemetry processing, preventing any blocking operations during error handling.\\nrelated issues\\n\\nimplements phases 1-3 of #833 (async notification/telemetry system)\\naddresses performance concerns from #825 (error handling optimization)\\nincludes error deduplication from #827 (reduce telemetry noise)\\n\\nchanges\\nphase 1: core event bus infrastructure \\n\\ncreated internal/events package with non-blocking event bus\\nimplemented worker pool pattern with configurable workers (default: 4)\\nadded trypublish() method that never blocks (drops events if buffer full)\\ncomprehensive unit tests with 100% coverage\\nstructured logging with internal/logging package\\natomic operations for thread-safe metrics\\n\\nphase 2: error deduplication system \\n\\nhash-based deduplication with configurable ttl (default: 5 minutes)\\nlru eviction for memory-bounded cache (max 10,000 entries)\\nperiodic cleanup goroutine for expired entries\\ncomprehensive deduplication metrics (hit rate, suppression count)\\nreduces telemetry volume by suppressing duplicate errors\\n\\nphase 3: error package integration \\n\\nenhanced enhancederror to implement errorevent interface\\ncreated eventpublisher interface to avoid circular dependencies\\nadapter pattern connects errors and events packages\\nmaintains backward compatibility - falls back to sync processing if event bus not initialized\\nverified no circular dependencies through compilation tests\\n\\narchitecture\\nerrors package → eventbus → deduplication → notification workers (future)\\n                                         ↘ → telemetry workers (future)\\nkey design principles\\n\\nzero-cost when disabled: no overhead when telemetry/notifications are off\\nnon-blocking guarantees: trypublish() never blocks, uses select with default\\nno circular dependencies: uses interfaces to decouple packages\\nbackward compatible: falls back to legacy sync processing\\nproduction ready: proper error handling, metrics, and tests\\n\\nperformance characteristics\\n\\nerror creation overhead: < 100ns when telemetry disabled (maintains #825 optimizations)\\nevent publishing: non-blocking with overflow protection\\ndeduplication: o(1) hash lookup with < 100ns overhead\\nmemory usage: bounded by configuration (10k events max)\\nzero goroutine leaks verified\\n\\ntesting\\n\\ncomprehensive unit tests for all components\\nintegration tests verify no circular dependencies\\nfixed deadlock issues in error hooks\\nproper test isolation and cleanup\\nall tests pass without timeouts or race conditions\\n\\nconfiguration\\nthe system supports configuration through the new event bus config:\\n```go\\ntype config struct {\\n    buffersize    int                    // event buffer size (default: 10,000)\\n    workers       int                    // worker goroutines (default: 4)\\n    enabled       bool                   // enable event bus (default: true)\\n    deduplication *deduplicationconfig   // deduplication settings\\n}\\ntype deduplicationconfig struct {\\n    enabled         bool          // enable deduplication (default: true)\\n    ttl             time.duration // duplicate window (default: 5m)\\n    maxentries      int          // max cache size (default: 10,000)\\n    cleanupinterval time.duration // cleanup frequency (default: 1m)\\n}\\n```\\nnext steps\\nthis pr lays the foundation for async processing. future phases will:\\n- phase 4: migrate notification system to use event bus workers\\n- phase 5: migrate telemetry system with batching and circuit breakers\\n- phase 6: remove legacy sync processing code\\n- phase 7: add monitoring and production tuning\\nbreaking changes\\nnone. the system maintains full backward compatibility.\\nchecklist\\n\\n[x] tests pass\\n[x] linter passes (golangci-lint run)\\n[x] no circular dependencies\\n[x] backward compatible\\n[x] performance requirements met\\n[x] documentation updated\\n\\nhow to test\\n\\nrun tests: go test ./internal/events/... ./internal/errors/...\\nverify no circular dependencies compile\\ncheck deduplication with repeated errors\\nconfirm non-blocking behavior under load\\n\\n generated with  code\\nco-authored-by:  noreply@anthropic.com\\n this is an auto-generated comment: release notes by coderabbit.ai \\nsummary by coderabbit\\n\\nnew features\\nintroduced an asynchronous event bus system for non-blocking error event processing with deduplication and metrics.\\nadded error deduplication to suppress duplicate error events within a configurable time window.\\nprovided integration between error reporting and the event bus for improved decoupling and extensibility.\\n\\nadded new error accessors for retrieving underlying error and message details.\\n\\n\\nbug fixes\\n\\n\\nimproved thread safety and encapsulation in error context handling.\\n\\n\\ntests\\n\\n\\nadded comprehensive unit and integration tests for event bus, deduplication, and error-event integration.\\n\\n\\nrefactor\\n\\nupdated error category handling to use string values for improved consistency.\\n\\nimproved synchronization and state management in error hook and telemetry logic.\\n\\n\\ndocumentation\\n\\nexpanded best practices and lessons learned on defensive coding, testing, atomic usage, performance, and error handling.\\n\\n end of auto-generated comment: release notes by coderabbit.ai', \"performance optimizations and code quality improvements for gsy github app flutter overview\\nthis pr implements comprehensive performance optimizations and code quality improvements for the gsy github app flutter project. the changes focus on reducing memory usage, improving build performance, and enhancing code maintainability while preserving all existing functionality.\\n key optimizations\\nwidget performance improvements\\n\\nconst constructor optimizations: converted widgets like gsycarditem, gsyflexbutton, and gsyicontext to use const constructors with static constants, reducing object creation during builds\\nstatelesswidget conversion: converted gsyinputwidget from statefulwidget to statelesswidget for better performance\\nwidget building logic: optimized gsytitlebar with better conditional rendering and sizedbox.shrink() instead of empty container()\\n\\nnetwork layer optimizations\\n\\n <URL>  singleton: implemented proper singleton pattern with lazy initialization and batch interceptor addition\\nresource management: improved memory management and reduced object creation\\n\\ndatabase layer improvements\\n\\nsqlmanager optimization: added singleton pattern with initialization state management and optimized table existence queries\\nbasedbprovider enhancement: improved type safety, error handling, and resource management\\n\\napplication layer optimizations\\n\\nlocale checking: optimized language locale checking in app.dart using any() instead of loops\\nlogger performance: made logger debug-aware to reduce memory usage in production builds\\nerror handling: improved errorpage with constants and better code structure\\n\\nbuild & dependency management\\n\\npubspec.yaml cleanup: removed commented dependencies and organized remaining ones by category with documentation\\nanalysis_options.yaml: added performance-oriented lint rules including const constructor checks\\nenhanced .gitignore: added more build artifacts and platform-specific exclusions\\n\\n performance benefits\\nmemory usage\\n\\nreduced runtime object creation through static constants\\nbetter resource management with proper singleton patterns\\ndebug-aware logging reduces production memory footprint\\n\\nbuild performance\\n\\nmore const constructors enable flutter's build optimizations\\nreduced widget tree rebuilds through statelesswidget conversions\\noptimized database initialization prevents redundant operations\\n\\ncode quality\\n\\nimproved type safety and null safety handling\\nbetter error handling and resource cleanup\\nenhanced code documentation and organization\\n\\n technical details\\nbefore/after examples\\nwidget optimization example:\\n```dart\\n// before: runtime object creation\\nwidget build(buildcontext context) {\\n  edgeinsets? margin = this.margin;\\n  margin ??= const edgeinsets.only(left: 10.0, top: 10.0, right: 10.0, bottom: 10.0);\\n  // ...\\n}\\n// after: compile-time constants\\nstatic const edgeinsets _defaultmargin = edgeinsets.only(left: 10.0, top: 10.0, right: 10.0, bottom: 10.0);\\nconst gsycarditem({\\n  this.margin = _defaultmargin,\\n  // ...\\n});\\n```\\ndatabase optimization example:\\n```dart\\n// before: potential multiple initializations\\nstatic init() async {\\n  _database = await opendatabase(path, version: _version);\\n}\\n// after: singleton with state management\\nstatic future init() async {\\n  if (_database != null) return;\\n  if (_isinitializing) {\\n    await _dbcompleter.future;\\n    return;\\n  }\\n  // safe initialization logic\\n}\\n```\\n documentation\\nadded comprehensive optimizations.md documenting all changes and their performance impact, serving as a reference for future optimization efforts.\\n testing\\nall optimizations maintain backward compatibility and preserve existing functionality. the changes focus on implementation improvements without altering public apis.\\n impact\\nthese optimizations provide:\\n-  faster app startup and navigation\\n-  reduced memory consumption\\n-  better code maintainability\\n-  improved user experience\\n-  enhanced developer experience with better linting\\n\\nnote: these changes follow flutter best practices and are designed to be non-breaking while providing measurable performance improvements.\\n\\n[!warning]\\n\\nfirewall rules blocked me from connecting to one or more addresses\\ni tried to connect to the following addresses, but was blocked by firewall rules:\\n\\ncanonical-bos01.cdn.snapcraftcontent.com\\nesm.ubuntu.com\\ntriggering command: /usr/lib/apt/methods/ <URL>  (dns block)\\n <URL> \\ntriggering command: curl --retry 3 --continue-at - --location --output /home/redacted/flutter/bin/cache/dart-sdk-linux-x64.zip redacted (http block)\\n <URL> \\ntriggering command: wget -o flutter.tar.xz redacted (http block)\\n\\nif you need me to access, download, or install something from one of these locations, you can either:\\n\\nconfigure actions setup steps to set up my environment, which run before the firewall is enabled\\nadd the appropriate urls or hosts to the custom allowlist in this repository's  coding agent settings (admins only)\\n\\n\\n\\n start  coding agent tips \\n\\n you can make  smarter by setting up custom instructions, customizing its development environment and configuring model context protocol (mcp) servers. learn more  coding agent tips in the docs.\", 'add simd optimizations for 23.5% performance improvementsummary\\nthis pr implements comprehensive simd optimizations for the probe code search engine, addressing the challenge that bm25 simd wasn\\'t providing expected performance gains due to sparse vector characteristics. instead of abandoning simd, we pivoted to target string processing operations where simd acceleration excels.\\nthe journey: from bm25 to string processing simd\\ninitial challenge: after implementing bm25 simd optimizations, we discovered they weren\\'t delivering meaningful performance improvements. the core issue was that bm25 operates on sparse vectors (most terms have zero scores), making vectorized operations less effective than anticipated.\\nstrategic pivot: rather than abandon simd entirely, we analyzed the codebase to identify workloads that could genuinely benefit from simd acceleration. we found that string processing operations - tokenization and pattern matching - were ideal candidates as they process dense character data where simd truly shines.\\nimplementation approach: we implemented two separate architect-driven solutions:\\n1. simd-accelerated camelcase splitting in tokenization\\n2. simd-accelerated multi-term pattern matching\\nevolution to production: the implementation evolved through several key phases:\\n- initial simd tokenization showing 7.2% improvement\\n- integration challenges with parallel processing requiring arc wrappers\\n- hybrid pattern matching combining simd with ripgrep fallbacks\\n- thread safety improvements replacing environment variable manipulation\\n- default-enabled configuration with opt-out flags\\nperformance improvements\\ndetailed performance analysis\\ntest environment:\\n- query: \"yaml workflow agent multi-agent user input\"\\n- target: ~/go/src/semantic-kernel/ (large codebase)\\n- method: built binaries comparison (cargo build --release)\\ncomprehensive timing breakdown:\\n| metric | old version | new version (simd) | improvement | time saved |\\n|--------|-------------|-------------------|-------------|------------|\\n| total time | 1053.97ms | 929.82ms | 11.8% | 124.15ms |\\n| file scanning | 24.44ms (2.3%) | 23.63ms (2.5%) | 3.3% | 0.81ms |\\n| term matching | 867.00ms (82.3%) | 719.75ms (77.4%) | 17.0% | 147.25ms |\\n| ast parsing | 118.65ms (11.3%) | 139.02ms (14.9%) | -17.2% | -20.37ms |\\n| ranking | 35.42ms (3.4%) | 39.49ms (4.2%) | -11.5% | -4.07ms |\\n| result formatting | 8.46ms (0.8%) | 7.93ms (0.9%) | 6.3% | 0.53ms |\\nkey insights:\\n- massive term matching improvement: 17.0% faster (147.25ms saved)\\n- overall performance gain: 11.8% improvement despite some overhead\\n- primary bottleneck addressed: term matching (82.3% → 77.4% of total time)\\nsimd tokenization benchmark\\nsimple query performance:\\n```\\nquery: \"agent workflow\"\\ntarget: ~/go/src/semantic-kernel/\\nbefore simd tokenization: 841.74ms\\nafter simd tokenization: 780.90ms\\nimprovement: 7.2% (60.84ms faster)\\n```\\ncomparative strategy analysis\\nhybrid vs always-simd vs always-ripgrep testing:\\n```\\npattern matching strategy comparison:\\n hybrid (simd + ripgrep): 13.9% improvement (best overall)\\n always-simd: 11.2% improvement\\n always-ripgrep: baseline performance\\nconclusion: hybrid approach optimal for diverse pattern complexity\\n```\\nsimd features implemented\\n1. simd-accelerated tokenization (src/search/simd_tokenization.rs)\\n\\nfast camelcase boundary detection using character classification tables\\nsimd-accelerated ascii character processing with 256-element lookup table\\nsmart fallback to scalar implementation for unicode or complex patterns like oauth2, xml, http\\nthread-safe configuration system replacing environment variable manipulation\\nhandles complex patterns: xml <URL>  → [\"xml\", \" <URL>  \"request\"]\\n\\n2. simd pattern matching (src/search/simd_pattern_matching.rs)\\n\\nmulti-pattern string matching using memchr and aho-corasick\\nhybrid intelligence: automatically detects pattern complexity and chooses optimal strategy:\\nsimd for simple literal patterns (faster)\\nripgrep for complex regex patterns (maintains compatibility)\\npattern complexity analysis checks for regex metacharacters like \\\\b, (?i)\\nseamless integration with existing search pipeline\\n\\n3. enhanced simd ranking (src/search/result_ranking.rs)\\n\\nelement-wise simd multiplication for bm25 scoring using simsimd\\noptimized sparse-to-dense vector conversion reducing memory allocations\\nmemory allocation optimization for better cache performance\\nthread-safe configuration without environment variable races\\n\\narchitecture improvements & problem solving\\nthread safety crisis & resolution\\nproblem: initial implementation used std::env::set_var() for recursive call prevention, causing thread safety issues in concurrent scenarios.\\nsolution: implemented simdconfig struct with explicit configuration passing:\\nrust\\npub struct simdconfig {\\n    pub simd_enabled: bool,\\n    pub in_recursive_call: bool,\\n}\\nthis eliminated all environment variable manipulation and race conditions.\\nmerge strategy evolution\\nchallenge: rebasing the feature branch on main created complex merge conflicts.\\nresolution: switched from rebase to merge strategy, which provided cleaner conflict resolution. used a specialized agent to handle complex search_runner.rs conflicts, resulting in the optimal hybrid simd/ripgrep implementation.\\nc# language support fix\\nissue discovered: during benchmarking, found that c# files were showing \"unknown\" language.\\nroot cause: missing c# mapping in formatter and tree-sitter compatibility issue.\\nfix: added proper c# language detection and fixed unsafe transmute operations.\\ntechnical deep dive\\ncharacter classification table optimization\\nrust\\n// simd lookup table for fast ascii character classification\\nstatic char_class_table: [u8; 256] = [\\n    // each byte: bit 0 = uppercase, bit 1 = lowercase, bit 2 = digit\\n    // enables simd boundary detection in single table lookup\\n];\\nhybrid pattern selection logic\\nrust\\nlet use_simd = crate::search::simd_pattern_matching::is_simd_pattern_matching_enabled()\\n    && pattern_strings.iter().all(|p| \\\\!p.contains(r\"\\\\b\") && \\\\!p.contains(\"(?i)\"));\\nconfiguration system design\\n\\ndefault behavior: simd enabled by default for maximum performance\\nopt-out flags: disable_simd_tokenization=1, disable_simd_pattern_matching=1, disable_simd_ranking=1\\ngraceful fallback: automatic detection of simd capability and intelligent degradation\\n\\ndependencies & integration\\nnew dependencies:\\n- memchr = \"2.7\" - simd-accelerated string searching (used by ripgrep internally)\\n- wide = \"0.7\" - simd vector operations for character classification\\n- aho-corasick = \"1.1\" - multi-pattern string matching with simd acceleration\\nintegration points:\\n- seamless integration with existing tokenization pipeline\\n- backward-compatible api with configuration parameter addition\\n- zero breaking changes to public interfaces\\nquality assurance & testing\\ncomprehensive test coverage\\n\\nequivalence testing: simd results must match scalar implementations exactly\\nthread safety testing: concurrent execution with different configurations\\ncomplex pattern testing: xml <URL>  oauth2provider, parsejson2html5\\nperformance regression testing: automated benchmarking against baseline\\n\\nerror resolution journey\\n\\ncharacter table size mismatch: fixed 257→256 element array\\nprivate function access: resolved import scope issues\\ntype mismatches: fixed f64→f32 conversions for simsimd\\nmerge conflicts: strategic resolution preserving both simd and ripgrep benefits\\ntest failures: fixed boundary detection for complex camelcase patterns\\n\\nproduction readiness\\nbackward compatibility\\n\\nfull backward compatibility maintained\\ngraceful degradation on platforms without simd support\\nno breaking changes to public apis\\nexisting tests pass with simd optimizations enabled\\n\\nperformance validation\\n\\nreal-world testing: benchmarks against actual codebases (semantic-kernel)\\nmultiple query types: both simple and complex query patterns tested\\nconsistent improvements: 7.2% to 17.0% improvements across different scenarios\\n\\nfuture implications\\nthis implementation demonstrates that strategic simd application yields better results than broad simd adoption. by focusing on string processing operations where simd naturally excels, we achieved significant performance improvements while maintaining code clarity and reliability.\\nthe hybrid approach preserves the benefits of both worlds: simd speed for simple operations and ripgrep\\'s sophisticated regex engine for complex patterns.\\n generated with  code\\nco-authored-by:  noreply@anthropic.com']"
        ],
        [
         "1",
         "1",
         "137",
         "1_compiler_testvm_ir_compile_onnxscript",
         "['compiler', 'testvm_ir', 'compile', 'onnxscript', 'x86', 'bytecode', 'vm', 'testvm', 'testvm_tpch', 'runtime']",
         "['convert wormholecontract to sol_storage! macro for testvm compatibilitywormholecontract size optimization: achieved 24 kib target \\nsummary\\nsuccessfully reduced wormholecontract size from 25.2 kb to 12.5 kb (12.7 kb reduction) by removing the k256 cryptographic dependency and disabling signature verification. this achieves the stylus size requirement of under 24 kib contract size while maintaining testvm integration functionality.\\nchanges made\\n1. testvm integration (original goal) \\n\\nconverted wormholecontract from #[storage] to sol_storage! macro\\nenabled wormholecontract::from(&testvm::default()) pattern for testing\\nall tests now use proper stylus sdk testing framework\\n\\n2. size optimization (primary goal) \\n\\ncontract size: 25.2 kb → 12.5 kb (12.7 kb reduction)\\nwasm size: 82.3 kb (well under 100 kib target)\\nremoved k256 dependency entirely from workspace and contract cargo.toml\\nreplaced complex verify_signature function with stub that returns ok(true)\\npreserved on-chain storage structure (no changes to stored elements)\\n\\nsize optimization results\\n| metric | before | after | target | status |\\n|--------|--------|-------|--------|--------|\\n| contract size | 25.2 kb | 12.5 kb | < 24 kib |  50% reduction |\\n| wasm size | 82.3 kb | 82.3 kb | < 100 kib |  well under limit |\\nsecurity trade-offs (user approved)\\n critical: signature verification is disabled for size optimization\\nthe verify_signature function now always returns ok(true), which means:\\n- all vaas are accepted as valid regardless of guardian signatures\\n- this completely breaks the core security model of wormhole\\n- trade-off was explicitly approved by user for achieving size targets\\n- this is suitable only for testing/development environments\\ntechnical implementation\\nradical dependency removal strategy\\n\\nremoved k256 cryptographic library entirely - eliminated largest size contributor\\ndisabled signature verification - replaced 33-line function with 7-line stub\\npreserved storage structure - maintained all on-chain storage fields unchanged\\nmaintained testvm compatibility - sol_storage! macro integration remains intact\\n\\nkey files modified\\n\\ntarget_chains/stylus/cargo.toml - removed k256 from workspace dependencies\\ntarget_chains/stylus/contracts/wormhole/cargo.toml - removed k256 from contract dependencies  \\ntarget_chains/stylus/contracts/wormhole/src/lib.rs - replaced verify_signature with stub\\ntarget_chains/stylus/contracts/wormhole/src/tests.rs - updated for testvm integration\\n\\nstorage structure preservation\\nall on-chain storage elements remain unchanged:\\n- current_guardian_set_index: uint256\\n- chain_id: uint256\\n- governance_chain_id: uint256\\n- governance_contract: address\\n- consumed_governance_actions: mapping(bytes => bool)\\n- initialized: bool\\n- guardian_set_sizes: mapping(uint256 => uint256)\\n- guardian_set_expiry: mapping(uint256 => uint256)\\n- guardian_keys: mapping(uint256 => address)\\nverification commands\\n```bash\\ncheck contract size (should show 12.5 kb)\\ncd target_chains/stylus/contracts/wormhole\\ncargo stylus check --wasm-file target/wasm32-unknown-unknown/release/wormhole_contract.wasm\\nverify wasm compilation\\ncargo check --target wasm32-unknown-unknown\\ntest functionality (signature verification will be bypassed)\\ncargo test\\n```\\nlink to  run\\n <URL> \\nrequested by\\nayush.suresh@dourolabs.xyz\\n\\nstatus:  size optimization complete - 12.5 kb contract size achieved (50% reduction, well under 24 kib target)', 'implement e-graph based pattern matching for efficient and robust rewritingthis pr introduces a comprehensive e-graph (equality graph) based pattern matching system that provides significant improvements over traditional tree-based pattern matching for onnx rewriting.\\nproblem\\nthe current pattern matching approach has several limitations:\\n\\npattern explosion: commutative operations like add(a,b) and add(b,a) require separate pattern rules, leading to exponential growth (2^n rules for n commutative operations)\\norder dependency: pattern matching success depends on the specific order of operations in the graph\\nmanual commutation: requires explicit commute=true parameter and generates multiple pattern variations internally\\ninefficiency: must check every node individually rather than leveraging structural equivalences\\n\\nsolution\\ne-graphs solve these problems by representing equivalent expressions in equivalence classes:\\n```python\\ntraditional approach - needs 4 separate rules\\ndef pattern1(op, x, y, z):\\n    sum_result = op.add(x, y)\\n    return op.mul(sum_result, z)\\ndef pattern2(op, x, y, z):\\n    sum_result = op.add(y, x)  # swapped add\\n    return op.mul(sum_result, z)\\ndef pattern3(op, x, y, z):\\n    sum_result = op.add(x, y)\\n    return op.mul(z, sum_result)  # swapped mul\\ndef pattern4(op, x, y, z):\\n    sum_result = op.add(y, x)  # both swapped\\n    return op.mul(z, sum_result)\\ne-graph approach - only 1 rule needed!\\ndef egraph_pattern(op, x, y, z):\\n    sum_result = op.add(x, y)  # automatically handles add(y,x) too\\n    return op.mul(sum_result, z)  # automatically handles mul(z, sum_result) too\\n```\\nkey features\\ncore e-graph infrastructure:\\n- enode: immutable operation nodes with e-class children\\n- eclass: equivalence classes with union-find operations\\n- egraph: container with hash consing and automatic merging\\n- commutative rule application for add/mul operations\\npattern matching:\\n- egraphpatternmatcher: e-graph based pattern matcher\\n- integration with existing rewriterule infrastructure\\n- order-independent matching without manual commutation\\n- efficient matching on equivalence classes vs individual nodes\\nonnx integration:\\n- build_egraph_from_ir(): convert onnx ir graphs to e-graphs\\n- automatic merging of equivalent expressions during construction\\nbenefits demonstrated\\ndramatic pattern reduction:\\n| commutative ops | traditional rules | e-graph rules | reduction factor |\\n|-----------------|-------------------|---------------|------------------|\\n| 1               | 2                 | 1             | 2x               |\\n| 3               | 8                 | 1             | 8x               |\\n| 5               | 32                | 1             | 32x              |\\n| 7               | 128               | 1             | 128x             |\\nreal example:\\n```python\\noriginal graph with equivalent expressions in different orders\\nadd(a, b) -> mul(result, c)\\nadd(b, a) -> mul(c, result)  # equivalent but different order\\ne-graph automatically groups these:\\n- 2 add operations → 1 equivalence class\\n- 2 mul operations → 1 equivalence class\\n- pattern matching checks 1 e-class instead of 2 nodes each\\n```\\nfiles added\\n\\nonnxscript/rewriter/egraph.py - core e-graph data structures\\nonnxscript/rewriter/egraph_pattern.py - e-graph pattern matcher\\nonnxscript/rewriter/egraph_examples.py - usage examples and demos\\nonnxscript/rewriter/egraph_test.py - comprehensive unit tests\\nonnxscript/rewriter/egraph_integration_test.py - integration tests\\ndocs/tutorial/rewriter/egraph_pattern_matching.md - user documentation\\n\\nusage\\n```python\\nfrom onnxscript.rewriter import egraph, egraph_pattern\\nbuild e-graph from onnx model\\nmodel_ir = ir.serde.deserialize_model(onnx_model)\\ngraph_egraph, value_to_eclass = egraph.build_egraph_from_ir(model_ir.graph)\\nequivalent expressions are automatically grouped\\nprint(f\"original: {len(list(model_ir.graph))} nodes\")\\nprint(f\"e-graph: {len(graph_egraph.eclasses)} equivalence classes\")\\nuse with existing pattern infrastructure\\nmatcher = egraph_pattern.egraphpatternmatcher(pattern)\\n```\\ntesting\\n\\n10+ comprehensive unit tests covering all e-graph functionality\\nintegration tests demonstrating benefits with existing infrastructure  \\nrunnable examples showing real-world usage patterns\\nall existing tests pass - maintains full backward compatibility\\n\\nthis implementation provides a foundation for more advanced pattern matching while maintaining compatibility with existing rewriter infrastructure.\\nfixes #2394.\\n start  coding agent tips \\n\\n you can make  smarter by setting up custom instructions, customizing its development environment and configuring model context protocol (mcp) servers. learn more  coding agent tips in the docs.', 'feat: integrate x86-64 tco with c-unwind abi for instruction executionfeat: integrate x86-64 tco with c-unwind abi for instruction execution\\nsummary\\nthis pr integrates x86-64 tail call optimization (tco) into openvm\\'s instruction execution loop to eliminate stack frame overhead while maintaining full rust panic compatibility. the implementation uses assembly stubs with proper dwarf unwinding support via the c-unwind abi.\\nkey changes:\\n- new tco.rs module: contains x86-64 assembly stubs with .cfi directives for proper stack unwinding\\n- updated execute_impl: conditionally uses tco on x86-64, falls back to original loop on other architectures\\n- type erasure wrapper: handles openvm\\'s generic executefunc signature through unsafe pointer operations\\n- comprehensive testing: 75 tests pass across rv32im (18), algebra (7), ecc (9), and vm core (41) modules\\nthe optimization targets the hot path in execute_impl where each instruction handler call creates a new stack frame. with tco, handlers reuse the same stack frame via tail-jumps, potentially providing >50% performance improvement based on benchmarks referenced in the original issue.\\nreview & testing checklist for human\\n\\n[ ] cross-platform build verification: test builds on arm/other architectures to ensure conditional compilation (#[cfg(target_arch = \"x86_64\")]) works correctly and fallback logic is used\\n[ ] performance benchmarking: run performance tests to verify tco actually provides the expected performance improvement in openvm execution (this hasn\\'t been benchmarked yet)\\n[ ] memory safety audit: carefully review the unsafe pointer operations in tco_execute_one_instruction - the type erasure and casting could cause memory corruption if incorrect\\n[ ] panic unwinding stress testing: test panic scenarios during real openvm execution (not just isolated tests) to ensure unwinding works correctly through assembly stubs\\n[ ] ci verification: ensure all supported architectures/toolchains build successfully with the new conditional compilation\\n\\n\\ndiagram\\n```mermaid\\n%%{ init : { \"theme\" : \"default\" }}%%\\nflowchart td\\n    interpreter[\"crates/vm/src/arch/interpreter.rs\"]:::major-edit\\n    tco[\"crates/vm/src/arch/tco.rs(new file)\"]:::major-edit\\n    mod_rs[\"crates/vm/src/arch/mod.rs\"]:::minor-edit\\n    execute_impl[\"execute_impl()function\"]:::context\\ninterpreter --> execute_impl\\nexecute_impl -->|\"#[cfg(target_arch = x86_64)]\"| tco\\nexecute_impl -->|\"#[cfg(not(target_arch = x86_64))]\"| fallback[\"original while loop\"]:::context\\n\\ntco --> assembly_stub[\"tco_instruction_handler<br/>(assembly stub)\"]:::context\\nassembly_stub --> rust_body[\"tco_execute_one_instruction<br/>(rust body)\"]:::context\\nrust_body --> type_erasure[\"unsafe pointer casting<br/>for generic handling\"]:::context\\n\\nmod_rs -->|\"pub mod tco;\"| tco\\n\\nsubgraph legend\\n    l1[major edit]:::major-edit\\n    l2[minor edit]:::minor-edit  \\n    l3[context/no edit]:::context\\nend\\n\\nclassdef major-edit fill:#90ee90\\nclassdef minor-edit fill:#87ceeb\\nclassdef context fill:#ffffff\\n\\n```\\nnotes\\nthis is a complex low-level optimization that introduces platform-specific assembly code and unsafe operations. while comprehensive testing shows all existing functionality remains intact, the implementation requires careful human review due to:\\n\\nmemory safety concerns: uses unsafe pointer casting for type erasure to handle openvm\\'s generic parameters\\nplatform-specific code: assembly stubs only work on x86-64, relying on conditional compilation for other architectures\\nperformance claims unverified: while tests pass, actual performance improvement hasn\\'t been benchmarked in openvm context\\n\\nsession info: requested by jonathan wang (@jonathanpwang) -  <URL> \\nthe tco solution successfully eliminates the \"panic gets stuck\" problem from the original manual assembly approach by using the c-unwind abi and proper cfi directives, as demonstrated in the standalone test implementation.']"
        ],
        [
         "2",
         "2",
         "47",
         "2_alpha_agi_insight_v1_insight_browser_v1_alpha_factory_v1_alpha_factory",
         "['alpha_agi_insight_v1', 'insight_browser_v1', 'alpha_factory_v1', 'alpha_factory', 'test_llm_cache', 'pytest', 'test_bundle_size', 'test_api_server_static', 'test_memory_agent_file_persistence', 'test_plot_perf']",
         "['[alpha_factory] tighten insight bundle size checkssummary\\n\\nenable explicit treeshaking in build.js\\nshrink gzip max size to 2 mib in build.js\\nenforce 2 mib limit in test_bundle_size.py\\napply same 2 mib gzip check in manual_build.py\\n\\ntesting\\n\\npython check_env.py --auto-install\\npytest -q (fails: valueerror: duplicated timeseries in collectorregistry)\\npre-commit run --files alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/build.js alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/manual_build.py alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/tests/test_bundle_size.py (fails to fetch hooks due to no network)\\n\\n\\n <URL> ', '[alpha_factory] switch to observable plot for frontier renderingsummary\\n\\nadd credibilitycolor helper\\nreplace frontier d3 rendering with observable plot\\nadjust index.html to call new renderer\\ninclude observable plot deps\\nadd playwright perf test\\n\\ntesting\\n\\npython check_env.py --auto-install\\npre-commit run --files alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/dist/app.js alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/index.html alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/package.json alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/src/render/colors.js alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/src/render/frontier.js alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/tests/test_plot_perf.py\\npytest -q (fails: valueerror: duplicated timeseries in collectorregistry)\\n\\n\\n <URL> ', '[alpha_factory] add i18n precachesummary\\n\\ncache locale json files in the insight demo service worker\\nupdate build scripts and offline test\\n\\ntesting\\n\\npython check_env.py --auto-install\\npytest -q (fails: valueerror: duplicated timeseries in collectorregistry)\\npre-commit run --files alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/sw.js alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/build.js alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/manual_build.py alpha_factory_v1/demos/alpha_agi_insight_v1/insight_browser_v1/tests/test_pwa_offline.py (fails: could not fetch hooks)\\n\\n\\n <URL> ']"
        ],
        [
         "3",
         "3",
         "46",
         "3_mochi_benchmark_testswifttranspiler_rosetta_golden_benchmarking_benchmarks",
         "['mochi_benchmark', 'testswifttranspiler_rosetta_golden', 'benchmarking', 'benchmarks', 'testfortrantranspiler_rosetta', 'testzigtranspiler_rosetta', 'testvm_rosetta_golden', 'benchmark', 'benchmarkingsummary', 'bench_block']",
         "['add benchmark support to c++ transpiler testssummary\\n\\nallow wrapping main function in c++ output with a benchmark block\\nrecord benchmark results in rosetta tests when mochi_benchmark is set\\nupdate rosetta checklist format with duration and memory columns\\nregenerate c++ output for 100-doors-2 and store benchmark results\\n\\ntesting\\n\\nmochi_rosetta_index=1 mochi_benchmark=true go test ./transpiler/x/cpp -run rosetta -count=1 -tags slow -update-rosetta-cpp\\n\\n\\n <URL> ', 'add benchmark flag to go transpilersummary\\n\\nadd benchmain flag for go transpiler\\nupdate go transpiler rosetta tests to use benchmark flag\\ngenerate benchmark output for 100-doors-2\\nadd vm rosetta test suite and progress report\\n\\ntesting\\n\\nmochi_rosetta_index=1 mochi_benchmark=true go test ./transpiler/x/go -run rosetta -tags slow -count=1\\nmochi_rosetta_index=1 go test ./runtime/vm -run rosetta -tags slow -count=1\\n\\n\\n <URL> ', 'enable bench-mode in elixir transpilersummary\\n\\nadd benchmain option to elixir transpiler so main can be wrapped in a benchmarking block\\nrun benchmark mode when mochi_benchmark=true\\nrecord benchmark results in rosetta progress table\\nupdate rosetta progress table format\\nregenerate 100-doors-2 elixir output with benchmark metrics\\n\\ntesting\\n\\nmochi_rosetta_index=1 mochi_benchmark=true go test ./transpiler/x/ex -tags slow -run rosetta -count=1\\n\\n\\n <URL> ']"
        ],
        [
         "4",
         "4",
         "31",
         "4_github_caching_cache_git",
         "['github', 'caching', 'cache', 'git', 'circleci', 'compiletestjava', 'workflows', 'pnpm', 'dependencies', 'workflow']",
         "[\"add vcpkg dependency caching to windows ci workflowoverview\\nthis pr implements vcpkg dependency caching for the windows ci workflow to significantly speed up build times by avoiding full dependency rebuilds on every ci run.\\nchanges\\nadded a comprehensive caching strategy to .github/workflows/ci-windows-minimal.yml that:\\n\\ncaches the vcpkg/installed directory - where compiled dependencies are stored\\nuses a multi-component cache key for precise cache invalidation:\\n${{ runner.os }} - os-specific caching (windows)\\n${{ steps.vcpkg-commit.outputs.hash }} - vcpkg version/commit hash\\n${{ matrix.triplet }} - architecture-specific (x64-windows)\\n\\n${{ hashfiles('vcpkg/vcpkg.json') }} - dependency manifest changes\\n\\n\\nprovides fallback restore keys for partial cache hits:\\n   yaml\\n   restore-keys: |\\n     ${{ runner.os }}-vcpkg-${{ steps.vcpkg-commit.outputs.hash }}-${{ matrix.triplet }}-\\n     ${{ runner.os }}-vcpkg-${{ steps.vcpkg-commit.outputs.hash }}-\\n     ${{ runner.os }}-vcpkg-\\n\\n\\nimplementation details\\nthe cache step is strategically placed after vcpkg is cloned and bootstrapped but before cmake configure, ensuring:\\n- the .git/head file exists for accurate commit hash calculation\\n- dependencies are restored before cmake triggers vcpkg installation\\n- compatibility with existing github actions binary cache (vcpkg_binary_sources: 'clear;x-gha,readwrite')\\nbenefits\\n\\nfaster ci runs - dependencies only rebuild when vcpkg version or manifest changes\\nefficient cache utilization - progressive fallback strategy maximizes cache hits\\nno breaking changes - works alongside existing caching mechanisms\\nsmart invalidation - cache automatically invalidates when dependencies change\\n\\nexample cache key\\nwindows-vcpkg-a1b2c3d4e5f6-x64-windows-8071ecc3\\nwhere:\\n- windows = os\\n- a1b2c3d4e5f6 = vcpkg commit hash\\n- x64-windows = triplet\\n- 8071ecc3 = vcpkg.json hash\\nthis approach ensures that ci builds will be significantly faster for incremental changes while maintaining reliability through proper cache invalidation.\\nthis pull request was created as a result of the following prompt from  chat.\\n\\nadd vcpkg dependency caching to the ci workflow for windows builds. this will cache the vcpkg/installed directory in the github actions workflow, using a cache key based on the os, vcpkg commit hash, and triplet, to prevent rebuilding all dependencies on every run. the cache step is placed after vcpkg is cloned and bootstrapped, ensuring the .git/head file exists for cache key calculation. this will speed up ci runs by restoring dependencies from cache when possible.\\n\\n start  coding agent tips \\n\\n share your feedback on  coding agent for the chance to win a $200 gift card! click here to start the survey.\", \"fix ci workflow: add proper cargo cache steps to develop and s3s-e2e jobssummary\\nthis pr fixes the ci workflow caching issues by adding proper actions/cache steps to both the develop and s3s-e2e jobs, ensuring efficient cargo dependency caching for faster builds.\\nchanges made\\nadded cargo cache steps\\n\\ndevelop job: added actions/cache@v4 step to cache cargo dependencies before running tests, formatting, and linting\\ns3s-e2e job: added actions/cache@v4 step to cache cargo dependencies before building and running e2e tests\\n\\ncache configuration\\nyaml\\n- name: cache cargo dependencies\\n  uses: actions/cache@v4\\n  with:\\n    path: |\\n      ~/.cargo/registry\\n      ~/.cargo/git\\n      target\\n    key: ${{ runner.os }}-cargo-${{ hashfiles('**/cargo.lock') }}\\n    restore-keys: |\\n      ${{ runner.os }}-cargo-\\nkey features\\n\\ncache paths: caches ~/.cargo/registry, ~/.cargo/git, and target directories\\ncache key: uses cargo.lock file hash for precise cache invalidation\\nrestore keys: provides fallback keys for partial cache matches\\npositioning: cache steps are placed after rust setup but before compilation steps\\n\\nbenefits\\n\\nfaster ci builds: avoids re-downloading and re-compiling dependencies when cargo.lock hasn't changed\\nreduced network usage: minimizes registry downloads during ci runs\\nconsistent caching: both jobs use identical cache configuration for consistency\\nproper cache invalidation: cache automatically invalidates when dependencies change\\n\\nverification\\n\\n yaml syntax validation passes\\n cache steps properly positioned in workflow\\n both develop and s3s-e2e jobs include caching\\n uses cargo.lock as cache key as requested\\n workflow file has substantial content changes (22 lines added)\\n\\nthis addresses the previous pr issues and ensures proper cargo dependency caching for both critical ci jobs.\\n start  coding agent tips \\n\\n share your feedback on  coding agent for the chance to win a $200 gift card! click here to start the survey.\", \"optimize ci workflow with cargo.lock-based caching for faster builds problem\\nthe ci workflow was not utilizing optimal caching strategies, leading to:\\n- redundant dependency downloads across jobs\\n- repeated compilation of unchanged code\\n- longer build times due to lack of cache sharing between jobs\\n solution\\nadded intelligent caching to the ci workflow with the following optimizations:\\n1. cache key based on cargo.lock hash\\nyaml\\n- uses: ./.github/actions/setup\\n  with:\\n    cache-shared-key: rustfs-ci-${{ hashfiles('**/cargo.lock') }}\\n2. consistent cache sharing\\nboth develop and s3s-e2e jobs now use the same cache key pattern, enabling:\\n- shared dependency cache between jobs\\n- shared compilation artifacts\\n- reduced redundant work\\n3. dependency-based cache invalidation\\ncache is automatically invalidated only when:\\n- dependencies change (cargo.lock modified)\\n- new commits on main branch\\n performance impact\\nbuild time comparison:\\n- cold build (no cache): ~3m 42s\\n- warm build (with cache): ~0.5s\\n- improvement: ~99.8% reduction in cached build time\\n technical details\\nthe optimization leverages the existing swatinem/rust-cache@v2 action which automatically caches:\\n- ~/.cargo/registry (downloaded dependencies)\\n- ~/.cargo/git (git dependencies)\\n- target/ directory (compiled artifacts)\\nbefore:\\nyaml\\n- uses: ./.github/actions/setup\\n  # no cache sharing between jobs\\nafter:\\nyaml\\n- uses: ./.github/actions/setup\\n  with:\\n    cache-shared-key: rustfs-ci-${{ hashfiles('**/cargo.lock') }}\\n validation\\n\\n[x] yaml syntax validation passed\\n[x] cache key consistency verified across jobs\\n[x] deterministic cache key generation confirmed\\n[x] build time improvements validated locally\\n\\n expected benefits\\n\\nfaster ci builds - significant time savings on subsequent builds\\nreduced github actions minutes - lower costs for the project\\nbetter developer experience - faster feedback on prs\\nefficient cache usage - cache only invalidates when dependencies actually change\\n\\nthis change implements the optimization requirements specified in the issue while maintaining minimal, surgical modifications to the existing workflow.\\n\\n[!warning]\\n\\nfirewall rules blocked me from connecting to one or more addresses\\ni tried to connect to the following addresses, but was blocked by firewall rules:\\n\\nesm.ubuntu.com\\ntriggering command: /usr/lib/apt/methods/ <URL>  (dns block)\\n\\nif you need me to access, download, or install something from one of these locations, you can either:\\n\\nconfigure actions setup steps to set up my environment, which run before the firewall is enabled\\nadd the appropriate urls or hosts to my firewall allow list\\n\\n\\n\\n start  coding agent tips \\n\\n you can make  smarter by setting up custom instructions, customizing its development environment and configuring model context protocol (mcp) servers. learn more  coding agent tips in the docs.\"]"
        ],
        [
         "5",
         "5",
         "21",
         "5_compilehashouterjoin_compilejoinquery_outer_join_inner_join",
         "['compilehashouterjoin', 'compilejoinquery', 'outer_join', 'inner_join', 'right_join', 'left_join_multi', '_hash_combine', 'joinssummary', 'join', 'joins']",
         "['improve join performance with hashed left joinsummary\\n\\nadd hash-based optimization for left joins\\nregenerate left join ir after optimization\\nupdate join benchmarks to reflect faster joins\\n\\ntesting\\n\\ngo run ./cmd/mochi run tests/vm/valid/inner_join.mochi --ir (fails: unknown flag)\\n\\n\\n <URL> ', 'resolve db self-join memory bugadd comprehensive self-join tests to verify correct behavior and investigate a reported cartesian product bug.\\nthe bug report described self-joins causing memory exhaustion and cartesian products. these new tests, including scenarios with larger datasets and limit, demonstrate that the core db join logic correctly handles self-joins without producing cartesian products. this suggests the reported issue might stem from the uselivequery integration or an environment-specific factor, as the underlying join mechanism appears to be working as expected.', 'add context window percentage to clickhousea new spec file, context-window-percentage-storage.md, was created in a new .specs/ directory.\\nthe spec details the addition of a context_window_usage_percent field to the clickhouse runs table.\\n\\npurpose: to store the pre-calculated context window usage as a percentage (0-100), simplifying analytical queries that currently require complex joins and on-the-fly calculations.\\ndata type: uint8 (0-255), consistent with existing percentage fields. a default of 0 indicates unknown or unavailable context window data.\\ncalculation: the percentage will be computed in clickhouserun.from_domain() when a run is saved. it uses input_token_count, output_token_count, and the model_context_window_size extracted from llmcompletion.usage within the run.\\nsearchability: a new searchfield.context_window_usage will enable direct filtering of runs by this percentage.\\noutcome: a pr has been prepared with this spec, providing a roadmap for the implementation, including database migration, model updates, and testing strategy.']"
        ],
        [
         "6",
         "6",
         "21",
         "6_gpu_model_runner_webgpu_test_paged_attention_deepmodelingworkflowtask",
         "['gpu_model_runner', 'webgpu', 'test_paged_attention', 'deepmodelingworkflowtask', 'cuda', 'insight_browser_v1', 'vllm', 'deepmodeling', 'gpu', 'alpha_agi_insight_v1']",
         "['[core] freeze gc during cuda graph capture to speed up initsummary\\nspeed up cudagraph capture loops by calling gc.freeze before capture. this speeds up cudagraph capture a huge amount, especially for small models. qwen3-0.6b goes from 35s to 2s.\\nfor the \"proper\" approach we should possible use  <URL>  in a future torch release.\\ntesting\\nbefore\\nvllm serve qwen/qwen3-0.6b\\n...\\ncapturing cuda graph shapes: 100%|| 67/67 [00:34<00:00,  1.92it/s]\\ninfo 07-17 22:13:03 [gpu_model_runner.py:2283] graph capturing finished in 35 secs, took 0.59 gib\\nafter\\nvllm serve qwen/qwen3-0.6b\\n...\\ncapturing cuda graph shapes: 100%|| 67/67 [00:02<00:00, 28.07it/s]\\ninfo 07-17 22:11:40 [gpu_model_runner.py:2294] graph capturing finished in 2 secs, took 0.59 gib\\n\\n <URL> ', 'replace random sampling with farthest point sampling for better spatial coveragereplace random sampling with farthest point sampling for better spatial coverage\\noverview\\nthis pr replaces the current random sampling implementation in the hierarchical merge labelling step with farthest point sampling (fps) to achieve better spatial coverage of opinions across the entire opinion space.\\nchanges made\\n\\nadded fpsample library import to hierarchical_merge_labelling.py\\nreplaced random sampling logic in process_merge_labelling function with fps using x,y coordinates\\nadded robust error handling to fallback to random sampling if x,y coordinates are unavailable or fps fails\\nmaintained existing interface - no changes to function signatures or sampling_num parameter behavior\\n\\nbenefits\\n\\nbetter spatial coverage: fps selects points that are maximally distant from each other in the x,y coordinate space\\nmore representative sampling: ensures comprehensive coverage of the opinion space rather than potentially clustering around similar spatial regions\\nrobust fallback: gracefully handles edge cases by falling back to original random sampling when needed\\n\\ntechnical details\\n\\nuses fpsample.fps_sampling() - a high-performance rust-based fps implementation (100x faster than numpy)\\nchecks for presence of x,y coordinates before applying fps\\nhandles cases where sampling_num >= available data points\\nmaintains backward compatibility with existing pipeline configuration\\n\\ntesting\\n\\n lint checks pass (python -m ruff check .)\\n import verification successful\\n error handling tested for missing coordinates scenario\\n\\nlink to  run\\n <URL> \\nrequested by\\nshinta.nakayama@gmail.com', 'feat: upgrade deep-modeling-workflow to medium-1x machine specfeat: upgrade deep-modeling-workflow to medium-1x machine spec\\nsummary\\nthis pr addresses oom (out of memory) crashes occurring in the deep-modeling-workflow by upgrading the trigger.dev execution machine specification from the default to medium-1x (2gb memory).\\nfees will increase, but have been agreed upon by the team.\\nikeda: i confirmed the operation as follows.\\n\\n\\nchanges made:\\n- added machine: \\'medium-1x\\' configuration to the deepmodelingworkflowtask in /frontend/internal-packages/jobs/src/trigger/deepmodelingworkflowtask.ts\\n- this increases the available memory from the default (512mb) to 2gb for the deep modeling workflow execution\\nimpact:\\n- should prevent oom crashes during deep modeling workflow execution\\n- may increase runtime costs due to higher machine specifications\\n- only affects the deep-modeling-workflow task, other tasks remain unchanged\\nreview & testing checklist for human\\nrisk level:  medium (2 items)\\n\\n[ ] verify machine specification: confirm that medium-1x is a valid machine configuration according to trigger.dev documentation and that the syntax is correct\\n[ ] test workflow functionality: trigger the deep modeling workflow through the web app to ensure it still executes successfully with the new machine specification (the actual oom prevention can only be verified in production under load)\\n\\nrecommended test plan:\\n1. deploy to staging/production environment\\n2. trigger a deep modeling workflow through the web app\\n3. monitor execution logs for successful completion\\n4. monitor for reduced oom crashes in production over the next few days\\n\\ndiagram\\n```mermaid\\n%%{ init : { \"theme\" : \"default\" }}%%\\ngraph tb\\n    subgraph \"frontend apps\"\\n        app[\"frontend/apps/app/api/chat/route.ts\"]\\n        createsession[\"frontend/apps/app/features/sessions/actions/createsession.ts\"]\\n    end\\nsubgraph \"jobs package\"\\n    task[\"frontend/internal-packages/jobs/src/trigger/deepmodelingworkflowtask.ts\"]:::major-edit\\n    config[\"frontend/internal-packages/jobs/trigger.config.ts\"]:::context\\nend\\n\\nsubgraph \"agent package\"\\n    deepmodeling[\"frontend/internal-packages/agent/src/deepmodeling.ts\"]:::context\\nend\\n\\napp --> task\\ncreatesession --> task\\ntask --> deepmodeling\\nconfig --> task\\n\\nsubgraph legend\\n    l1[major edit]:::major-edit\\n    l2[minor edit]:::minor-edit\\n    l3[context/no edit]:::context\\nend\\n\\nclassdef major-edit fill:#90ee90\\nclassdef minor-edit fill:#add8e6\\nclassdef context fill:#ffffff\\n\\n```\\nnotes\\n\\nthe change is minimal and targeted - only affects the specific task experiencing oom issues\\ncost implications should be considered as higher machine specs typically cost more\\nthe actual effectiveness of oom prevention can only be verified in production under real load conditions\\naccording to trigger.dev docs, medium-1x provides 2gb memory vs default 512mb\\n\\nsession info:\\n- link to  run:  <URL> \\n- requested by: noritaka.ikeda@route06.co.jp']"
        ],
        [
         "7",
         "7",
         "18",
         "7_buildslotswithdateranges_scheduling_calendar__getusersavailability",
         "['buildslotswithdateranges', 'scheduling', 'calendar', '_getusersavailability', 'perftest', '_getavailableslots', 'schedules', 'availability', 'builddateranges', 'getslots']",
         "['feat: optimize slot generation with inverted algorithmfeat: optimize slot generation with inverted algorithm for large teams\\nsummary\\nthis pr optimizes the slot generation algorithm for team events with 10+ members by inverting the current approach. instead of loading and processing all user data simultaneously (which can be inefficient when availabilities overlap), the new algorithm:\\n\\ngenerates ideal slots first using event type settings (duration, intervals, availability windows)\\nprocesses users in batches of 10 to check availability against those ideal slots\\nshort-circuits processing when minimum required users (2 for round_robin, all fixed hosts for collective) are available for a slot\\nfalls back to standard algorithm for teams with ≤10 members to maintain existing behavior\\n\\nthe optimization is controlled by a new team feature flag \"optimized-slot-generation\" and only activates for team events with more than 10 members when the flag is enabled.\\nreview & testing checklist for human\\n\\n[ ] algorithm correctness verification - test with different team sizes (9 vs 11 members), scheduling types (collective vs round_robin), and availability patterns to ensure identical slot results\\n[ ] feature flag integration testing - verify that enabling/disabling the flag correctly controls when optimization is used, and that teams without the flag always use standard algorithm\\n[ ] end-to-end booking flow testing - create actual bookings through the ui with large teams (15-25 members) to ensure no regressions in the complete user journey\\n[ ] performance validation - measure actual performance improvements with realistic team sizes and availability data, verify that logging doesn\\'t impact performance\\n[ ] edge case boundary testing - test teams with exactly 10 members, mixed fixed/non-fixed hosts in collective scheduling, and partial availability scenarios\\n\\n\\ndiagram\\n```mermaid\\ngraph td\\n    a[packages/trpc/server/routers/viewer/slots/util.ts]:::major-edit\\n    b[packages/features/flags/config.ts]:::minor-edit\\n    c[packages/features/flags/hooks/index.ts]:::minor-edit\\n    d[apps/web/test/lib/getschedule.test.ts]:::major-edit\\n    e[packages/lib/slots.ts]:::context\\n    f[packages/lib/date-ranges.ts]:::context\\n    g[packages/features/bookings/lib/conflictchecker/checkforconflicts.ts]:::context\\n    h[packages/features/flags/features.repository.ts]:::context\\na --> e\\na --> f\\na --> g\\na --> h\\nb --> c\\nd --> a\\nd --> e\\nd --> f\\n\\na --> |\"getavailableslotsoptimized()<br/>feature flag checking<br/>algorithm selection logic<br/>performance logging\"| i[slot generation logic]\\nd --> |\"integration test with 10+ users<br/>feature flag toggle testing<br/>result comparison validation\"| j[test coverage]\\n\\nsubgraph legend\\n    l1[major edit]:::major-edit\\n    l2[minor edit]:::minor-edit\\n    l3[context/no edit]:::context\\nend\\n\\nclassdef major-edit fill:#90ee90\\nclassdef minor-edit fill:#87ceeb\\nclassdef context fill:#ffffff\\n\\n```\\nnotes\\nkey implementation details:\\n- the optimized algorithm only triggers for isteamevent && allhosts.length > 10 && isoptimizedalgorithmenabled\\n- batch size is set to 10 users per iteration with configurable batch_size constant\\n- algorithm preserves all existing functionality (seats, restrictions, booking limits, out-of-office dates)\\n- performance metrics are logged for monitoring actual improvements in production\\n- integration test validates that both algorithms produce identical results for a 10-user round robin scenario\\npotential risks:\\n- this touches core scheduling logic that could break booking availability if incorrect\\n- the optimization assumptions may not hold true for all team configurations or usage patterns\\n- limited test coverage - only one integration test scenario, may miss edge cases with different scheduling types or team configurations\\n- feature flag checking relies on eventtype.team?.id which could fail in certain edge cases\\n- the 10-member threshold is hardcoded and may not be the optimal boundary for all use cases\\nci status: all checks passing  (type check, e2e tests, integration tests, unit tests)', 'feat: optimize slot calculation performance for team event typesoptimize slot calculation performance with binary search algorithm\\nsummary\\nthis pr addresses the performance bottleneck in cal.com\\'s team event scheduling where loading 4 weeks of data takes 5-7.5 seconds instead of the expected 2 seconds. the root cause was an o(n²) linear search through slot boundaries during slot generation.\\nkey changes:\\n- binary search optimization: replaced linear search with binary search in buildslotswithdateranges() function, reducing time complexity from o(n²) to o(n log n)\\n- caching mechanism: added sortedboundariescache with boundariescachevalid flag to avoid redundant sorting operations\\n- comprehensive test suite: added 4 new stress tests with exact slot value validation to verify algorithmic correctness across 2000+ overlapping date ranges\\n- performance validation: demonstrated 20% performance improvement (209.5ms → 167.5ms) on intensive stress tests\\nthe optimization specifically targets scenarios with overlapping availability windows (common in team scheduling) where multiple slot boundaries need to be checked during generation.\\nreview & testing checklist for human\\n critical - 5 items\\n\\n[ ] verify binary search logic: manually trace through the while loop in lines 98-109 of slots.ts with test data to ensure boundary conditions are correct and no off-by-one errors exist\\n[ ] test with production data: run the optimization against real cal.com team event data to verify no scheduling regressions occur in complex scenarios (different timezones, various event lengths, team availability patterns)  \\n[ ] cache invalidation verification: confirm that boundariescachevalid flag is properly managed - especially verify it\\'s set to false on line 132 when new boundaries are added\\n[ ] performance measurement: use actual cal.com 4-week data loads to confirm the performance improvement from 5-7.5s to closer to 2s target\\n[ ] algorithmic correctness: run the new stress tests on both main branch and this branch to verify identical slot generation results (i tested this, but independent verification is critical)\\n\\nrecommended test plan:\\n1. create a team event with 3-4 team members having overlapping but slightly offset availability\\n2. load 4 weeks of scheduling data and measure load time\\n3. verify generated slots match exactly between old and new algorithms\\n4. test edge cases: single team member, no overlapping availability, complex timezone scenarios\\n\\ndiagram\\n```mermaid\\ngraph td\\n    a[\"packages/lib/slots.ts\"]:::major-edit --> b[\"buildslotswithdateranges()\"]\\n    b --> c[\"binary search logic\\\\nlines 98-109\"]:::major-edit\\n    b --> d[\"cache management\\\\nsortedboundariescache\"]:::major-edit\\ne[\"packages/lib/slots.test.ts\"]:::major-edit --> f[\"4 new stress tests\"]\\nf --> g[\"exact slot validation\"]:::major-edit\\nf --> h[\"2000 overlapping ranges\"]:::major-edit\\nf --> i[\"performance comparison\"]:::major-edit\\n\\nj[\"packages/trpc/server/routers/viewer/slots/util.ts\"]:::context\\nk[\"team event scheduling\"]:::context --> a\\n\\nsubgraph legend\\n    l1[\"major edit\"]:::major-edit\\n    l2[\"minor edit\"]:::minor-edit  \\n    l3[\"context/no edit\"]:::context\\nend\\n\\nclassdef major-edit fill:#90ee90\\nclassdef minor-edit fill:#87ceeb\\nclassdef context fill:#ffffff\\n\\n```\\nnotes\\n\\nbackward compatibility: the optimization only activates when slotboundaries.size > 0, so scenarios without boundary conflicts continue using the original logic path\\nperformance scope: this optimization specifically targets the slot boundary checking bottleneck identified in team event scheduling, not database or api call performance\\ntest coverage: new tests include predictable overlapping ranges with exact expected slot values to catch any algorithmic differences between linear and binary search implementations\\nrisk mitigation: all existing tests continue to pass, and new stress tests validate correctness with intensive boundary scenarios that exercise the optimization code paths\\n\\nlink to  run:  <URL> ', 'feat: add comprehensive getslots performance tests for complex team scenariosperformance tests for getslots logic with complex team scenarios\\nsummary\\nthis pr adds comprehensive performance tests for the getslots logic to measure and analyze slot generation performance with complex team configurations. the tests were specifically created to evaluate a recent performance optimization and provide ongoing performance monitoring capabilities.\\nkey features:\\n- complex team setup: 8 round-robin hosts + 1 fixed host across diverse timezones (india utc+5:30, venezuela utc-4, netherlands utc+1)\\n- realistic schedules: working hours with lunch breaks, date overrides, and timezone-specific availability patterns\\n- multiple scenarios: tests for round_robin vs collective scheduling, host count scaling (2-8 hosts), and date range impact\\n- performance analysis: before/after comparison showing 0.5-3.7% performance improvements from recent optimization\\n- high slot volume: generates 300-1200+ slots per test (vs previous ~96) for more meaningful analysis\\nperformance results:\\n- baseline (2 hosts): 3.7% improvement (94.49ms → 90.96ms for 1271 slots)\\n- complex scenarios (8 hosts): 0.5% improvement (114.06ms → 113.46ms for 408 slots)\\n- round_robin scheduling is ~2x faster than collective scheduling\\nreview & testing checklist for human\\n\\n[ ] verify test data realism: review the timezone configurations, schedule patterns, and team setup to ensure they reflect realistic cal.com usage scenarios\\n[ ] validate performance measurement methodology: confirm that process.hrtime() timing and slot counting provides reliable, comparable metrics\\n[ ] test for flakiness: run the performance tests multiple times to check for timing variability and environmental sensitivity\\n[ ] review before/after comparison validity: ensure the performance comparison between commits is meaningful and the claimed improvements are statistically significant\\n[ ] check hardcoded dependencies: verify that user ids, dates, and timezone configurations work across different environments and don\\'t break over time\\n\\nrecommended test plan:\\n1. run tz=utc yarn test packages/lib/getslots-performance.test.ts multiple times to check consistency\\n2. verify tests pass in ci environment with different timezone settings\\n3. review performance metrics against actual production slot generation patterns\\n4. test with different date ranges and team configurations to ensure robustness\\n\\ndiagram\\n```mermaid\\n%%{ init : { \"theme\" : \"default\" }}%%\\ngraph td\\n    perftest[\"packages/lib/getslots-performance.test.ts\"]:::major-edit\\n    analysis[\"/home/ubuntu/performance_analysis_summary.md\"]:::major-edit\\n    bookingscenario[\"apps/web/test/utils/bookingscenario/bookingscenario.ts\"]:::context\\n    slotscore[\"packages/lib/slots.ts\"]:::context\\n    availableslots[\"packages/trpc/server/routers/viewer/slots/util.ts\"]:::context\\nperftest --> bookingscenario\\nperftest --> availableslots\\navailableslots --> slotscore\\nperftest --> analysis\\n\\nsubgraph legend\\n    l1[major edit]:::major-edit\\n    l2[minor edit]:::minor-edit  \\n    l3[context/no edit]:::context\\nend\\n\\nclassdef major-edit fill:#90ee90\\nclassdef minor-edit fill:#87ceeb\\nclassdef context fill:#ffffff\\n```\\nnotes\\n\\nsession details: requested by alex@cal.com (@emrysal) -  session\\nperformance optimization context: this test suite was created to measure the impact of a recent perf: faster logic by preventing instanceof dayjs in slots.ts optimization\\ntest infrastructure: leverages existing cal.com test patterns from bookingscenario.ts and follows established performance testing conventions\\nfuture value: provides ongoing performance monitoring capabilities for the critical slot generation logic\\nrisk assessment: medium risk due to test complexity and environmental dependencies, but provides valuable performance insights for a core cal.com feature']"
        ],
        [
         "8",
         "8",
         "17",
         "8_mvn_wget_java_jsonparser",
         "['mvn', 'wget', 'java', 'jsonparser', 'parsersummary', 'testdefaultkvwriter_writesweepconfig', 'improve', 'commit_memory', 'concurrenttest', 'resolver']",
         "['implement kv batch putmanysummary\\n\\nadd putmany rpc to kv.proto and regenerate protobuf code\\nextend kvservice server and nats store with putmany handling\\nupdate sync poller to batch kv writes using putmany\\nrevise kv interfaces and mocks for putmany support\\nrefresh armis integration tests and sweep result utilities for new apis\\n\\ntesting\\n\\ngo test ./... (fails: testarmisintegration_fetch_withupdaterandcorrelation, testdefaultkvwriter_writesweepconfig, testsweepresultsquery_pagination in pkg/sync/integrations/armis)\\n\\n\\n <URL> ', 'improve wget builder defaults and reading efficiencysummary\\n\\nuse char buffer for better efficiency in wget.fetch\\ncreate default connection provider lazily in wget.builder\\ndocument default value for maxresponsebytes\\n\\ntesting\\n\\nmvn -q test (fails: mvn: command not found)\\n\\n\\n <URL> ', 'improve wget builder defaults and reading efficiencysummary\\n\\nuse char buffer for better efficiency in wget.fetch\\ncreate default connection provider lazily in wget.builder\\ndocument default value for maxresponsebytes\\nupdate appendable exception test to match new fetch behaviour\\n\\ntesting\\n\\nmvn -q test (fails: mvn: command not found)\\n\\n\\n <URL> ']"
        ],
        [
         "9",
         "9",
         "16",
         "9_dotnet_csproj_tests_inlining",
         "['dotnet', 'csproj', 'tests', 'inlining', 'net9', 'structs', 'aggressiveinlining', 'net8', 'lightresults', 'inline']",
         "['apply aggressiveinlining attributessummary\\n\\nhint jit to inline frequently used methods\\ndecorate helper enumerators and string helpers for inlining\\ninline comparison and formatting helpers\\n\\ntesting\\n\\ndotnet test tests/lightresults.tests/lightresults.tests.csproj -f net9.0\\n\\n\\n <URL> ', 'improve method inliningsummary\\n\\ninline equals, gethashcode, and tostring to allow aggressive jit inlining\\nuse methodimploptions.aggressiveinlining in core structs\\n\\ntesting\\n\\ndotnet test tests/lightresults.tests/lightresults.tests.csproj -f net8.0\\n\\n\\n <URL> ', 'improve in parameter modifier example with meaningful struct-based demonstrationfixes #25422\\nproblem\\nthe current example for the in parameter modifier uses a simple int parameter, which doesn\\'t effectively demonstrate the purpose and benefits of the in modifier. as pointed out in the issue:\\n\\nwithout the in keyword, the value would still be 44 (since int is a value type)\\nthe example doesn\\'t show why you\\'d use in in the first place  \\nit only demonstrates that you can\\'t modify the parameter (via commented code)\\n\\nsolution\\nreplaced the inadequate int example with a comprehensive struct-based demonstration that addresses all concerns:\\nbefore (problematic):\\n```csharp\\nint readonlyargument = 44;\\ninargexample(readonlyargument);\\nconsole.writeline(readonlyargument);     // value is still 44\\nvoid inargexample(in int number)\\n{\\n    // uncomment the following line to see error cs8331\\n    //number = 19;\\n}\\n```\\nafter (improved):\\n```csharp\\nvar largestruct = new largestruct { value1 = 42, value2 = 3.14, value3 = \"hello\" };\\n// using \\'in\\' avoids copying the large struct and prevents modification\\nprocesslargestruct(in largestruct);\\nconsole.writeline($\"original value unchanged: {largestruct.value1}\");\\n// without \\'in\\', the struct would be copied (less efficient for large structs)\\nprocesslargestructbyvalue(largestruct);\\nconsole.writeline($\"original value still unchanged: {largestruct.value1}\");\\nvoid processlargestruct(in largestruct data)\\n{\\n    // can read the values\\n    console.writeline($\"processing: {data.value1}, {data.value2}, {data.value3}\");\\n// uncomment the following line to see error cs8331\\n// data.value1 = 99; // compilation error: cannot assign to \\'in\\' parameter\\n\\n}\\nvoid processlargestructbyvalue(largestruct data)\\n{\\n    // this method receives a copy of the struct\\n    console.writeline($\"processing copy: {data.value1}, {data.value2}, {data.value3}\");\\n// modifying the copy doesn\\'t affect the original\\ndata.value1 = 99;\\n\\n}\\n```\\nwhat the new example demonstrates\\n\\nwhy use in: performance benefits when passing large structs (avoids copying)\\nrealistic scenario: processing data without needing to modify it\\nclear comparison: shows both in parameter and regular parameter methods side-by-side\\nimmutability: demonstrates that in parameters cannot be modified\\neducational value: enhanced comments explain the behavior and benefits\\n\\nchanges made\\n\\nupdated firstinexample() method in refparametermodifier.cs with meaningful struct-based example\\nadded largestruct definition with multiple fields to demonstrate performance benefits\\nenhanced comments explaining the purpose and benefits of in modifier\\nmaintained backward compatibility with existing documentation structure\\n\\nthe example now clearly shows why developers would choose to use the in modifier and provides a practical, educational demonstration of its benefits.\\n start  coding agent tips \\n\\n you can make  smarter by setting up custom instructions, customizing its development environment and configuring model context protocol (mcp) servers. learn more  coding agent tips in the docs.']"
        ],
        [
         "10",
         "10",
         "16",
         "10_gradle_gradlew_kotlinx_asset_conversion_refactor",
         "['gradle', 'gradlew', 'kotlinx', 'asset_conversion_refactor', 'sdk', 'modding', 'jdk', 'assembledebug', 'jvm', 'android']",
         "['fix heavy ui updates on main threadsummary\\n\\noffload contact selection loops to dispatchers.default\\ncollect duplicate contacts on dispatchers.default instead of dispatchers.io\\n\\ntesting\\n\\n./gradlew tasks --all\\n./gradlew assembledebug (fails: sdk location not found)\\n\\n\\n <URL> ', 'implement conditional cocoa linking for targets:scroll: description\\nmoved sentry cocoa framework linking configuration to execute only when apple targets are present in the gradle task graph. this prevents unnecessary execution of cocoa-specific logic for non-apple builds\\n:bulb: motivation and context\\npreviously, the plugin would eagerly configure cocoa framework linking on macos, even if the build only involved android or jvm targets. this could lead to build failures if the sentry cocoa xcframework was not found, despite not being needed. this change ensures the linking setup is deferred and only runs when relevant, improving build robustness and efficiency for mixed-platform projects.\\n:green_heart: how did you test it?\\nintegration test\\n:pencil: checklist\\nyou have to check all boxes before merging:\\n\\n[x] i reviewed the submitted code.\\n[x] i added tests to verify the changes.\\n[x] no new pii added or sdk only sends newly added pii if senddefaultpii is enabled.\\n[ ] i updated the docs if needed.\\n[ ] review from the native team if needed.\\n[x] no breaking change or entry added to the changelog.\\n\\n:crystal_ball: next steps', 'feat: complete android project optimization with ksp migrationoverview\\nthis pr implements comprehensive android project optimization focusing on dependency updates, kotlin version management, and complete migration from kapt to ksp.\\nkey changes\\n performance optimizations\\n\\ncomplete kapt to ksp migration: removed all kapt usage in favor of ksp for faster annotation processing\\ngradle update: updated from 8.7 to 8.14 for latest performance improvements\\nbuild performance: enabled parallel builds, caching, and incremental compilation\\n\\n dependency updates\\n\\nkotlin ecosystem: updated kotlinx-coroutines (1.7.3 → 1.8.1) and kotlinx-serialization (1.6.0 → 1.7.3)\\nandroid libraries: updated lifecycle libraries (2.8.7 → 2.9.0) and work-runtime (2.10.0 → 2.11.0)\\nthird-party: updated glide (4.16.0 → 4.17.0) and okhttp (5.0.0-alpha.2 → 4.12.0 stable)\\n\\n build system improvements\\n\\nenhanced gradle properties for better performance\\noptimized resource handling\\nadded build reports for performance monitoring\\n\\nexpected benefits\\n\\n40-60% faster clean builds\\n20-30% faster incremental builds\\nreduced memory usage during compilation\\nbetter developer experience with improved tooling\\n\\ntesting\\n\\n all changes have been verified for compatibility\\n ksp migration tested with annotation processors\\n dependency updates validated\\n\\ndocumentation\\n\\nadded comprehensive  with detailed analysis\\ndocumented all changes and performance expectations\\n\\nrisk assessment\\n\\nlow risk: dependency updates and build optimizations\\nmedium risk: complete kapt to ksp migration (thoroughly tested)\\nmitigation: comprehensive testing and rollback plan available\\n\\nready for review and testing!']"
        ],
        [
         "11",
         "11",
         "22",
         "11_dotnetbuild_dotnet_dotnetbuildfromvmr_csproj",
         "['dotnetbuild', 'dotnet', 'dotnetbuildfromvmr', 'csproj', 'msbuild', 'nuget', 'aspnetcore', 'dotnetbuildsourceonly', 'visualstudio', 'npm']",
         "['make generatedepsfile and generateruntimeconfigurationfiles tasks internally-incrementalthe generatedepsfile and generateruntimeconfigurationfiles tasks were always writing new output files, even when the content was identical to existing files. this caused unnecessary rebuilds throughout the build chain, significantly impacting build performance.\\nchanges made\\ngeneratedepsfile.cs:\\n- modified writedepsfile() method to generate content in memory first\\n- added xxhash64-based comparison with existing file content\\n- only writes the file when content actually differs\\n- preserves all existing validation logic\\ngenerateruntimeconfigurationfiles.cs:\\n- modified writetojsonfile() method to use the same incremental approach\\n- generates json content in memory before comparing with existing file\\n- uses xxhash64 for fast content comparison\\nimplementation details\\n\\nuses system.io.hashing.xxhash64 for efficient content comparison (package already referenced)\\ngenerates output content in memory to compare against existing files\\nmaintains identical output format and all existing functionality\\nhandles edge cases: non-existent files, empty content, large files\\n\\ntesting\\nadded comprehensive unit tests that verify:\\n- files are not rewritten when content is unchanged (timestamp preservation)\\n- files are properly updated when content changes\\n- first-time file creation works correctly\\nperformance impact\\nthis change eliminates unnecessary file writes that were causing cascading rebuilds. based on the issue description, this was identified as a major cause of rebuild performance problems in large repositories.\\nexample behavior\\nbefore: task always writes file, causing downstream rebuilds\\ngeneratedepsfile -> writes deps.json (even if identical) -> triggers dependent tasks\\nafter: task only writes when content changes\\ngeneratedepsfile -> compares content -> skips write if identical -> no unnecessary rebuilds\\nfixes #49349.\\n\\n[!warning]\\n\\nfirewall rules blocked me from connecting to one or more addresses\\ni tried to connect to the following addresses, but was blocked by firewall rules:\\n\\n7tjvsblobprodcus341.vsblob.vsassets.io\\ntriggering command: dotnet test src/tasks/microsoft.net.build.tasks.unittests/microsoft.net.build.tasks.unittests.csproj --filter itdoesnotoverwritefilewithsamecontent --no-build --verbosity normal (dns block)\\ntriggering command: dotnet build src/tasks/microsoft.net.build.tasks/microsoft.net.build.tasks.csproj --verbosity minimal --no-restore (dns block)\\nc78vsblobprodcus322.vsblob.vsassets.io\\ntriggering command: dotnet test src/tasks/microsoft.net.build.tasks.unittests/microsoft.net.build.tasks.unittests.csproj --filter itdoesnotoverwritefilewithsamecontent --no-build --verbosity normal (dns block)\\ntriggering command: dotnet build src/tasks/microsoft.net.build.tasks/microsoft.net.build.tasks.csproj --verbosity minimal --no-restore (dns block)\\nd0svsblobprodcus381.vsblob.vsassets.io\\ntriggering command: dotnet test src/tasks/microsoft.net.build.tasks.unittests/microsoft.net.build.tasks.unittests.csproj --filter itdoesnotoverwritefilewithsamecontent --no-build --verbosity normal (dns block)\\ntriggering command: dotnet build src/tasks/microsoft.net.build.tasks/microsoft.net.build.tasks.csproj --verbosity minimal --no-restore (dns block)\\njd4vsblobprodcus366.vsblob.vsassets.io\\ntriggering command: dotnet test src/tasks/microsoft.net.build.tasks.unittests/microsoft.net.build.tasks.unittests.csproj --filter itdoesnotoverwritefilewithsamecontent --no-build --verbosity normal (dns block)\\ntriggering command: dotnet build src/tasks/microsoft.net.build.tasks/microsoft.net.build.tasks.csproj --verbosity minimal --no-restore (dns block)\\nl49vsblobprodcus358.vsblob.vsassets.io\\ntriggering command: dotnet test src/tasks/microsoft.net.build.tasks.unittests/microsoft.net.build.tasks.unittests.csproj --filter itdoesnotoverwritefilewithsamecontent --no-build --verbosity normal (dns block)\\ntriggering command: dotnet build src/tasks/microsoft.net.build.tasks/microsoft.net.build.tasks.csproj --verbosity minimal --no-restore (dns block)\\nlylvsblobprodcus31.vsblob.vsassets.io\\ntriggering command: dotnet test src/tasks/microsoft.net.build.tasks.unittests/microsoft.net.build.tasks.unittests.csproj --filter itdoesnotoverwritefilewithsamecontent --no-build --verbosity normal (dns block)\\ntriggering command: /home/redacted/work/sdk/sdk/.dotnet/dotnet msbuild /m /nologo /clp:summary /v:minimal /nr:true /warnaserror /p:treatwarningsaserrors=true /p:continuousintegrationbuild=false /home/redacted/work/sdk/sdk/artifacts/toolset/restore.proj /t:__writetoolsetlocation /clp:errorsonly;nosummary /p:__toolsetlocationoutputfile=/home/redacted/work/sdk/sdk/artifacts/toolset/10.0.0-beta.25316.103.txt (dns block)\\ntriggering command: dotnet build src/tasks/microsoft.net.build.tasks/microsoft.net.build.tasks.csproj --verbosity minimal --no-restore (dns block)\\nuy6vsblobprodcus34.vsblob.vsassets.io\\ntriggering command: dotnet test src/tasks/microsoft.net.build.tasks.unittests/microsoft.net.build.tasks.unittests.csproj --filter itdoesnotoverwritefilewithsamecontent --no-build --verbosity normal (dns block)\\ntriggering command: dotnet build src/tasks/microsoft.net.build.tasks/microsoft.net.build.tasks.csproj --verbosity minimal --no-restore (dns block)\\nvb4vsblobprodcus33.vsblob.vsassets.io\\ntriggering command: dotnet test src/tasks/microsoft.net.build.tasks.unittests/microsoft.net.build.tasks.unittests.csproj --filter itdoesnotoverwritefilewithsamecontent --no-build --verbosity normal (dns block)\\ntriggering command: dotnet build src/tasks/microsoft.net.build.tasks/microsoft.net.build.tasks.csproj --verbosity minimal --no-restore (dns block)\\n\\nif you need me to access, download, or install something from one of these locations, you can either:\\n\\nconfigure actions setup steps to set up my environment, which run before the firewall is enabled\\nadd the appropriate urls or hosts to my firewall allow list\\n\\n\\n\\n start  coding agent tips \\n\\n you can make  smarter by setting up custom instructions, customizing its development environment and configuring model context protocol (mcp) servers. learn more  coding agent tips in the docs.', 'build: use --follow-symlinks in vscethis pr implements support for the new --follow-symlinks flag in vsce to eliminate the unnecessary copying of powershell editor services (pses) modules during builds, as requested in #5169.\\nwhat changed\\npackage.json scripts:\\n- added --follow-symlinks flag to both vsce package and vsce publish commands\\n- this allows vsce to handle symbolic links natively instead of requiring file copies\\nbuild script simplification:\\n- removed complex symlink-to-copy conversion logic from the package task (eliminated 8 lines)\\n- modified release configuration to use symlinks consistently with debug mode\\n- removed the \"dumb task of copying pses to vscode for a build\" as mentioned in the issue\\n- added explanatory comments about the new vsce symlink support\\nbenefits\\n\\n faster builds - no more file copying during packaging\\n simplified logic - consistent symlink handling between debug and release modes  \\n reduced complexity - net reduction of 10+ lines of build script code\\n better maintainability - vsce now handles symlinks natively\\n\\nbefore/after comparison\\nbefore: build script would detect symlinks and convert them to copies before packaging:\\npowershell\\nif ((get-item ./modules -erroraction silentlycontinue).linktype -eq \"symboliclink\") {\\n    write-build darkred \"pses is a symbolic link, replacing with copy!\"\\n    remove-builditem ./modules\\n    copy-item -recurse -force \"$(split-path (get-editorservicespath))/module\" ./modules\\n}\\nafter: vsce handles symlinks directly with --follow-symlinks flag:\\njson\\n\"package\": \"vsce package --out out/ --no-githubissuelinking --follow-symlinks\"\\nthis change leverages the symlink following support added to vsce in october 2024 (microsoft/vscode-vsce@51e122a).\\nfixes #5169.\\n\\n[!warning]\\n\\nfirewall rules blocked me from connecting to one or more addresses\\ni tried to connect to the following addresses, but was blocked by firewall rules:\\n\\n0t3vsblobprodcus362.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\n11vvsblobprodcus336.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\n1javsblobprodcus364.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\n1k9vsblobprodcus379.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\n1oavsblobprodcus350.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\n1p7vsblobprodcus324.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\n1s1vsblobprodcus386.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\n2kmvsblobprodcus39.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\n2zrvsblobprodcus388.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\n37bvsblobprodcus311.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\n37cvsblobprodcus359.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\n4m6vsblobprodcus384.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\n4myvsblobprodcus32.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\n4vyvsblobprodcus361.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\n4zjvsblobprodcus390.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\n51yvsblobprodcus36.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\n5dkvsblobprodcus355.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\n5rqvsblobprodcus385.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\n6s7vsblobprodcus313.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\n7devsblobprodcus323.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\n7k6vsblobprodcus337.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\n7tjvsblobprodcus341.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\n80zvsblobprodcus35.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\n8xbvsblobprodcus382.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\n96bvsblobprodcus338.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\najhvsblobprodcus363.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nbcnvsblobprodcus378.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nc50vsblobprodcus330.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nc78vsblobprodcus322.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ncflvsblobprodcus383.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: /home/redacted/work/_temp/ghcca-node/node/bin/node /home/redacted/work/_temp/-developer-action-main/dist/index.js (dns block)\\ntriggering command: npm ci (dns block)\\nckzvsblobprodcus347.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\nd0svsblobprodcus381.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\ndlbvsblobprodcus316.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ne7bvsblobprodcus348.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nfdpvsblobprodcus345.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\nfrdvsblobprodcus327.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\ngbsvsblobprodcus365.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\ngervsblobprodcus329.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nh6tvsblobprodcus346.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\ni1qvsblobprodcus353.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nibzvsblobprodcus369.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\nimzvsblobprodcus368.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\njd4vsblobprodcus366.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\njosvsblobprodcus372.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\njrqvsblobprodcus343.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nk0ivsblobprodcus356.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nk4kvsblobprodcus344.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nkgfvsblobprodcus314.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nkh4vsblobprodcus325.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nkijvsblobprodcus387.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nkmuvsblobprodcus389.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nkxqvsblobprodcus376.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nl49vsblobprodcus358.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nl7avsblobprodcus319.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nljcvsblobprodcus317.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nlylvsblobprodcus31.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nm16vsblobprodcus374.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\nm6xvsblobprodcus342.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nm8dvsblobprodcus37.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nmfjvsblobprodcus373.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nn3kvsblobprodcus335.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\nnn8vsblobprodcus340.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\no3svsblobprodcus318.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nofvvsblobprodcus315.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\np2ovsblobprodcus312.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\npc2vsblobprodcus360.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\npdfvsblobprodcus380.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\npe3vsblobprodcus354.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\npe4vsblobprodcus351.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\npkvvsblobprodcus321.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nrcxvsblobprodcus328.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\ns4uvsblobprodcus326.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\ns8mvsblobprodcus38.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: /home/redacted/work/_temp/ghcca-node/node/bin/node /home/redacted/work/_temp/-developer-action-main/dist/index.js (dns block)\\ntriggering command: npm ci (dns block)\\nsc4vsblobprodcus331.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nse1vsblobprodcus349.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nsqdvsblobprodcus333.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nst8vsblobprodcus339.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\ntphvsblobprodcus375.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nu3hvsblobprodcus371.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nu6ovsblobprodcus377.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nukkvsblobprodcus352.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nuy6vsblobprodcus34.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nv53vsblobprodcus320.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nvb4vsblobprodcus33.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nvwvvsblobprodcus334.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nx3yvsblobprodcus370.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nxupvsblobprodcus332.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\nyluvsblobprodcus367.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nyttvsblobprodcus357.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\nytvvsblobprodcus310.vsblob.vsassets.io\\ntriggering command: npm ci (dns block)\\ntriggering command: npm ci (dns block)\\n\\nif you need me to access, download, or install something from one of these locations, you can either:\\n\\nconfigure actions setup steps to set up my environment, which run before the firewall is enabled\\nadd the appropriate urls or hosts to my firewall allow list\\n\\n\\n\\n start  coding agent tips \\n\\n you can make  smarter by setting up custom instructions, customizing its development environment and configuring model context protocol (mcp) servers. learn more  coding agent tips in the docs.', \"replace math.divrem with bit operations in bitarray for wasm performancethis pr addresses significant performance regressions in system.collections.bitarray operations when compiled for webassembly (wasm), where operations were 1.1x to 5.4x slower than expected.\\nproblem\\nbitarray operations showed major performance regressions in wasm compilation mode:\\n- bitarrayget: 1.41x slower (183.17 ns → 259.16 ns)\\n- bitarrayset: 1.42x slower (34.17 ns → 48.42 ns) \\n- bitarraynot: 4.82x slower (28.54 ns → 137.40 ns)\\n- bitarraysetall: 3.00x slower (35.48 ns → 106.32 ns)\\n- bitarraycopytoboolarray: 1.22x slower (25.45 μs → 31.08 μs)\\n- other operations showing 1.08x to 5.39x slowdowns\\nroot cause\\nthe performance regression was caused by math.divrem function calls that don't compile efficiently in webassembly. bitarray extensively used math.divrem for critical index calculations in hot paths.\\nsolution\\nreplaced all math.divrem calls with mathematically equivalent bit operations optimized for powers of 2:\\ndivision by 8 (bitsperbyte):\\n```csharp\\n// before:\\n(uint byteindex, uint bitoffset) = math.divrem((uint)index, bitsperbyte);\\n// after: \\nuint byteindex = (uint)index >> 3; // equivalent to index / 8\\nuint bitoffset = (uint)index & 7;  // equivalent to index % 8\\n```\\ndivision by 32 (bitsperint32):\\n```csharp\\n// before:\\n(uint wordindex, uint extrabits) = math.divrem((uint)length, bitsperint32);\\n// after:\\nuint wordindex = (uint)length >> 5; // equivalent to length / 32\\nuint extrabits = (uint)length & 31; // equivalent to length % 32\\n```\\nmethods optimized\\n\\nget(int index) and set(int index, bool value) - core bit access methods\\nclearhighextrabits() - internal cleanup method\\nboolean array constructor and copyto remainder loops\\nrightshift and leftshift operations\\n\\nperformance impact\\n\\nmicro-benchmark shows math.divrem is ~50% slower than equivalent bit operations\\nexpected to eliminate the 1.1x to 5.4x performance regressions in wasm\\nno functional changes (all 1698 bitarray tests continue to pass)\\nminimal code size increase due to inline bit operations\\n\\nvalidation\\n\\n all existing bitarray tests pass (1698 tests)\\n libraries build successfully \\n micro-benchmarks confirm performance improvement\\n mathematically equivalent operations verified\\n\\nthe optimization leverages the fact that bitsperbyte (8) and bitsperint32 (32) are powers of 2, allowing division and modulo operations to be replaced with faster bit shifts and masks - a standard compiler optimization that works particularly well in wasm where function call overhead is significant.\\nfixes #116965.\\n\\n[!warning]\\n\\nfirewall rules blocked me from connecting to one or more addresses\\ni tried to connect to the following addresses, but was blocked by firewall rules:\\n\\nfrdvsblobprodcus327.vsblob.vsassets.io\\ntriggering command: /home/redacted/work/runtime/runtime/.dotnet/dotnet /home/redacted/work/runtime/runtime/.dotnet/sdk/10.0.100-preview.6.25302.104/nuget.build.tasks.console.dll recursive=true;enablebinarylogger=true;binaryloggerparameters=%2fhome%2fredacted%2fwork%2fruntime%2fruntime%2fartifacts%2flog%2fdebug%2frestore-build.proj.binlog;cleanupassetsforunsupportedprojects=true;disableparallel=false;force=false;forceevaluate=false;hidewarningsanderrors=false;ignorefailedsources=false;interactive=false;nocache=false;no <URL>  /home/redacted/work/runtime/runtime/.dotnet/sdk/10.0.100-preview.6.25302.104/msbuild.dll /home/redacted/work/runtime/runtime/build.proj continuousintegrationbuild=false;configuration=debug;reporoot=/home/redacted/work/runtime/runtime/;dotnetbuild=false;dotnetbuildsourceonly=false;dotnetbuildfromvmr=false;rebuild=false;test=false;pack=false;integrationtest=false;performancetest=false;restorestaticgraphenablebinarylogger=true;subset=libs;targetos=browser;targetarchitecture=wasm;buildarchitecture=x64;cmakeargs=;versionspropspath=/home/redacted/work/runtime/runtime/eng/versions.props;__buildphase=solutionrestore;_netcore_engineering_telemetry=restore;msbuildrestoresessionid=d7d64be8-6a4e-4080-9054-54a11074636c;restoreusestaticgraphevaluation=true;restoreembedfilesinbinlog=true;restorestaticgraphbinaryloggerparameters=/home/redacted/work/runtime/runtime/artifacts/log/debug/restore-build.proj.binlog;excluderestorepackageimports=true;originalmsbuildstartupdirectory=/home/redacted/work/runtime/runtime (dns block)\\njosvsblobprodcus372.vsblob.vsassets.io\\ntriggering command: /home/redacted/work/runtime/runtime/.dotnet/dotnet /home/redacted/work/runtime/runtime/.dotnet/sdk/10.0.100-preview.6.25302.104/nuget.build.tasks.console.dll recursive=true;enablebinarylogger=true;binaryloggerparameters=%2fhome%2fredacted%2fwork%2fruntime%2fruntime%2fartifacts%2flog%2fdebug%2frestore-build.proj.binlog;cleanupassetsforunsupportedprojects=true;disableparallel=false;force=false;forceevaluate=false;hidewarningsanderrors=false;ignorefailedsources=false;interactive=false;nocache=false;no <URL>  /home/redacted/work/runtime/runtime/.dotnet/sdk/10.0.100-preview.6.25302.104/msbuild.dll /home/redacted/work/runtime/runtime/build.proj continuousintegrationbuild=false;configuration=debug;reporoot=/home/redacted/work/runtime/runtime/;dotnetbuild=false;dotnetbuildsourceonly=false;dotnetbuildfromvmr=false;rebuild=false;test=false;pack=false;integrationtest=false;performancetest=false;restorestaticgraphenablebinarylogger=true;subset=libs;targetos=browser;targetarchitecture=wasm;buildarchitecture=x64;cmakeargs=;versionspropspath=/home/redacted/work/runtime/runtime/eng/versions.props;__buildphase=solutionrestore;_netcore_engineering_telemetry=restore;msbuildrestoresessionid=d7d64be8-6a4e-4080-9054-54a11074636c;restoreusestaticgraphevaluation=true;restoreembedfilesinbinlog=true;restorestaticgraphbinaryloggerparameters=/home/redacted/work/runtime/runtime/artifacts/log/debug/restore-build.proj.binlog;excluderestorepackageimports=true;originalmsbuildstartupdirectory=/home/redacted/work/runtime/runtime (dns block)\\nkmuvsblobprodcus389.vsblob.vsassets.io\\ntriggering command: /home/redacted/work/runtime/runtime/.dotnet/dotnet /home/redacted/work/runtime/runtime/.dotnet/sdk/10.0.100-preview.6.25302.104/nuget.build.tasks.console.dll recursive=true;enablebinarylogger=true;binaryloggerparameters=%2fhome%2fredacted%2fwork%2fruntime%2fruntime%2fartifacts%2flog%2fdebug%2frestore-build.proj.binlog;cleanupassetsforunsupportedprojects=true;disableparallel=false;force=false;forceevaluate=false;hidewarningsanderrors=false;ignorefailedsources=false;interactive=false;nocache=false;no <URL>  /home/redacted/work/runtime/runtime/.dotnet/sdk/10.0.100-preview.6.25302.104/msbuild.dll /home/redacted/work/runtime/runtime/build.proj continuousintegrationbuild=false;configuration=debug;reporoot=/home/redacted/work/runtime/runtime/;dotnetbuild=false;dotnetbuildsourceonly=false;dotnetbuildfromvmr=false;rebuild=false;test=false;pack=false;integrationtest=false;performancetest=false;restorestaticgraphenablebinarylogger=true;subset=libs;targetos=browser;targetarchitecture=wasm;buildarchitecture=x64;cmakeargs=;versionspropspath=/home/redacted/work/runtime/runtime/eng/versions.props;__buildphase=solutionrestore;_netcore_engineering_telemetry=restore;msbuildrestoresessionid=d7d64be8-6a4e-4080-9054-54a11074636c;restoreusestaticgraphevaluation=true;restoreembedfilesinbinlog=true;restorestaticgraphbinaryloggerparameters=/home/redacted/work/runtime/runtime/artifacts/log/debug/restore-build.proj.binlog;excluderestorepackageimports=true;originalmsbuildstartupdirectory=/home/redacted/work/runtime/runtime (dns block)\\nnn8vsblobprodcus340.vsblob.vsassets.io\\ntriggering command: /home/redacted/work/runtime/runtime/.dotnet/dotnet /home/redacted/work/runtime/runtime/.dotnet/sdk/10.0.100-preview.6.25302.104/nuget.build.tasks.console.dll recursive=true;enablebinarylogger=true;binaryloggerparameters=%2fhome%2fredacted%2fwork%2fruntime%2fruntime%2fartifacts%2flog%2fdebug%2frestore-build.proj.binlog;cleanupassetsforunsupportedprojects=true;disableparallel=false;force=false;forceevaluate=false;hidewarningsanderrors=false;ignorefailedsources=false;interactive=false;nocache=false;no <URL>  /home/redacted/work/runtime/runtime/.dotnet/sdk/10.0.100-preview.6.25302.104/msbuild.dll /home/redacted/work/runtime/runtime/build.proj continuousintegrationbuild=false;configuration=debug;reporoot=/home/redacted/work/runtime/runtime/;dotnetbuild=false;dotnetbuildsourceonly=false;dotnetbuildfromvmr=false;rebuild=false;test=false;pack=false;integrationtest=false;performancetest=false;restorestaticgraphenablebinarylogger=true;subset=libs;targetos=browser;targetarchitecture=wasm;buildarchitecture=x64;cmakeargs=;versionspropspath=/home/redacted/work/runtime/runtime/eng/versions.props;__buildphase=solutionrestore;_netcore_engineering_telemetry=restore;msbuildrestoresessionid=d7d64be8-6a4e-4080-9054-54a11074636c;restoreusestaticgraphevaluation=true;restoreembedfilesinbinlog=true;restorestaticgraphbinaryloggerparameters=/home/redacted/work/runtime/runtime/artifacts/log/debug/restore-build.proj.binlog;excluderestorepackageimports=true;originalmsbuildstartupdirectory=/home/redacted/work/runtime/runtime (dns block)\\nofvvsblobprodcus315.vsblob.vsassets.io\\ntriggering command: /home/redacted/work/runtime/runtime/.dotnet/dotnet /home/redacted/work/runtime/runtime/.dotnet/sdk/10.0.100-preview.6.25302.104/nuget.build.tasks.console.dll recursive=true;enablebinarylogger=true;binaryloggerparameters=%2fhome%2fredacted%2fwork%2fruntime%2fruntime%2fartifacts%2flog%2fdebug%2frestore-build.proj.binlog;cleanupassetsforunsupportedprojects=true;disableparallel=false;force=false;forceevaluate=false;hidewarningsanderrors=false;ignorefailedsources=false;interactive=false;nocache=false;no <URL>  /home/redacted/work/runtime/runtime/.dotnet/sdk/10.0.100-preview.6.25302.104/msbuild.dll /home/redacted/work/runtime/runtime/build.proj continuousintegrationbuild=false;configuration=debug;reporoot=/home/redacted/work/runtime/runtime/;dotnetbuild=false;dotnetbuildsourceonly=false;dotnetbuildfromvmr=false;rebuild=false;test=false;pack=false;integrationtest=false;performancetest=false;restorestaticgraphenablebinarylogger=true;subset=libs;targetos=browser;targetarchitecture=wasm;buildarchitecture=x64;cmakeargs=;versionspropspath=/home/redacted/work/runtime/runtime/eng/versions.props;__buildphase=solutionrestore;_netcore_engineering_telemetry=restore;msbuildrestoresessionid=d7d64be8-6a4e-4080-9054-54a11074636c;restoreusestaticgraphevaluation=true;restoreembedfilesinbinlog=true;restorestaticgraphbinaryloggerparameters=/home/redacted/work/runtime/runtime/artifacts/log/debug/restore-build.proj.binlog;excluderestorepackageimports=true;originalmsbuildstartupdirectory=/home/redacted/work/runtime/runtime (dns block)\\npdfvsblobprodcus380.vsblob.vsassets.io\\ntriggering command: /home/redacted/work/runtime/runtime/.dotnet/dotnet /home/redacted/work/runtime/runtime/.dotnet/sdk/10.0.100-preview.6.25302.104/nuget.build.tasks.console.dll recursive=true;enablebinarylogger=true;binaryloggerparameters=%2fhome%2fredacted%2fwork%2fruntime%2fruntime%2fartifacts%2flog%2fdebug%2frestore-build.proj.binlog;cleanupassetsforunsupportedprojects=true;disableparallel=false;force=false;forceevaluate=false;hidewarningsanderrors=false;ignorefailedsources=false;interactive=false;nocache=false;no <URL>  /home/redacted/work/runtime/runtime/.dotnet/sdk/10.0.100-preview.6.25302.104/msbuild.dll /home/redacted/work/runtime/runtime/build.proj continuousintegrationbuild=false;configuration=debug;reporoot=/home/redacted/work/runtime/runtime/;dotnetbuild=false;dotnetbuildsourceonly=false;dotnetbuildfromvmr=false;rebuild=false;test=false;pack=false;integrationtest=false;performancetest=false;restorestaticgraphenablebinarylogger=true;subset=libs;targetos=browser;targetarchitecture=wasm;buildarchitecture=x64;cmakeargs=;versionspropspath=/home/redacted/work/runtime/runtime/eng/versions.props;__buildphase=solutionrestore;_netcore_engineering_telemetry=restore;msbuildrestoresessionid=d7d64be8-6a4e-4080-9054-54a11074636c;restoreusestaticgraphevaluation=true;restoreembedfilesinbinlog=true;restorestaticgraphbinaryloggerparameters=/home/redacted/work/runtime/runtime/artifacts/log/debug/restore-build.proj.binlog;excluderestorepackageimports=true;originalmsbuildstartupdirectory=/home/redacted/work/runtime/runtime (dns block)\\nse1vsblobprodcus349.vsblob.vsassets.io\\ntriggering command: /home/redacted/work/runtime/runtime/.dotnet/dotnet /home/redacted/work/runtime/runtime/.dotnet/sdk/10.0.100-preview.6.25302.104/nuget.build.tasks.console.dll recursive=true;enablebinarylogger=true;binaryloggerparameters=%2fhome%2fredacted%2fwork%2fruntime%2fruntime%2fartifacts%2flog%2fdebug%2frestore-build.proj.binlog;cleanupassetsforunsupportedprojects=true;disableparallel=false;force=false;forceevaluate=false;hidewarningsanderrors=false;ignorefailedsources=false;interactive=false;nocache=false;no <URL>  /home/redacted/work/runtime/runtime/.dotnet/sdk/10.0.100-preview.6.25302.104/msbuild.dll /home/redacted/work/runtime/runtime/build.proj continuousintegrationbuild=false;configuration=debug;reporoot=/home/redacted/work/runtime/runtime/;dotnetbuild=false;dotnetbuildsourceonly=false;dotnetbuildfromvmr=false;rebuild=false;test=false;pack=false;integrationtest=false;performancetest=false;restorestaticgraphenablebinarylogger=true;subset=libs;targetos=browser;targetarchitecture=wasm;buildarchitecture=x64;cmakeargs=;versionspropspath=/home/redacted/work/runtime/runtime/eng/versions.props;__buildphase=solutionrestore;_netcore_engineering_telemetry=restore;msbuildrestoresessionid=d7d64be8-6a4e-4080-9054-54a11074636c;restoreusestaticgraphevaluation=true;restoreembedfilesinbinlog=true;restorestaticgraphbinaryloggerparameters=/home/redacted/work/runtime/runtime/artifacts/log/debug/restore-build.proj.binlog;excluderestorepackageimports=true;originalmsbuildstartupdirectory=/home/redacted/work/runtime/runtime (dns block)\\n\\nif you need me to access, download, or install something from one of these locations, you can either:\\n\\nconfigure actions setup steps to set up my environment, which run before the firewall is enabled\\nadd the appropriate urls or hosts to my firewall allow list\\n\\n\\n\\n start  coding agent tips \\n\\n share your feedback on  coding agent for the chance to win a $200 gift card! click here to start the survey.\"]"
        ],
        [
         "12",
         "12",
         "17",
         "12_github_git_gitlab_bitbucket",
         "['github', 'git', 'gitlab', 'bitbucket', 'writechangesetoptimized', 'filechanges', 'commits', 'determinefilechanges', 'test_docs_build', 'commit']",
         "['fix github api rate limiting in chess workflow by replacing api calls with local file storageproblem\\nthe chess game workflow was experiencing rate limiting issues due to excessive github api calls. every time a move was made, the workflow would call @octokit.list_issues() to:\\n\\ncheck if the same user made the previous move (consecutive move prevention)\\nbuild the \"last few moves\" section in the readme\\ngenerate the \"top 20 leaderboard\" with move counts\\n\\nwith the popularity of the chess game, these api calls were hitting github\\'s rate limits and causing the workflow to fail.\\nsolution\\nthis pr eliminates the rate limiting issue by replacing api calls with local file storage while maintaining 100% compatibility with the existing user experience.\\nkey changes\\n removed api dependencies:\\n- eliminated all @octokit.list_issues() calls that were causing rate limiting\\n- replaced api-based data retrieval with local file operations\\n added local data storage:\\n- chess_games/recent_moves.txt - stores last 5 moves with automatic rotation\\n- chess_games/leaderboard.txt - maintains top 20 players sorted by move count\\n- chess_games/last_mover.txt - tracks last player for consecutive move prevention\\n improved performance:\\n- instant file i/o operations instead of slow api calls\\n- no more rate limiting delays or failures\\n- faster readme generation\\n robust error handling:\\n- graceful fallbacks when files don\\'t exist or are corrupted\\n- non-blocking operations that won\\'t break gameplay\\n- consistent \"history temporarily unavailable\" messages for any issues\\nbefore vs after\\nbefore (rate limited):\\n```ruby\\nheavy api call that causes rate limiting\\nissues = @octokit.list_issues(env.fetch(\\'repository\\'), state: \\'closed\\')\\nmoves = issues.select{|issue| issue.title.start_with? \\'chess|move|\\'}\\nprocess hundreds/thousands of issues...\\n```\\nafter (local files):\\n```ruby\\nfast local file read\\nleaderboard_lines = file.readlines(\"chess_games/leaderboard.txt\")\\nleaderboard_lines.each { |line| new_readme.concat \"#{line}\\\\n\" }\\n```\\npreserved functionality\\n recent moves still display in readme\\n leaderboard still shows accurate player rankings\\n consecutive move prevention still works\\n all game mechanics remain identical\\n same readme format and styling\\n all existing links and workflows preserved  \\ntesting\\nthe changes have been thoroughly tested with simulations covering:\\n- file operations when files exist and don\\'t exist\\n- leaderboard updates and sorting logic\\n- recent moves rotation (maintains exactly 5 entries)\\n- consecutive move prevention with local data\\n- readme generation producing identical output format\\n- error handling scenarios\\nfixes #19584.\\n\\n you can make  smarter by setting up custom instructions, customizing its development environment and configuring model context protocol (mcp) servers. learn more  coding agent tips in the docs.', \"implement selective file push for github multi-file sync to reduce unnecessary writesproblem\\ncurrently, when syncing to github in multi-file mode, all json files (token sets, themes, metadata) are pushed regardless of whether they've actually changed. this results in:\\n\\nunnecessary writes and longer sync times\\nbloated commit histories with unchanged files\\nincreased risk of merge conflicts\\nharder change reviews\\n\\nsolution\\nthis pr implements selective file pushing that only syncs files that have actually changed, leveraging the existing change detection from @changedstatelist.tsx.\\nkey changes\\n new feature flag\\n\\nadded selectivesync feature flag to control the new behavior\\nmaintains backward compatibility when flag is disabled\\n\\n core implementation\\n\\ngittokenstorage: added optional changedfiles parameter to filter files during push\\ngithub provider: integrated usechangedstate hook to detect changes and pass selective file information\\nremote tokens: enhanced pushtokens to support feature flags\\n\\n utility functions\\n\\ngetchangedfiles(): determines which files have changes based on state comparison\\nfilterchangedfiles(): filters file arrays to only include changed items\\n\\nhow it works\\n```typescript\\n// when both selectivesync and multifilesync flags are enabled:\\nconst changedfiles = getchangedfiles(changedpushstate);\\n// only files in changedfiles set are included in the github push\\n// example: if only 'global' token set changed:\\nchangedfiles = new set(['global']) // only global.json is pushed\\n```\\nbehavior\\n| feature flags | behavior |\\n|---------------|----------|\\n| selectivesync: false or missing | push all files (existing behavior) |\\n| selectivesync: true + multifilesync: true | push only changed files |\\n| selectivesync: true + multifilesync: false | push all files (single file mode) |\\ntesting\\n\\n comprehensive unit tests for utility functions\\n gittokenstorage tests covering selective and full sync scenarios  \\n integration tests ensuring backward compatibility\\n mock updates to support new hook dependencies\\n\\nbenefits\\n\\nfaster syncs: only changed files are processed\\ncleaner history: commits only include actual changes\\nbetter reviews: easier to see what actually changed\\nreduced conflicts: less chance of merge conflicts on unchanged files\\n\\nthis change is fully backward compatible and only activates when both feature flags are enabled.\\n\\n[!warning]\\n\\nfirewall rules blocked me from connecting to one or more addresses\\ni tried to connect to the following addresses, but was blocked by firewall rules:\\n\\n <URL> \\ntriggering command: /usr/bin/python3 -u bin/walinuxagent-2.13.1.1-py3.9.egg -collect-logs (http block)\\n\\nif you need me to access, download, or install something from one of these locations, you can either:\\n\\nconfigure actions setup steps to set up my environment, which run before the firewall is enabled\\nadd the appropriate urls or hosts to my firewall allow list\\n\\n\\n\\n\\n you can make  smarter by setting up custom instructions, customizing its development environment and configuring model context protocol (mcp) servers. learn more  coding agent tips in the docs.\", 'optimize github sync functionality by using lastsyncedstate instead of remote pullsproblem\\nthe current github sync functionality is inefficient because it:\\n1. pulls from remote before comparing changes, even when lastsyncedstate is available locally\\n2. pushes all files in a changeset, regardless of whether they actually changed\\n3. makes unnecessary api calls that increase sync time and api usage\\nthis differs from the approach in pr #3402 and instead leverages the lastsyncedstate stored in the application state to determine what files have actually changed.\\nsolution\\nthis pr implements a github-specific optimization that:\\n eliminates unnecessary remote pulls\\n\\nuses lastsyncedstate stored locally to determine what has changed\\nonly pulls from remote when lastsyncedstate is unavailable or invalid\\n\\n implements file-level change detection\\n\\nnew determinefilechanges() utility compares current state with lastsyncedstate\\nidentifies exactly which files need to be created, updated, or deleted\\nsupports both single-file and multi-file repository structures\\n\\n optimizes push operations\\n\\nonly includes changed files in the github push changeset\\nproperly handles file deletions for removed token sets\\nfalls back gracefully to regular sync when optimization isn\\'t applicable\\n\\nkey changes\\ncore implementation\\n\\nsrc/utils/determinefilechanges.ts - new utility for file-level change detection\\nsrc/storage/githubtokenstorage.ts - added writechangesetoptimized() method\\nsrc/app/store/providers/github/github.tsx - integrated optimization into push flow\\nsrc/selectors/index.ts - export missing tokenformatselector\\n\\ntesting\\n\\ncomprehensive unit tests for determinefilechanges() (87% coverage)\\nintegration tests validating the complete optimization flow\\nall existing github storage tests continue to pass\\nverified other storage providers (gitlab, bitbucket, ado) are unaffected\\n\\nexamples\\nmulti-file optimization\\n```typescript\\n// before: pushes all files + pulls remote tree\\nawait storage.writechangeset(allfiles, message, branch);\\n// after: only pushes changed files, no remote pull needed\\nconst filechanges = determinefilechanges(tokens, themes, format, lastsyncedstate, path, ismultifile, issinglefile);\\nif (filechanges.haschanges) {\\n  await storage.writechangesetoptimized(onlychangedfiles, message, branch, false, filechanges.filestodelete);\\n}\\n```\\nchange detection logic\\n```typescript\\n// detects new token sets\\nexpect(filechanges.filestocreate).tocontain(\\'tokens/semantic.json\\');\\n// detects updated files\\nexpect(filechanges.filestoupdate).tocontain(\\'tokens/global.json\\');\\n// detects files to delete\\nexpect(filechanges.filestodelete).tocontain(\\'tokens/oldtokenset.json\\');\\n```\\nperformance impact\\n\\nreduced github api calls by eliminating pre-push remote tree fetching\\nminimized data transfer by only sending files that actually changed\\nfaster sync times especially for large repositories with many token files\\nbetter user experience with reduced loading times during push operations\\n\\nbackward compatibility\\n\\n maintains full backward compatibility\\n falls back to regular sync when optimization cannot be applied\\n no changes to other storage providers (gitlab, bitbucket, ado)\\n no breaking changes to existing apis\\n\\ntesting\\n```bash\\nrun optimization-specific tests\\nnpm test -- --testpathpattern=\"(determinefilechanges|githuboptimization)\"\\nverify github storage functionality\\nnpm test -- --testpathpattern=\"githubtokenstorage\"\\nensure build still works\\nnpm run build:dev\\n```\\nall tests pass with 39/39  and the build compiles successfully.\\n start  coding agent tips \\n\\n share your feedback on  coding agent for the chance to win a $200 gift card! click here to start the survey.']"
        ],
        [
         "13",
         "13",
         "14",
         "13_pharmacyincomecostbillitemdto_opd_income_report_dto_labincomereportdto_opdincomereportdto",
         "['pharmacyincomecostbillitemdto', 'opd_income_report_dto', 'labincomereportdto', 'opdincomereportdto', 'dto', 'dtossummary', 'opdreportcontroller', 'fetchopdincomereportdtos', 'maven', 'pharmacyfastretailsaleforcashiercontroller']",
         "['implement bill item dto for pharmacy income reportsummary\\n\\noptimize pharmacy income & cost report by bill item\\nadd pharmacyincomecostbillitemdto\\nfetch bill item dtos in billservice\\nsupport dto rows in incomebundle and incomerow\\nadjust report controller and jsf view\\nallow loading bill by id in bill search\\n\\ntesting\\n\\nmvn test (fails: mvn not installed)\\n\\n\\n <URL> ', 'implement bill item dto for pharmacy income reportsummary\\n\\noptimize pharmacy income & cost report by bill item\\nadd pharmacyincomecostbillitemdto\\nfetch bill item dtos in billservice\\nsupport dto rows in incomebundle and incomerow\\nadjust report controller and jsf view\\nallow loading bill by id in bill search\\nfix constructor clash and missing import\\n\\ncloses #13176\\ntesting\\n\\nmvn test (fails: mvn not installed)\\n\\n\\n <URL> ', 'optimize opd income report with dto queriessummary\\n\\nadd opdincomereportdto with basic and detailed constructors\\nextend incomerow and incomebundle for new dto\\nimplement billservice.fetchopdincomereportdtos\\nsupport optimized navigation in opdreportcontroller\\nadd dto-based page opd_income_report_dto.xhtml\\ninclude navigation toggle on legacy page\\nadd unit tests for dto query method\\n\\nnavigation path for qa testing:\\n1. opd → analytics → summary reports → opd income report\\n2. toggle between legacy/optimized pages using buttons\\n3. configuration key opd income report - optimized method enables optimized page by default\\ncloses #0\\n\\n <URL> ']"
        ],
        [
         "14",
         "14",
         "12",
         "14_testfortrancompiler_vmvalid_golden_fortran_gfortran_compiler",
         "['testfortrancompiler_vmvalid_golden', 'fortran', 'gfortran', 'compiler', 'compilersummary', 'folding', 'fold', 'constants', 'compile', 'foldingsummary']",
         "['improve fortran compiler constant foldingsummary\\n\\nfold constant string lists in append and set operations\\nprecompute membership checks when the list contains constant strings\\ntrack constant string lists during code generation\\n\\ntesting\\n\\ngo test ./compiler/x/fortran -tags slow -count=1\\n\\n\\n <URL> ', 'improve fortran constant list foldingsummary\\n\\nadd constant list propagation to the fortran compiler\\nfold len, count, append, union, except, etc. when lists are stored in variables\\ndocument new capability in tests/machine/x/fortran/readme.md\\nrecord progress in compiler/x/fortran/tasks.md\\n\\ntesting\\n\\ngo test ./...\\n\\n\\n <URL> ', 'improve fortran constant foldingsummary\\n\\nextend the fortran compiler with tracking for boolean and float list constants\\nfold len, count, append, union, union_all, except, and intersect when applied to constant boolean or float lists\\nexpose helpers literalboolunary, literalfloatunary and propagate literal bool/float expressions\\ndocument the new optimisation in the fortran readme and tasks\\n\\ntesting\\n\\ngo test -c ./compiler/x/fortran -tags slow\\n\\n\\n <URL> ']"
        ],
        [
         "15",
         "15",
         "15",
         "15_left_join_outer_join_testclojurecompiler_vmvalid_golden_compiler",
         "['left_join', 'outer_join', 'testclojurecompiler_vmvalid_golden', 'compiler', 'joins', 'join', 'testgocompiler_goldenoutput', 'compilersummary', 'clojure', 'testscalacompiler_goldenoutput']",
         "['improve go join compilationsummary\\n\\nimprove go query compiler: detect simple left joins so helper is skipped\\nregenerate left_join.go\\nnote new join behaviour in go machine readme\\n\\ntesting\\n\\ngo test ./compiler/x/go -tags slow\\ngo test ./compiler/x/go -tags slow -run testgocompiler_validprograms/left_join\\n\\n\\n <URL> ', 'improve clojure join compilationsummary\\n\\noptimize clojure compiler by translating simple left/right/outer joins to direct comprehensions\\nregenerate outer_join machine translation without _query helper\\nupdate machine readme checklist\\nnote progress in clojure tasks\\n\\ntesting\\n\\ngo test -tags slow ./compiler/x/clj -run testclojurecompiler_vmvalid_golden/outer_join -count=1\\n\\n\\n <URL> ', 'add map-based left join in go compilersummary\\n\\nimplement eqjoinkeystyped helper for join key types\\ngenerate optimized map-based left join queries in the go backend\\nregenerate go machine output for left join programs\\nmark left join task complete\\nregenerate tpch q1 go code\\n\\ntesting\\n\\ngo test ./compiler/x/go -tags slow -run tpch -update -count=1\\ngo test ./compiler/x/go -tags slow -run validprograms/left_join -update -count=1\\n\\n\\n <URL> ']"
        ],
        [
         "16",
         "16",
         "11",
         "16_messagecache_chat_messageids_useaiassistantchat",
         "['messagecache', 'chat', 'messageids', 'useaiassistantchat', 'frontend', 'latestmessage', 'isaichatopened', 'messagesnewerthan', 'frontends', 'conversations']",
         "['feat: optimize duplicate checking in userealtimemessages with o(1) set/map lookupsoptimize duplicate checking in userealtimemessages with o(1) set/map lookups\\nsummary\\nthis pr implements efficient duplicate checking in the userealtimemessages hook by replacing o(n) array.some() operations with o(1) set/map lookups, addressing the todo comment on line 179.\\nchanges made\\n\\nadded messagecache interface with messageids set and usercontentmap map for o(1) lookups\\nreplaced o(n) duplicate checking with efficient set/map-based approach\\nsplit isduplicatemessage function to reduce cognitive complexity (was 19, now under 15)\\nadded proper typescript type guards for optional timestamp property handling\\nmaintained existing functionality for both message id and content-based duplicate detection\\nremoved completed todo comment about implementing efficient duplicate checking\\n\\ntechnical details\\nbefore (o(n) complexity):\\ntypescript\\nconst duplicatebyid = messages.some((msg) => msg.id === newentry.id)\\nconst contentduplicate = messages.some((msg) => { /* complex logic */ })\\nafter (o(1) complexity):\\ntypescript\\nconst messagecache = createmessagecache(messages) // creates set/map\\nif (messagecache.messageids.has(newentry.id)) return true\\nconst existingmessages = messagecache.usercontentmap.get(newentry.content)\\nperformance impact\\n\\nmessage id checking: o(n) → o(1)\\ncontent duplicate checking: o(n) → o(1) for lookup + o(k) for timestamp comparison (where k is number of messages with same content)\\noverall improvement: significant performance boost for chat sessions with many messages\\n\\ntesting\\n\\n code passes all linting checks (pnpm lint)\\n typescript compilation successful\\n maintains existing duplicate detection logic\\n proper type safety with optional timestamp handling\\n\\nfiles changed\\n\\nfrontend/apps/app/components/chat/hooks/userealtimemessages.ts\\n\\nlink to  run\\n <URL> \\nrequested by\\nhirotaka.miyagi@route06.co.jp', \"optimize chat api/job schema transfer by removing http payload overheadoptimize chat api/job schema transfer by removing http payload overhead\\nsummary\\nthis pr optimizes the chat api/job system by removing unnecessary schemadata transfer through http payloads and leveraging the existing repository pattern for schema retrieval within the job context.\\nproblem\\nthe current implementation had significant inefficiencies:\\n\\nlarge http payloads: schemadata was being passed through http request bodies in both the api route and job trigger, resulting in large json transfers\\nredundant data transfer: schema data was being sent via http when the job already had access to retrieve it directly from the database\\nunnecessary coupling: frontend components needed to pass schema data they didn't actually use\\n\\nsolution\\nchanges made\\n\\napi route optimization (frontend/apps/app/app/api/chat/route.ts)\\nremoved schemadata from chatrequestschema validation\\n\\neliminated schemaschema import as it's no longer needed\\n\\n\\njob payload optimization (frontend/internal-packages/jobs/src/trigger/chatjobs.ts)\\n\\nupdated chatjobpayload type to exclude schemadata\\nimplemented schema fetching using repositories.schema.getschema(designsessionid)\\nadded proper error handling for schema retrieval failures\\n\\nused sophisticated type inference to maintain type safety\\n\\n\\nfrontend cleanup \\n\\nchat component (frontend/apps/app/components/chat/chat.tsx): removed schemadata from sendchatmessage calls\\nmessage service (frontend/apps/app/components/chat/services/aimessageservice.ts): \\nremoved schemadata from sendchatmessageparams interface\\nupdated callchatapi function signature\\nremoved schema import as it's no longer needed\\n\\n\\n\\nbenefits\\n\\nreduced network overhead: eliminates large schema json from http request bodies\\nimproved performance: faster api calls due to smaller payloads\\nbetter architecture: proper separation of concerns - data fetching happens where it's needed\\nmaintained functionality: all existing chat features work exactly the same\\n\\ntechnical details\\n\\nleverages existing @liam-hq/agent repository pattern\\nuses supabaseschemarepository.getschema(designsessionid) for schema retrieval\\nmaintains type safety through sophisticated typescript type inference\\npasses all linting checks (biome, eslint, typescript)\\n\\ntesting\\n\\n all linting checks pass (pnpm lint)\\n typescript compilation successful\\n no breaking changes to existing interfaces\\n repository pattern integration verified\\n\\nlink to  run:  <URL> \\nrequested by: hirotaka.miyagi@route06.co.jp\", \"introduce uid support for messages to enable efficient frontend trackingthis pr introduces unique identifiers (uids) for all message types to enable efficient message tracking and filtering, particularly beneficial for frontend applications that need to sync message state without unnecessary data transfer.\\noverview\\nall messages now generate deterministic sha256-based uids from their content, enabling:\\n- efficient sync: frontend can request only messages newer than a known uid\\n- deduplication: identical messages are easily identified by matching uids\\n- state management: track which messages have been processed/displayed\\n- bandwidth optimization: avoid re-transmitting known messages\\nchanges made\\ncore message interface\\n\\nadded getuid(): string method to messageinterface\\nall message types (systemmessage, assistantmessage, usermessage, toolcallmessage) now implement uid generation\\nuids are deterministic - same content always produces the same uid\\n\\nmessagebag enhancements\\nadded new methods to messagebag and messagebaginterface:\\n- findbyuid(string $uid): ?messageinterface - find message by uid\\n- hasmessagewithuid(string $uid): bool - check if uid exists\\n- getuids(): array - get all uids in order\\n- messagesafteruid(string $uid): array - get messages after a specific uid\\n- messagesnewerthan(string $uid): self - get messagebag with newer messages\\nuid generation strategy\\n\\nuses sha256 hashing for deterministic, content-based uids\\nincludes message role and content in hash computation\\ncomplex content (tool calls, multiple content objects) is serialized consistently\\n64-character hex string format\\n\\nusage example\\n```php\\nuse phpllm\\\\llmchain\\\\platform\\\\message\\\\message;\\nuse phpllm\\\\llmchain\\\\platform\\\\message\\\\messagebag;\\n// messages generate consistent uids\\n$message1 = message::forsystem('welcome');\\n$message2 = message::forsystem('welcome'); // same uid as message1\\n$usermsg = message::ofuser('hello');\\n$bag = new messagebag($message1, $usermsg);\\n// get only new messages since last sync\\n$lastknownuid = $message1->getuid();\\n$newmessages = $bag->messagesnewerthan($lastknownuid);\\n// frontend can now request only messages it hasn't seen\\n$uids = $bag->getuids();\\n$latestmessage = $bag->findbyuid(end($uids));\\n```\\nfrontend integration benefits\\nthis enables the exact use case described in the issue - frontends can now:\\n1. track the last known message uid\\n2. request only messages newer than that uid\\n3. avoid downloading the entire conversation history on each update\\n4. efficiently manage real-time message updates\\nbackward compatibility\\nthis change is fully backward compatible. existing code continues to work unchanged, and the new uid functionality is opt-in.\\ntesting\\n\\ncomprehensive test suite covering all message types and uid scenarios\\nverified deterministic behavior across identical content\\ntested messagebag filtering functionality\\nvalidated practical frontend-backend sync patterns\\n\\nfixes #77.\\n\\n[!warning]\\n\\nfirewall rules blocked me from connecting to one or more addresses\\ni tried to connect to the following addresses, but was blocked by firewall rules:\\n\\n <URL> \\ntriggering command: /usr/bin/php8.3 -n -c /tmp/e7slnf /usr/bin/composer install --ignore-platform-req=ext-mongodb (http block)\\n <URL> \\ntriggering command: /usr/bin/php8.3 -n -c /tmp/9szrkl /usr/bin/composer install --ignore-platform-req=ext-mongodb --no-dev (http block)\\n\\nif you need me to access, download, or install something from one of these locations, you can either:\\n\\nconfigure actions setup steps to set up my environment, which run before the firewall is enabled\\nadd the appropriate urls or hosts to my firewall allow list\\n\\n\\n\\n start  coding agent tips \\n\\n you can make  smarter by setting up custom instructions, customizing its development environment and configuring model context protocol (mcp) servers. learn more  coding agent tips in the docs.\"]"
        ],
        [
         "17",
         "17",
         "14",
         "17_csharp_linq_debuggability_cs",
         "['csharp', 'linq', 'debuggability', 'cs', 'compatibility', 'trimstacktrace', 'net', 'littleendian', 'stacktrace', 'validating']",
         "['optimize trimstacktrace to use spans instead of regex.splitthis pr optimizes the trimstacktrace method in exceptionhelper.cs by replacing the inefficient regex.split usage with a span-based approach that eliminates unnecessary allocations.\\nproblem\\nthe current implementation uses regex.split(stacktrace, environment.newline) which is overkill for simple string splitting and causes unnecessary allocations:\\ncsharp\\nstring[] stackframes = regex.split(stacktrace, environment.newline);\\nthis approach:\\n- compiles a regex for a simple string split operation\\n- allocates a string[] array to hold the split results\\n- creates individual string objects for each stack frame\\nsolution\\nreplaced the regex-based implementation with a span-based approach that processes the input without allocations:\\n```csharp\\nreadonlyspan remaining = stacktrace.asspan();\\nreadonlyspan newline = environment.newline.asspan();\\nwhile (!remaining.isempty)\\n{\\n    int index = remaining.indexof(newline);\\n    readonlyspan line = index >= 0 \\n        ? remaining.slice(0, index) \\n        : remaining;\\n// process line without creating string objects\\nif (!line.isempty && !hasreferencetoutf(line))\\n{\\n    result.append(line);\\n    result.append(environment.newline);\\n}\\n\\nremaining = index >= 0 \\n    ? remaining.slice(index + newline.length) \\n    : readonlyspan<char>.empty;\\n\\n}\\n```\\nchanges made\\n\\noptimized trimstacktrace method: replaced regex.split with manual span-based parsing\\nadded span-compatible overload: new hasreferencetoutf(readonlyspan<char>) method for zero-allocation string matching\\nmaintained backward compatibility: existing hasreferencetoutf(string) method preserved\\nadded comprehensive tests: new test suite covering all edge cases and validating identical behavior\\n\\nperformance benefits\\n\\neliminates regex overhead: no regex compilation for simple string operations\\nreduces allocations: no string[] allocation from splitting operations\\nzero-copy processing: uses spans to process input without intermediate string creation\\nmaintains identical behavior: 100% behavioral compatibility verified through extensive testing\\n\\nvalidation\\ncreated comprehensive test suite that validates identical behavior between original and optimized implementations across all edge cases including:\\n- empty strings and single lines\\n- different newline formats (\\\\r\\\\n, \\\\n)\\n- mixed content with utf framework references\\n- empty lines and trailing whitespace\\nfixes #4266.\\n start  coding agent tips \\n\\n share your feedback on  coding agent for the chance to win a $200 gift card! click here to start the survey.', 'add littleendian methods and optimize bigendian performance in extensions.csproblem\\nthe extensions.cs file in src/supersocket.protobase/extensions.cs had two main issues:\\n\\n\\nmissing littleendian support: only provided tryreadbigendian methods for unsigned integers, limiting library utility for protocols that use little-endian byte order.\\n\\n\\nperformance bottlenecks in bigendian methods: the existing tryreadbigendian methods for uint and ulong used inefficient floating-point arithmetic (math.pow(256, n)) and division loops instead of optimized bit operations.\\n\\n\\nsolution\\n added littleendian methods\\nimplemented three new extension methods with comprehensive xml documentation:\\n\\ntryreadlittleendian(out ushort value) - reads 16-bit values in little-endian format\\ntryreadlittleendian(out uint value) - reads 32-bit values in little-endian format  \\ntryreadlittleendian(out ulong value) - reads 64-bit values in little-endian format\\n\\n optimized bigendian performance\\nreplaced inefficient implementations with fast bit operations:\\nbefore (inefficient):\\ncsharp\\nvar unit = (int)math.pow(256, 3);  // floating-point arithmetic\\nfor (var i = 0; i < 4; i++) {\\n    v += unit * b;\\n    unit = unit / 256;  // division in loop\\n}\\nafter (optimized):\\ncsharp\\nvalue = (uint)((b0 << 24) | (b1 << 16) | (b2 << 8) | b3);  // direct bit shifting\\n comprehensive test coverage\\nadded complete test suite for all new littleendian methods:\\n- testreadlittleendianushort()\\n- testreadlittleendianuint() \\n- testreadlittleendianulong()\\ntests use binaryprimitives.writeuint*littleendian() to generate proper test data and validate against edge cases including maxvalue and minvalue.\\nperformance impact\\nexpected improvements:\\n- 10-100x faster uint reading (eliminates math.pow + division loops)\\n- 10-100x faster ulong reading (eliminates math.pow + division loops)\\n- cpu cache friendly (no floating-point operations)\\n- reduced allocations (no intermediate calculations)\\ncode quality\\n\\n maintains consistent api patterns and error handling\\n comprehensive xml documentation for all new methods\\n follows existing code style and naming conventions\\n same method signatures and behavior patterns as bigendian counterparts\\n proper bounds checking and graceful error handling\\n\\nfiles changed\\n\\nsrc/supersocket.protobase/extensions.cs: +84 lines of new functionality, optimized existing methods\\ntest/supersocket.tests/sequencereaderextensiontest.cs: +66 lines of comprehensive test coverage\\n\\nthis enhancement provides complete endianness support for the supersocket library while significantly improving performance for binary data processing scenarios.\\n start  coding agent tips \\n\\n share your feedback on  coding agent for the chance to win a $200 gift card! click here to start the survey.', 'refactor: python.net wrapper with improved type conversions, error handling, and performance optimizationsthis pr comprehensively refactors the python.net wrapper implementation to follow best practices for .net interoperability, focusing on type conversions, error handling, and performance improvements while maintaining full backward compatibility.\\nkey improvements\\nenhanced type conversions\\n\\ndatetime conversions: replaced string-based parsing with direct c# constructor calls for better performance and reliability\\ndecimal conversions: added support for multiple input types (int, float, decimal, str) with optimized conversion paths\\nproper null handling: fixed c# nullable type handling to prevent runtime errors when working with optional ohlcv data\\n\\nrobust error handling\\n\\ncustom exception hierarchy: added stockindicatorserror, typeconversionerror, validationerror, and indicatorcalculationerror for better error categorization\\nstructured initialization: improved .net assembly loading with detailed error reporting and troubleshooting guidance\\ncomprehensive validation: added input validation throughout the type conversion pipeline\\n\\nmemory management & performance\\n\\nremoved deprecated methods: eliminated the deprecated reload() and done() methods from indicatorresults\\noptimized collections: streamlined c# list creation with better error handling\\nenhanced quote handling: improved ohlcv data handling with proper nullable value support\\n\\ncode quality improvements\\n\\nbetter type annotations: enhanced type hints throughout the codebase for improved ide support and documentation\\ncomprehensive documentation: added detailed parameter descriptions and error condition explanations\\nconsistent error messages: standardized error reporting with actionable troubleshooting information\\n\\nexample usage\\nthe improvements are transparent to end users - all existing code continues to work unchanged:\\n```python\\nfrom stock_indicators import indicators\\nfrom stock_indicators.indicators.common import quote\\nfrom datetime import datetime\\nenhanced quote creation with better null handling\\nquotes = [\\n    quote(datetime(2023, 1, 1), close=100.5),  # partial data now handled correctly\\n    quote(datetime(2023, 1, 2), open=101, high=102, low=100, close=101.5, volume=1000)\\n]\\nall indicator calculations work as before\\nresults = indicators.get_rsi(quotes, 14)\\n```\\nerror handling is now more informative:\\n```python\\nfrom stock_indicators import validationerror, typeconversionerror\\ntry:\\n    # better error messages guide users to solutions\\n    invalid_quote = quote(\"not a date\", close=100)\\nexcept typeerror as e:\\n    print(f\"clear error message: {e}\")\\n```\\ntesting\\n\\nall existing tests pass without modification, ensuring backward compatibility\\nadded comprehensive test suite validating the improvements\\nverified performance improvements in type conversion operations\\ntested edge cases and error conditions extensively\\n\\nbreaking changes\\nnone - this refactor maintains full backward compatibility while providing enhanced functionality and better error handling.\\n start  coding agent tips \\n\\n share your feedback on  coding agent for the chance to win a $200 gift card! click here to start the survey.\\n this is an auto-generated comment: release notes by coderabbit.ai \\nsummary by coderabbit\\n\\nnew features\\nintroduced custom exception classes for clearer error reporting and handling.\\n\\nenhanced error messages and logging throughout the library for improved troubleshooting.\\n\\n\\nbug fixes\\n\\nimproved type validation and error handling for date, decimal, and list conversions.\\n\\nenhanced input validation and error handling in indicator result and quote classes.\\n\\n\\nrefactor\\n\\nmodularized .net assembly initialization with better error handling and logging.\\n\\nupdated indicator function signatures to enforce keyword-only arguments for clarity and safety.\\n\\n\\nstyle\\n\\n\\nadded linter directive comments to suppress false warnings related to dynamic or interop members.\\n\\n\\nchores\\n\\nupdated github actions workflow to use a newer windows runner version for tests.\\n\\n end of auto-generated comment: release notes by coderabbit.ai']"
        ],
        [
         "18",
         "18",
         "11",
         "18_testprologcompiler_testerlangcompiler_goldenoutput_testtranspilergolden_testtranspile_printhello",
         "['testprologcompiler', 'testerlangcompiler_goldenoutput', 'testtranspilergolden', 'testtranspile_printhello', 'compiler', 'transpiler', 'compile', 'prolog', 'improve', 'compilersummary']",
         "['improve prolog transpilersummary\\n\\ntranspiler/pl: fold trivial zero-arg functions during compilation\\nadd bool_chain golden test output for prolog\\nupdate prolog readme checklist and progress log\\n\\ntesting\\n\\ngo test -tags slow ./transpiler/x/pl (fails: output mismatch)\\n\\n\\n <URL> ', 'improve prolog compiler map indexingsummary\\n\\nspecialize map indexing/field access using get_dict\\nprefer mutable lookups in lookupvar\\nregenerate prolog machine outputs for map_assign and map_nested_assign\\ndocument progress and update checklist\\n\\ntesting\\n\\ngo test ./compiler/x/pl -run testprologcompiler/map_assign -tags slow -count=1\\ngo test ./compiler/x/pl -run testprologcompiler/map_nested_assign -tags slow -count=1\\n\\n\\n <URL> ', 'improve prolog transpilersummary\\n\\ntranspiler/pl: treat const variables as literals to enable compile-time folding\\nadd golden test output for while_loop\\nregenerate readme checklist and tasks for prolog transpiler\\n\\ntesting\\n\\ngo test ./transpiler/x/pl -tags=slow -run testtranspile_printhello\\ngo vet -tags=slow ./transpiler/x/pl\\n\\n\\n <URL> ']"
        ],
        [
         "19",
         "19",
         "11",
         "19_sitemap_pagination_frontend_sitemaps",
         "['sitemap', 'pagination', 'frontend', 'sitemaps', 'paginate', 'jsdelivr', 'paginated', 'seo', 'backend', 'itemsperpage']",
         "['add yearly and monthly sitemap organization for large sitesthis pr implements a new sitemap feature that organizes content by publication year and month, similar to the approach used by metro.co.uk. this is particularly beneficial for large sites like news websites that host substantial amounts of historical content.\\nkey features\\nyearly/monthly sitemap organization:\\n- when enabled, the main sitemap.xml becomes an index that links to yearly or monthly sitemaps\\n- urls like sitemap.xml?yyyy=2024 show all posts from 2024\\n- urls like sitemap.xml?yyyy=2024&m=12 show posts from december 2024\\n- maintains existing query limits and honors all current settings\\nbenefits:\\n- optimized crawl budget - search engines can focus on newer content while still indexing older content efficiently\\n- faster generation - individual yearly/monthly sitemaps are smaller and generate faster\\n- better cache efficiency - each time period can be cached independently\\nimplementation details\\nsettings & ui:\\n- added \"use yearly sitemaps\" option (hidden when optimized sitemap is disabled)\\n- added \"use monthly sitemaps\" sub-option (hidden until yearly is enabled)\\n- proper javascript visibility toggles following existing patterns\\n- both options default to disabled for backward compatibility\\nbackend changes:\\n- enhanced registry::_init() to capture and sanitize $_get parameters\\n- modified base::generate_sitemap() to accept year/month filtering parameters\\n- added intelligent date filtering using wordpress date_query for efficient database queries\\n- implemented per-period caching with unique cache keys (e.g., base_2024_12)\\nurl structure:\\n```\\nmain sitemap (when yearly enabled) - shows sitemap index\\n/sitemap.xml\\nyearly sitemap - shows all posts from 2024\\n/sitemap.xml?yyyy=2024\\nmonthly sitemap - shows posts from december 2024\\n/sitemap.xml?yyyy=2024&m=12\\n```\\nparameter sanitization:\\n- years: 4-digit format (1900-2099 range)\\n- months: 1-2 digit format (1-12 range)\\n- invalid parameters are safely ignored\\ndeveloper features\\n\\nnew public method: \\\\the_seo_framework\\\\sitemap\\\\registry::get_sitemap_parameters() - gets sanitized year and month parameters\\nnew filter: the_seo_framework_sitemap_index_schemas - filter sitemap index xml schemas\\nbackward compatible - no changes to existing functionality when feature is disabled\\n\\ntesting\\nall modified files pass php syntax validation. the implementation includes comprehensive parameter sanitization with proper regex patterns and follows wordpress coding standards.\\nfixes #649.\\n start  coding agent tips \\n\\n you can make  smarter by setting up custom instructions, customizing its development environment and configuring model context protocol (mcp) servers. learn more  coding agent tips in the docs.', ' enhance blog experience: pagination, navigation & better discoverythis pr implements comprehensive blog improvements to transform the site from a single endless scroll of 90+ posts into a well-organized, performant, and discoverable content hub.\\n key improvements\\nperformance enhancements\\n\\npagination: implemented 12 posts per page with clean urls (/blog/page/1, /blog/page/2, etc.)\\nreduced initial load: blog homepage now shows structured sections instead of loading all 90+ posts\\nfaster page loads: measured 1.1s load time vs significantly slower with all posts loaded\\n\\nenhanced content discovery\\n restructured blog homepage (/blog)\\n\\nfeatured posts: prominent section highlighting important content using featured: true frontmatter\\nrecent posts: shows latest 8 articles with clean card layout\\nbrowse by topic: enhanced tag navigation with better visual hierarchy\\nbrowse by year: dynamic year-based navigation with post counts\\n\"view all posts\": link to paginated full archive\\n\\n new paginated archive (/blog/page/[number])\\n\\nclean paginated view showing 12 posts per page\\nmaintains search functionality within results\\nproper seo with meta tags and descriptions\\n\\n year-based archives (/blog/[year])\\n\\ndynamic year filtering (e.g., /blog/2024, /blog/2023)\\nseo-optimized with proper meta tags\\neasy historical content browsing\\n\\ntechnical implementation\\n\\nleveraged existing components: uses featuredsection, card, itemlist components\\nproper pagination: uses querycollection() with .limit() and .skip()\\nreading time utility: added utils/reading-time.ts for better metadata display\\nmaintained backward compatibility: all existing urls continue to work\\nmobile responsive: all new components work perfectly on mobile devices\\n\\n search & navigation\\n\\nenhanced search: works within paginated results\\ntag filtering: improved \"browse by topic\" section with clickable tags\\nmultiple discovery paths: users can find content by recency, topic, year, or search\\n\\n user experience\\nbefore: single endless scroll of 90+ blog posts\\nafter: structured homepage with multiple content discovery options and fast pagination\\n screenshots\\nnew blog homepage structure\\n\\npaginated blog archive\\n\\nsearch functionality\\n\\nthe blog experience is now significantly improved with better performance, multiple content discovery pathways, and enhanced user engagement while maintaining excellent seo.\\nfixes #492.\\n start  coding agent tips \\n\\n you can make  smarter by setting up custom instructions, customizing its development environment and configuring model context protocol (mcp) servers. learn more  coding agent tips in the docs.', \"optimize homepage post loading by eliminating n+1 queries for user dataproblem\\nthe homepage post loading was suffering from a classic n+1 query problem when fetching user data (post owners and contributors). for each post displayed on the homepage, the system was making individual database queries to fetch user information, resulting in:\\n\\n1 query to fetch posts\\nn queries to fetch post owners (one per post)  \\nm queries to fetch contributors (one per post with contributors)\\n\\nthis created a significant performance bottleneck, especially visible in the sql logs as repeated select users.* from users where users.id = $1 limit 2 queries.\\nsolution\\nimplemented batch fetching to reduce the query complexity from o(n) to o(1) for user data:\\n1. added batch user fetching capability\\njava\\n// new method in userservice\\nflux<user> getusersorghostbynames(set<string> usernames);\\nthis method uses a single query with in clause to fetch multiple users at once, falling back to ghost users for missing entries.\\n2. modified post processing to use batching\\njava\\n// in postpublicqueryserviceimpl.list()\\nreturn collectallusernames(posts)\\n    .flatmap(usernames -> contributorfinder.getuserservice().getusersorghostbynames(usernames))\\n    .collectmap(user -> user.getmetadata().getname())\\n    .flatmap(usercache -> processpostswithcache(posts, usercache));\\n3. cache-aware post conversion\\nadded overloaded methods that accept pre-fetched user data:\\njava\\nmono<listedpostvo> converttolistedvo(@nonnull post post, map<string, user> usercache);\\nperformance impact\\nbefore: 1 + n + m database queries\\nafter: 2 database queries (1 for posts, 1 for all users)\\nfor a homepage with 10 posts having 2 contributors each, this reduces queries from ~31 to 2 - a 93% reduction.\\nbackward compatibility\\n\\nall existing apis remain unchanged\\noriginal methods still work for individual post processing\\nnew batch methods are used internally for optimization\\nno breaking changes to external interfaces\\n\\ntesting\\n\\nall existing tests pass\\nadded validation for the new batch functionality\\nverified the optimization doesn't affect post display logic\\nmanual testing confirms user data is correctly populated\\n\\nfixes #7593\\n\\n[!warning]\\n\\nfirewall rules blocked me from connecting to one or more addresses\\ni tried to connect to the following addresses, but was blocked by firewall rules:\\n\\ns01.oss.sonatype.org\\ntriggering command: /usr/lib/jvm/temurin-17-jdk-amd64/bin/java --add-opens=java.base/java.lang=all-unnamed --add-opens=java.base/java.lang.invoke=all-unnamed --add-opens=java.base/java.util=all-unnamed --add-opens=java.prefs/java.util.prefs=all-unnamed --add-exports=jdk.compiler/com.sun.tools.javac.api=all-unnamed --add-exports=jdk.compiler/com.sun.tools.javac.util=all-unnamed --add-opens=java.base/java.util=all-unnamed --add-opens=java.prefs/java.util.prefs=all-unnamed --add-opens=java.base/java.nio.charset=all-unnamed --add-opens=java.base/java.net=all-unnamed --add-opens=java.base/java.util.concurrent.atomic=all-unnamed --add-opens=java.xml/javax.xml.namespace=all-unnamed -xx:maxmetaspacesize=384m -xx:&#43;heapdumponoutofmemoryerror -xms256m -xmx512m -dfile.encoding=utf-8 -duser.country -duser.language=en -duser.variant -cp /home/redacted/.gradle/wrapper/dists/gradle-8.14-bin/38aieal9i53h9rfe7vjup95b9/gradle-8.14/lib/gradle-daemon-main-8.14.jar -javaagent:/home/redacted/.gradle/wrapper/dists/gradle-8.14-bin/38aieal9i53h9rfe7vjup95b9/gradle-8.14/lib/agents/gradle-instrumentation-agent-8.14.jar org.gradle.launcher.daemon.bootstrap.gradledaemon 8.14 (dns block)\\n\\nif you need me to access, download, or install something from one of these locations, you can either:\\n\\nconfigure actions setup steps to set up my environment, which run before the firewall is enabled\\nadd the appropriate urls or hosts to the custom allowlist in this repository's  coding agent settings (admins only)\\n\\n\\n\\n start  coding agent tips \\n\\n you can make  smarter by setting up custom instructions, customizing its development environment and configuring model context protocol (mcp) servers. learn more  coding agent tips in the docs.\"]"
        ],
        [
         "20",
         "20",
         "11",
         "20_preview_docs_plotly_debounce_github",
         "['preview_docs', 'plotly', 'debounce', 'github', 'sync_playwright', 'github_workspace', 'pull_requests', 'javascript', 'netlify', 'python']",
         "[\"fix lag in pyplot example by implementing proper debouncingfix lag in pyplot example by implementing proper debouncing\\ndescription\\nthis pr fixes lag in the pyplot example in the documentation by properly implementing debouncing using reflex's built-in mechanism. the implementation:\\n\\nuses the .debounce() method on event handlers to add a 1.2 second debounce delay\\nremoves unnecessary custom debouncing code with asyncio and time tracking\\nsimplifies the event handlers for better readability\\n\\ntesting\\n\\ntested locally with the development server\\nverified that the debouncing works correctly with the slider and randomize button\\ntested with different numbers of points to ensure smooth performance\\nconfirmed no errors in the console\\n\\nlink to  run:  <URL> \\nrequested by: alek petuskey (alek@reflex.dev)\", 'docs: upgrade docusaurus from 3.7.0 to 3.8.1this pr upgrades docusaurus dependencies from version 3.7.0 to 3.8.1 (latest stable) to improve performance, security, and provide access to the latest features and bug fixes.\\nchanges made\\n dependencies updated\\n\\n@docusaurus/core: 3.7.0 → 3.8.1\\n@docusaurus/plugin-content-blog: 3.7.0 → 3.8.1  \\n@docusaurus/plugin-google-gtag: 3.7.0 → 3.8.1\\n@docusaurus/preset-classic: 3.7.0 → 3.8.1\\n@docusaurus/theme-common: 3.7.0 → 3.8.1\\n@docusaurus/faster: 3.7.0 → 3.8.1\\n@docusaurus/types: 3.7.0 → 3.8.1\\n\\n configuration changes\\nadded required future.v4.removelegacypostbuildheadattribute: true flag to docusaurus.config.js:\\njavascript\\nfuture: {\\n    experimental_faster: true,\\n    v4: {\\n        removelegacypostbuildheadattribute: true,\\n    },\\n},\\nthis flag is required when using the experimental_faster feature in docusaurus 3.8.x and prepares the site for docusaurus v4 compatibility.\\ntesting\\n development server - starts and runs successfully\\n production build - initiates without configuration errors\\n cli commands - all docusaurus commands functional\\n site functionality - no breaking changes to existing features  \\nmigration notes\\nthe upgrade from 3.7.0 to 3.8.1 was largely non-breaking. the only configuration change required was adding the v4 future flag. no changes were needed to:\\n- sidebar configuration\\n- plugin configurations\\n- custom components\\n- content structure\\n- build scripts\\nthis upgrade provides improved performance and bug fixes while maintaining full backward compatibility with the existing documentation site.\\nbreaking changes addressed\\n\\nfuture flag requirement: docusaurus 3.8.x requires the v4.removelegacypostbuildheadattribute flag when using experimental_faster\\nlockfile updates: updated pnpm-lock.yaml to reflect new dependency versions\\n\\nthe upgrade is ready for production deployment with no additional migration steps required.\\n start  coding agent tips \\n\\n share your feedback on  coding agent for the chance to win a $200 gift card! click here to start the survey.', 'rewrite dev/preview_docs.py in javascript to simplify preview-docs workflowthis pr rewrites the dev/preview_docs.py python script in javascript to simplify the preview-docs workflow and eliminate unnecessary dependencies.\\nproblem\\nthe current preview-docs.yml workflow required both node.js (for netlify deployment) and python (for pr comment management), leading to:\\n- slower ci runs due to python environment setup\\n- additional dependency management (pip install requests)\\n- workflow complexity with multiple language environments\\nsolution\\nreplaced the python script with a javascript implementation that uses the github-script action:\\nchanges made\\n\\ncreated .github/workflows/preview-docs.js - javascript version with identical functionality\\nupdated .github/workflows/preview-docs.yml - replaced python script calls with github-script actions  \\nremoved dev/preview_docs.py - original python script no longer needed\\n\\nkey benefits\\n faster ci runs - eliminates setup-python action\\n fewer dependencies - no more pip install requests\\n simpler workflow - uses built-in github api access\\n consistency - follows existing javascript patterns in the repository\\n 100% compatibility - maintains same comment templates and behavior  \\ntechnical details\\nthe javascript implementation preserves all original functionality:\\n- same <!-- documentation preview --> marker system\\n- handles both \"completed\" and \"failed\" stages\\n- creates/updates pr comments with identical templates\\n- uses the same github api endpoints and error handling\\nbefore/after comparison\\nbefore:\\nyaml\\n- uses: ./.github/actions/setup-python\\n- name: install dependencies\\n  run: pip install requests\\n- name: create preview link\\n  run: |\\n    python dev/preview_docs.py \\\\\\n      --commit-sha ${{ github.event.workflow_run.head_sha }} \\\\\\n      --pull-number $pr_number \\\\\\n      # ... other args\\nafter:\\nyaml\\n- name: create preview link\\n  uses: actions/github-script@60a0d83039c74a4aee543508d2ffcb1c3799cdea\\n  env:\\n    commit_sha: ${{ github.event.workflow_run.head_sha }}\\n    pull_number: ${{ github.event.workflow_run.pull_requests[0].number }}\\n    # ... other env vars\\n  with:\\n    script: |\\n      const script = require(`${process.env.github_workspace}/.github/workflows/preview-docs.js`);\\n      await script({ context, github, env: process.env });\\nthis addresses the todo comment in the original python script:\\n```python\\ntodo: rewrite this in javascript so we don\\'t have to setup both node (to deploy to netlify)\\nand python (to upsert pr comments with this script)\\n```\\nfixes #16550.\\n start  coding agent tips \\n\\n share your feedback on  coding agent for the chance to win a $200 gift card! click here to start the survey.']"
        ],
        [
         "21",
         "21",
         "10",
         "21_optimized_blas_kernels_fp16_openmp_opencl_onnx",
         "['optimized_blas_kernels_fp16', 'openmp', 'opencl', 'onnx', 'optimizations', 'cpp', 'ggml_interface', 'speedup', 'vectorization', 'cuda']",
         "[\"[wip] improve dft implementationdft implementation improvements - completed\\n all optimizations implemented and validated\\n1. core algorithm optimizations\\n\\n[x] next_power_of_2 optimization: 1.87x speedup using bit manipulation\\n[x] threading integration: 7 parallel execution paths added\\n[x] memory efficiency: optimized buffer management and cache usage\\n[x] numerical stability: maintained precision with type-safe conversions\\n\\n2. threading architecture\\n\\n[x] smart thresholding: automatic sequential vs parallel decision making\\n[x] cost-based execution: thread overhead avoided for small workloads  \\n[x] multi-algorithm support: both radix-2 fft and bluestein's enhanced\\n[x] thread safety: safe parallel access to shared data structures\\n\\n3. performance characteristics\\nsmall sizes (< 256 elements):\\n- sequential execution (no threading overhead)\\n- immediate benefit from bit manipulation optimizations\\nlarge sizes (> 256 elements):\\n- up to 4x theoretical speedup on multi-core systems\\n- validated threading thresholds prevent overhead\\n- scalable performance with available cpu cores\\n4. implementation quality\\n\\n[x] backward compatibility: zero api changes, existing code unaffected\\n[x] cross-platform: uses ort threadpool for portability\\n[x] robust fallbacks: graceful degradation without thread pool\\n[x] comprehensive testing: added large-size and edge-case validation\\n\\n5. deliverables\\ncode changes:\\n- onnxruntime/core/providers/cpu/signal/dft.cc (228 additions, 57 deletions)\\n- onnxruntime/test/providers/cpu/signal/signal_ops_test.cc (added performance tests)\\ndocumentation:\\n- docs/dft_performance_improvements.md (comprehensive implementation guide)\\n- tools/dft_benchmark.cpp (performance validation utility)\\nvalidation results:\\n-  1.87x speedup in next_power_of_2 microbenchmarks\\n-  4x theoretical parallel speedup for large transforms\\n-  syntax validation and bracket matching verified\\n-  threading integration complete with proper fallbacks\\nimpact summary\\nthis implementation addresses the dft performance bottlenecks mentioned in github issue #21164 by:\\n\\nimmediate performance gains through algorithmic optimizations\\nscalable multi-core utilization for compute-intensive transforms  \\nzero breaking changes - existing models automatically benefit\\nfoundation for future gpu optimizations - clean architecture for extension\\n\\nthe improvements maintain onnx runtime's commitment to performance while preserving stability and compatibility across all deployment scenarios.\\ntechnical excellence\\n\\nminimal code changes: surgical improvements focused on performance bottlenecks\\nprofessional implementation: follows ort threading patterns and conventions  \\ncomprehensive validation: multiple test scenarios and performance benchmarks\\nproduction-ready: robust error handling and edge case coverage\\n\\nfixes #24522.\\n start  coding agent tips \\n\\n share your feedback on  coding agent for the chance to win a $200 gift card! click here to start the survey.\", \"optimize onnx referenceevaluator resize performance with vectorized numpy implementationproblem\\nthe onnx referenceevaluator resize operator had severe performance issues due to element-by-element iteration in the _interpolate_nd function. for output shape (1, 384, 40, 40), the operation took approximately 24 minutes to complete, making it unusable for practical applications.\\nthe root cause was the inefficient loop at line 377:\\npython\\nfor x in _get_all_coords(ret):\\n    ret[tuple(x)] = _interpolate_nd_with_x(...)\\nthis approach iterates through every output coordinate individually (614,400 iterations for the problematic case) instead of leveraging numpy's vectorization capabilities.\\nsolution\\nimplemented a vectorized numpy-based interpolation engine that provides massive performance improvements while maintaining full backward compatibility:\\nkey features:\\n\\n~7,400x speedup for the problematic case (24 minutes → 0.2 seconds)\\n100% correctness preserved - outputs match original implementation exactly\\nintelligent fallback system - complex cases automatically use original implementation\\nzero breaking changes - existing code continues to work unchanged\\npure numpy implementation - no external dependencies added\\n\\nimplementation details:\\nnew functions added:\\n- _interpolate_nd_vectorized(): main entry point with smart linear interpolation detection\\n- _interpolate_nd_numpy_vectorized(): core vectorized interpolation engine\\n- _interpolate_2d_vectorized() & _interpolate_4d_vectorized(): optimized fast paths for common cases\\n- _interpolate_nd_original(): preserved original implementation for fallback\\nvectorization strategy:\\n- uses np.meshgrid() to generate coordinate grids efficiently\\n- applies coordinate transformations vectorially across all output points\\n- implements multilinear interpolation using numpy broadcasting\\n- handles 2d and 4d tensors with specialized optimized code paths\\nfallback logic:\\nthe optimization only applies to linear interpolation with simple coordinate transformations. complex cases automatically fall back to the original implementation:\\n- non-linear modes (nearest, cubic)\\n- roi-based resizing\\n- exclude_outside parameter\\n- complex coordinate transformation modes\\nperformance results:\\n| case | original time | optimized time | speedup |\\n|------|---------------|----------------|---------|\\n| (1, 16, 20, 20) → (1, 32, 40, 40) | ~5.1 seconds | 0.016 seconds | ~320x |\\n| (1, 384, 40, 40) | ~24 minutes | ~0.2 seconds | ~7,400x |\\nprocessing rate: 3+ million elements per second\\ntesting:\\n\\n correctness verified across multiple tensor dimensions\\n fallback behavior tested for all interpolation modes  \\n performance improvements confirmed in realistic scenarios\\n backward compatibility maintained\\n\\nthe optimization specifically targets the performance bottleneck while preserving all existing functionality and ensuring seamless integration.\\nfixes #6554.\\n\\n[!warning]\\n\\nfirewall rules blocked me from connecting to one or more addresses\\ni tried to connect to the following addresses, but was blocked by firewall rules:\\n\\nesm.ubuntu.com\\ntriggering command: /usr/lib/apt/methods/ <URL>  (dns block)\\n\\nif you need me to access, download, or install something from one of these locations, you can either:\\n\\nconfigure actions setup steps to set up my environment, which run before the firewall is enabled\\nadd the appropriate urls or hosts to my firewall allow list\\n\\n\\n\\n start  coding agent tips \\n\\n you can make  smarter by setting up custom instructions, customizing its development environment and configuring model context protocol (mcp) servers. learn more  coding agent tips in the docs.\", \"/inspect results of ggml_interface.cppthis pr is created by . @skykongkong8 needs to carefully review the commits.\\ndo not merge before @skykongkong8 's confirm.\\n@skykongkong8 please review this and update it. my prompt does not create code following the given style requirement, yet.\\nggml interface performance optimization summary\\ntarget file: nntrainer/tensor/cpu_backend/ggml_interface/ggml_interface.cpp\\nanalysis date: january 2025\\ntarget architectures: arm v9, x64 i5/i7 processors  \\n executive summary\\nthis document outlines critical performance optimizations applied to the ggml interface in nntrainer, focusing on three core areas that collectively provide 3-5x overall performance improvement across arm v9 and x64 processors.\\n performance impact overview\\n| optimization | arm v9 improvement | x64 i5/i7 improvement | memory impact |\\n|--------------|-------------------|----------------------|---------------|\\n| thread pool | 30-50% latency reduction | 35-45% latency reduction | no change |\\n| memory pool | 40-50% allocation overhead reduction | 45-55% allocation overhead reduction | 40-50% reduction |\\n| simd quantization | 200-400% quantization speedup | 300-500% quantization speedup | no change |\\n| combined effect | 3-4x overall improvement | 4-5x overall improvement | 40-50% memory reduction |\\n critical performance issues identified\\n1. thread pool implementation bottleneck\\n\\nissue: using openmp instead of available bs::thread_pool\\nimpact: 50-100μs overhead per gemm operation\\nroot cause: static thread allocation and poor work distribution\\nfrequency: every matrix operation (high frequency)\\n\\n2. memory allocation pattern inefficiency\\n\\nissue: frequent std::vector allocations in hot paths\\nimpact: 2-3x higher memory usage and allocation overhead\\nroot cause: no memory reuse strategy for quantization buffers\\nfrequency: every quantization operation (very high frequency)\\n\\n3. missing simd optimization\\n\\nissue: sequential quantization without vectorization\\nimpact: 3-5x slower than simd-optimized implementations\\nroot cause: no architecture-specific optimizations\\nfrequency: all quantization operations (critical path)\\n\\n implemented optimizations\\noptimization 1: advanced thread pool management\\nchanges made:\\n\\nreplaced all openmp #pragma directives with bs::thread_pool\\nimplemented adaptive thread count based on problem size\\nadded cache-line aligned work distribution\\nintroduced dynamic load balancing\\n\\ntechnical details:\\n```cpp\\n// before: fixed openmp threads\\npragma omp parallel for num_threads(4)\\n// after: adaptive bs thread pool\\nconst unsigned int n_threads = std::min(4u, std::max(1u, n / 64));\\nauto &bspool = threadpoolmanager::getinstance();\\nbs::multi_future multi_future = bspool.submit_loop(0, n, & {\\n    // optimized work with cache alignment\\n});\\n```\\nperformance gains:\\n\\narm v9: 30-50% latency reduction\\nx64: 35-45% latency reduction  \\nthread overhead: reduced from 50-100μs to <10μs per operation\\n\\noptimization 2: high-performance memory pool\\nchanges made:\\n\\nimplemented quantizationbufferpool singleton\\ncreated pooledbuffer raii wrapper\\nreplaced all std::vector with pooled allocations\\nadded cache-line alignment (64-byte boundaries)\\n\\ntechnical details:\\n```cpp\\n// before: frequent allocations\\nstd::vector qa = std::vector(qa_size);\\n// after: pooled memory management\\npooledbuffer qa(qa_size);  // automatic reuse and alignment\\n```\\nkey features:\\n\\ncache-line alignment: 64-byte boundaries for optimal cpu cache usage\\nconfigurable pool size: max 8 cached buffers per size class\\nthread-safe: mutex-protected buffer management\\nraii management: automatic return to pool on destruction\\n\\nperformance gains:\\n\\nmemory allocation overhead: 40-50% reduction\\nmemory fragmentation: significantly reduced\\ncache performance: improved due to alignment\\n\\noptimization 3: simd-accelerated quantization\\nchanges made:\\n\\ncreated ggml_simd_quant.h with runtime cpu detection\\nimplemented arm neon optimized quantization functions\\nimplemented x64 avx2 optimized quantization functions  \\nadded runtime dispatch with fallback support\\n\\ntechnical details:\\narm neon implementation:\\ncpp\\n// vectorized absolute maximum finding\\nfloat32x4_t max_vec = vdupq_n_f32(0.0f);\\nfor (int j = 0; j < qk_k; j += 16) {\\n    float32x4_t v0 = vld1q_f32(x + j);\\n    v0 = vabsq_f32(v0);\\n    max_vec = vmaxq_f32(max_vec, v0);\\n}\\nx64 avx2 implementation:\\ncpp\\n// 256-bit vector operations\\n__m256 max_vec = _mm256_setzero_ps();\\nfor (int j = 0; j < qk_k; j += 32) {\\n    __m256 v0 = _mm256_loadu_ps(x + j);\\n    v0 = _mm256_andnot_ps(sign_mask, v0);  // abs\\n    max_vec = _mm256_max_ps(max_vec, v0);\\n}\\nruntime dispatch:\\n```cpp\\ninline void quantize_row_q8_k_optimized(const float src, void dst, int64_t k) {\\n    const auto& features = cpufeatures::getinstance();\\nif (features.has_avx2) {\\n    quantize_row_q8_k_avx2(src, dst, k);\\n} else if (features.has_neon) {\\n    quantize_row_q8_k_neon(src, dst, k);\\n} else {\\n    ::quantize_row_q8_k(src, dst, k);  // fallback\\n}\\n\\n}\\n```\\nperformance gains:\\n\\narm neon: 200-400% quantization speedup\\nx64 avx2: 300-500% quantization speedup\\ncompatibility: full fallback support for unsupported architectures\\n\\n benchmarking results\\ngemv operations (m=1)\\n| architecture | before (ms) | after (ms) | improvement |\\n|--------------|-------------|------------|-------------|\\n| arm v9 (4096x4096) | 8.5 | 4.2 | 2.0x faster |\\n| x64 i5 (4096x4096) | 6.8 | 3.1 | 2.2x faster |\\n| x64 i7 (4096x4096) | 5.9 | 2.6 | 2.3x faster |\\ngemm operations (m>1)\\n| architecture | before (ms) | after (ms) | improvement |\\n|--------------|-------------|------------|-------------|\\n| arm v9 (1024x1024) | 45.2 | 11.8 | 3.8x faster |\\n| x64 i5 (1024x1024) | 38.6 | 8.2 | 4.7x faster |\\n| x64 i7 (1024x1024) | 32.1 | 6.9 | 4.7x faster |\\nmemory usage\\n| operation | before (mb) | after (mb) | reduction |\\n|-----------|-------------|------------|-----------|\\n| large model inference | 2.4 | 1.3 | 46% reduction |\\n| quantization buffers | 0.8 | 0.4 | 50% reduction |\\n code quality improvements\\nthread safety\\n\\nbefore: openmp threads with potential race conditions\\nafter: bs::thread_pool with proper synchronization and futures\\n\\nmemory management\\n\\nbefore: manual std::vector allocation/deallocation\\nafter: raii-based pooledbuffer with automatic lifecycle management\\n\\narchitecture support\\n\\nbefore: single scalar implementation\\nafter: multi-architecture with runtime detection and optimal dispatch\\n\\nmaintainability\\n\\nbefore: scattered openmp pragmas throughout code\\nafter: centralized thread pool management and clean simd abstractions\\n\\n implementation architecture\\nthread pool architecture\\nthreadpoolmanager (singleton)\\n bs::thread_pool instance\\n adaptive thread count calculation  \\n cache-line aligned work distribution\\n future-based synchronization\\nmemory pool architecture\\nquantizationbufferpool (singleton)\\n size-based buffer pools (unordered_map)\\n cache-line aligned allocations (64-byte)\\n thread-safe buffer management (mutex)\\n configurable pool limits (8 buffers/size)\\nsimd architecture\\nruntime cpu detection\\n arm neon support detection\\n x64 avx2 support detection\\n optimal function dispatch\\n fallback compatibility\\n technical deep dive\\ncache-line optimization\\n\\nalignment: all buffers aligned to 64-byte boundaries\\naccess pattern: sequential access optimized for cpu prefetchers\\nwork distribution: thread work blocks aligned to cache lines\\n\\nsimd instruction utilization\\n\\narm neon: uses 128-bit vectors (4x float32 or 8x float16)\\nx64 avx2: uses 256-bit vectors (8x float32)\\nthroughput: near-theoretical peak simd performance\\n\\nthread pool scalability\\n\\ndynamic adaptation: thread count scales with problem size\\nload balancing: work distributed to avoid thread starvation\\nmemory hierarchy: considers l1/l2/l3 cache sizes\\n\\n validation and testing\\ncorrectness verification\\n\\n all optimized functions produce identical results to reference implementation\\n floating-point precision maintained within acceptable tolerances\\n cross-platform compatibility verified\\n\\nperformance testing\\n\\n benchmarked on arm v9 (cortex-a78) processors\\n benchmarked on x64 i5-12600k and i7-12700k processors\\n tested across various matrix sizes (64x64 to 8192x8192)\\n\\nstress testing\\n\\n extended runs (24+ hours) without memory leaks\\n multi-threaded stress testing with concurrent operations\\n memory pool exhaustion and recovery testing\\n\\n recommendations for future optimization\\nshort-term (next release)\\n\\ngpu acceleration: implement opencl/cuda versions for large matrices\\nfp16 support: add half-precision floating-point simd optimizations\\nadvanced prefetching: implement software prefetching for better cache utilization\\n\\nmedium-term (6 months)\\n\\ncustom gemm kernels: develop highly optimized matrix multiplication kernels\\nmemory compression: implement lz4/snappy compression for stored quantized weights\\ndynamic profiling: add runtime performance monitoring and adaptive optimization\\n\\nlong-term (1 year)\\n\\nmachine learning optimization: use ml to predict optimal thread counts and work distribution\\nhardware-specific tuning: develop processor-specific optimization profiles\\ndistributed computing: enable multi-node gemm operations for very large matrices\\n\\n cost-benefit analysis\\ndevelopment investment\\n\\nimplementation time: 40 engineer-hours\\ntesting and validation: 20 engineer-hours\\ncode review and documentation: 10 engineer-hours\\ntotal investment: 70 engineer-hours\\n\\nperformance return\\n\\nuser experience: 3-5x faster neural network inference\\npower efficiency: 30-40% reduction in cpu utilization\\nmemory efficiency: 40-50% reduction in memory usage\\nscalability: better performance on high-core-count systems\\n\\nmaintenance overhead\\n\\nongoing: minimal (self-contained optimizations)\\ntesting: included in existing ci/cd pipeline\\ndocumentation: comprehensive inline documentation provided\\n\\n risk assessment and mitigation\\nidentified risks\\n\\nplatform compatibility: simd code may not work on all architectures\\nmitigation: comprehensive fallback implementations\\n\\ntesting: multi-architecture ci/cd validation\\n\\n\\nnumerical precision: simd operations may introduce floating-point differences\\n\\nmitigation: extensive precision testing and tolerance validation\\n\\nmonitoring: continuous integration checks for numerical stability\\n\\n\\nmemory pool fragmentation: pool may become fragmented with varied buffer sizes\\n\\nmitigation: size-based pools with configurable limits\\nmonitoring: pool utilization metrics and cleanup algorithms\\n\\nrisk probability and impact\\n| risk | probability | impact | mitigation effectiveness |\\n|------|-------------|---------|-------------------------|\\n| platform issues | low | medium | high (fallback code) |\\n| precision issues | very low | high | high (extensive testing) |\\n| memory fragmentation | low | low | medium (monitoring needed) |\\n success metrics\\nperformance kpis\\n\\n latency reduction: target 30-50% → achieved 30-50%\\n throughput increase: target 3-5x → achieved 3-5x \\n memory efficiency: target 40% reduction → achieved 40-50%\\n\\nquality kpis\\n\\n zero regressions: no functionality or accuracy loss\\n maintainability: clean, well-documented code structure\\n compatibility: works across all target platforms\\n\\nuser impact kpis\\n\\n inference speed: real-world model inference 3-5x faster\\n battery life: mobile devices see 30-40% battery improvement\\n scalability: better performance on multi-core systems\\n\\n conclusion\\nthe implemented optimizations successfully address the three critical performance bottlenecks in the ggml interface:\\n\\nthread management: eliminated openmp overhead with adaptive bs::thread_pool\\nmemory efficiency: implemented high-performance pooled allocation system  \\ncomputational performance: added architecture-specific simd optimizations\\n\\nthe 3-5x overall performance improvement makes neural network inference significantly more practical on both arm v9 and x64 processors, while maintaining full backward compatibility and code quality standards.\\nthese optimizations provide a solid foundation for future enhancements and position the ggml interface as a high-performance, production-ready component for neural network acceleration.\\n\\ndocument version: 1.0\\nlast updated: january 2025\\nauthor: performance optimization team\\nreview status:  approved for implementation\"]"
        ],
        [
         "22",
         "22",
         "14",
         "22_testdartcompiler_vmvalid_golden_testgocompiler_goldenoutput_compiler_compilersummary",
         "['testdartcompiler_vmvalid_golden', 'testgocompiler_goldenoutput', 'compiler', 'compilersummary', 'compile', 'refactor', 'codegen', 'testgocompiler_validprograms', 'testgocompiler_subsetprograms', 'cgo']",
         "['improve go compiler struct reusesummary\\n\\nenhance go compiler struct inference so existing structs are reused\\nadd structmatches helper\\nupdate checklists for tpch progress\\nregenerate machine output for cast_struct example\\n\\ntesting\\n\\ngo test ./compiler/x/go -tags slow -run testgocompiler_validprograms\\n\\n\\n <URL> ', 'optimize case builtins in go backendsummary\\n\\navoid runtime helpers for lower and upper when argument type is known\\nadd regression tests covering the new optimization\\n\\ntesting\\n\\ngo test ./compile/go -run testgocompiler_subsetprograms/upper_builtin -update -tags slow\\ngo test ./compile/go -run testgocompiler_subsetprograms/lower_builtin -update -tags slow\\ngo test ./compile/go -run testgocompiler_goldenoutput/upper_builtin -update -tags slow\\ngo test ./compile/go -run testgocompiler_goldenoutput/lower_builtin -update -tags slow\\ngo test ./...\\n\\n\\n <URL> ', 'n-api go html-to-markdownsummary\\n\\nreplace unstable koffi ffi with robust n-api implementation\\nadd hybrid fallback system: n-api → koffi → javascript\\nintegrate n-api build into docker and ci pipeline\\n\\nkey features\\n\\n memory safe: eliminates cgo mutex deadlocks and corruption\\n high performance: direct c++ interface, no ffi overhead\\n thread safe: built-in n-api thread safety mechanisms\\n zero config: automatic fallback if modules unavailable\\n compatible: drop-in replacement for existing parsemarkdown()\\n\\ntechnical details\\n\\ngo static library with timeout protection (30s)\\nc++ n-api wrapper with sync/async interfaces\\nmulti-stage docker build for automated compilation\\ncomprehensive test suite and validation scripts\\nsmart module loading with graceful degradation\\n\\nfiles added\\n\\nsharedlibs/go-html-to-md-napi/ - complete n-api module\\nvalidate-html-conversion.js - integration test suite\\nupdated dockerfile with n-api build stage\\nhybrid html-to-markdown.ts with intelligent fallback\\n\\nmigration path\\n\\nn-api module loads automatically if available\\nfalls back to existing koffi implementation\\nfinal fallback to javascript turndownservice\\nzero breaking changes to existing code\\n\\nthis resolves the koffi-related runtime panics and provides a stable,\\nhigh-performance html-to-markdown conversion system.\\n generated with  code\\nco-authored-by:  noreply@anthropic.com\\n this is an auto-generated description by cubic. \\n\\nsummary by cubic\\nreplaced the unstable koffi ffi html-to-markdown integration with a new n-api go module, adding a hybrid fallback system (n-api → koffi → javascript) and updating the docker and ci build to support the new module.\\n\\nnew features\\nadded a memory-safe, thread-safe n-api wrapper for the go html-to-markdown library with both sync and async interfaces.\\nautomatic fallback to koffi or javascript if the n-api module is unavailable.\\nintegrated n-api build and validation into docker and ci.\\nincluded a test suite and validation script to ensure conversion reliability.\\n\\n end of auto-generated description by cubic.']"
        ],
        [
         "23",
         "23",
         "11",
         "23_rustc_hash_rustc_hashing_lib_rs",
         "['rustc_hash', 'rustc', 'hashing', 'lib_rs', 'hash', 'fxhashset', 'rust', 'fxhashmap', 'fxhasher', 'hashset']",
         "['add database caching for folder scan results to improve performance / feature overview\\n\\nimplemented database caching functionality where the first folder scan creates a database, subsequent scans prioritize reading from the database, and any changes are written back to the database.\\n / key changes\\n database integration\\n\\nadded sqlite database support with rusqlite dependency\\ncreated comprehensive database module (src/database.rs) with crud operations\\nautomatic database schema creation and initialization\\ndatabase file automatically added to .gitignore\\n\\n smart scanning logic\\n\\nfirst scan: full filesystem scan + database creation\\nsubsequent scans: load from cache → check for changes → update database\\nperformance improvement: ~10-100x faster loading from cache\\nintelligent change detection and incremental updates\\n\\n enhanced scanner (src/scanner.rs)\\nrust\\n// new intelligent scanning flow:\\n1. check if database exists and has cached data\\n2. load cached data first (instant ui update)\\n3. perform filesystem scan in background\\n4. compare and detect changes\\n5. update database with changes only\\n6. clean up deleted folder records\\n ui improvements (src/tabs/clear_tab.rs)\\n\\ndatabase status indicator showing record count and last update\\nreal-time status messages: \"...\" → \"...\" → \"\"\\nvisual feedback for database operations\\n\\n / user experience\\n / first use\\n\\nnormal filesystem scan behavior\\ncreates appdata_cleaner.db automatically\\nno user action required\\n\\n / subsequent use\\n\\ninstant loading of cached folder data\\nbackground verification for changes\\nseamless user experience with improved performance\\n\\n / technical details\\ndatabase schema\\nsql\\ncreate table folder_scans (\\n    id integer primary key autoincrement,\\n    folder_type text not null,          -- roaming/local/locallow\\n    folder_name text not null,\\n    folder_size integer not null,\\n    last_modified text not null,\\n    created_at text not null,\\n    updated_at text not null,\\n    unique(folder_type, folder_name)\\n);\\nperformance optimizations\\n\\nbatch database operations for better performance\\nindexed queries for fast lookups\\nminimal memory footprint\\nautomatic cleanup of stale records\\n\\n / testing\\n\\n comprehensive unit tests for database operations\\n integration tests for scanner functionality\\n backward compatibility verified\\n error handling and fallback mechanisms tested\\n\\n / compatibility\\n\\nbackward compatible: works seamlessly without existing database\\nerror resilient: falls back to filesystem scan if database issues occur\\nno breaking changes: existing functionality unchanged\\ncross-platform: sqlite bundled for all platforms\\n\\nfiles changed\\n\\ncargo.toml - added rusqlite dependency\\nsrc/database.rs - new database module (complete implementation)\\nsrc/scanner.rs - enhanced with caching logic\\nsrc/tabs/clear_tab.rs - ui improvements and status handling\\nsrc/main.rs - added database module import\\n.gitignore - added database files to ignore list\\n\\nresolves the issue by implementing efficient database caching that dramatically improves scan performance while maintaining full compatibility with existing functionality.\\n start  coding agent tips \\n\\n you can make  smarter by setting up custom instructions, customizing its development environment and configuring model context protocol (mcp) servers. learn more  coding agent tips in the docs.', 'refactor unicharset script storage to use hash map instead of raw arraysummary\\nthis pr refactors the script storage mechanism in the unicharset class to replace manual memory management with modern c++ stl containers, improving performance, memory safety, and maintainability.\\nproblem\\nthe original implementation used a raw char** array with manual memory management:\\n- char **script_table - array of c-style strings\\n- int script_table_size_used - current number of scripts\\n- int script_table_size_reserved - allocated capacity\\n- manual new/delete[] operations with potential memory leaks\\n- o(n) linear search in get_script_id_from_name() \\n- complex manual array resizing logic\\nsolution\\nreplaced raw arrays with stl containers:\\n- std::unordered_map<std::string, int> script_name_to_id_ - for o(1) name→id lookup\\n- std::vector<std::string> script_names_ - for o(1) id→name reverse lookup\\nkey improvements:\\n- performance: script lookup is now o(1) hash map lookup instead of o(n) linear search\\n- memory safety: automatic memory management eliminates potential leaks and double-free errors\\n- exception safety: stl containers provide strong exception safety guarantees\\n- maintainability: cleaner, simpler code using standard data structures\\n- api compatibility: all public methods maintain identical signatures and behavior\\nchanges made\\nheader file (src/ccutil/unicharset.h):\\n\\nadded includes for <unordered_map> and <vector>\\nreplaced manual script storage variables with stl containers\\nupdated inline method implementations for get_script_table_size() and get_script_from_script_id()\\nsimplified clear() method to use container methods\\nupdated comments to reflect o(1) performance characteristics\\n\\nsource file (src/ccutil/unicharset.cpp):\\n\\nsimplified constructor (removed manual script table initialization)\\nrewrote add_script() to use hash map for uniqueness and vector for storage\\nrewrote get_script_id_from_name() to use hash map lookup\\nupdated post_load_setup() to work with vector size\\n\\ntesting\\ncomprehensive testing was performed to ensure:\\n-  all existing unicharset functionality works unchanged\\n-  script uniqueness is preserved\\n-  forward and reverse lookups work correctly\\n-  performance scales well with 45+ scripts\\n-  edge cases (invalid ids, non-existent scripts) handled properly\\n-  memory cleanup works correctly with clear()\\n-  full library builds and links successfully\\nbackward compatibility\\nthis is a pure refactoring with no breaking changes:\\n- all public method signatures remain identical\\n- all method behaviors remain the same\\n- script id assignment order is preserved\\n- existing code continues to work without modification\\nthe change is completely internal to the unicharset implementation and invisible to users of the class.\\n\\n[!warning]\\n\\nfirewall rules blocked me from connecting to one or more addresses\\ni tried to connect to the following addresses, but was blocked by firewall rules:\\n\\nesm.ubuntu.com\\ntriggering command: /usr/lib/apt/methods/ <URL>  (dns block)\\n\\nif you need me to access, download, or install something from one of these locations, you can either:\\n\\nconfigure actions setup steps to set up my environment, which run before the firewall is enabled\\nadd the appropriate urls or hosts to my firewall allow list\\n\\n\\n\\n start  coding agent tips \\n\\n you can make  smarter by setting up custom instructions, customizing its development environment and configuring model context protocol (mcp) servers. learn more  coding agent tips in the docs.', 'feat: rewrite sitemap xml parsing from javascript to rust (eng-2904)feat: rewrite sitemap xml parsing from javascript to rust (eng-2904)\\nsummary\\nthis pr replaces the xml parsing logic in getlinksfromsitemap from javascript (using xml2js) to rust (using roxmltree), while keeping network requests and recursive processing in javascript as requested. the change aims to improve performance of sitemap parsing operations.\\nkey changes:\\n- added roxmltree dependency for xml parsing in rust\\n- implemented parse_sitemap_xml rust function with ffi wrapper\\n- created parsesitemapxml wrapper function following existing filterlinks pattern\\n- updated sitemap processing to use rust function instead of xml2js\\n- maintains exact compatibility with existing data structures (urlset/sitemapindex format)\\nreview & testing checklist for human\\n\\n[x] end-to-end sitemap crawling verification: test both sitemap index files and regular sitemaps to ensure no functional regressions\\n[x] data structure compatibility: verify that the rust function returns identical json structure to xml2js.parsestringpromise() \\n[x] error handling: test with malformed xml inputs to ensure proper error propagation from rust to javascript\\n[x] build process: verify rust compilation works in ci environment (requires nightly toolchain for edition 2024)\\n[x] performance validation: compare sitemap processing performance before/after to confirm expected improvements\\n\\nrecommended test plan:\\n1. test crawling a site with sitemap index (nested sitemaps)\\n2. test crawling a site with regular sitemap (direct url list)\\n3. test error scenarios (malformed xml, network timeouts)\\n4. verify memory usage and performance under load\\n\\ndiagram\\n```mermaid\\n%%{ init : { \"theme\" : \"default\" }}%%\\ngraph tb\\n    subgraph \"apps/api\"\\n        sitemap[\"apps/api/src/scraper/webscraper/sitemap.ts\"]:::major-edit\\n        crawler_ts[\"apps/api/src/lib/crawler.ts\"]:::major-edit\\n    end\\nsubgraph \"rust crawler\"\\n    cargo[\"apps/api/sharedlibs/crawler/<br/>cargo.toml\"]:::minor-edit\\n    lib_rs[\"apps/api/sharedlibs/crawler/<br/>src/lib.rs\"]:::major-edit\\nend\\n\\nsubgraph \"dependencies\"\\n    xml2js[\"xml2js<br/>(removed)\"]:::context\\n    roxmltree[\"roxmltree<br/>(added)\"]:::context\\nend\\n\\nsitemap -->|\"calls parsesitemapxml()\"| crawler_ts\\ncrawler_ts -->|\"ffi call\"| lib_rs\\nlib_rs -->|\"uses\"| roxmltree\\nsitemap -.->|\"previously used\"| xml2js\\ncargo -->|\"defines\"| roxmltree\\n\\nsubgraph legend\\n    l1[major edit]:::major-edit\\n    l2[minor edit]:::minor-edit  \\n    l3[context/no edit]:::context\\nend\\n\\nclassdef major-edit fill:#90ee90\\nclassdef minor-edit fill:#87ceeb\\nclassdef context fill:#ffffff\\n\\n```\\nnotes\\n\\ncritical: this change requires rust nightly toolchain due to edition 2024 usage\\nmemory safety: ffi implementation follows existing filter_links pattern for proper memory management\\nno tests added: per user request, no new tests were created - relies on existing test suite and manual verification\\nbackward compatibility: maintains exact same function signature and return format as original implementation\\n\\nlink to  run:  <URL> \\nrequested by: mogery@sideguide.dev\\n this is an auto-generated description by cubic. \\n\\nsummary by cubic\\nrewrote sitemap xml parsing from javascript to rust to improve performance, while keeping the output format and api unchanged.\\n\\ndependencies\\nreplaced the xml2js javascript library with the roxmltree rust crate for xml parsing.\\n\\n end of auto-generated description by cubic.']"
        ],
        [
         "24",
         "24",
         "10",
         "24__getbusytimesfromteamlimits_refactors_bookingrepository_checkbookinglimits",
         "['_getbusytimesfromteamlimits', 'refactors', 'bookingrepository', 'checkbookinglimits', 'versionoptimize', 'checkbookinglimit', 'batch', 'getallacceptedteambookingsofusers', 'getallacceptedteambookingsofuser', 'queries']",
         "['perf: optimize team bookings query by fetching data for multiple users at onceoptimize team bookings query and busy times limits\\nthis pr optimizes the team bookings query and busy times limits by fetching data for multiple users at once, rather than making separate database calls for each user.\\nchanges\\n\\nadded a new getallacceptedteambookingsofusers function in bookingrepository that accepts multiple users\\ncreated a new getbusytimesfromteamlimitsforusers function in util.ts that processes team booking limits for multiple users\\nadded a new getbusytimesfromlimitsforusers function in util.ts that processes booking and duration limits for multiple users\\nmoved the condition checks from getuseravailability.ts to util.ts\\nupdated the getuseravailabilityinitialdata type to include teambookinglimits, teamforbookinglimits, busytimesfromlimits, and eventtypeforlimits properties\\nmodified the _getuseravailability function to use the batch-loaded data from initialdata when available\\n\\nbenefits\\n\\nreduces the number of database queries by fetching team bookings and busy times once for multiple users\\nimproves performance by avoiding redundant database calls\\nmaintains the same functionality while optimizing query execution\\nparticularly beneficial for team and collective scheduling types with many members\\n\\ntesting\\n\\nverified that all type checks pass with yarn type-check:ci\\n\\nlink to  run:  <URL> \\nrequested by: keith@cal.com', \"perf: optimize team bookings query by using batch versionoptimize team bookings query by using batch version\\nwhat's being changed and why\\nthis pr addresses a database performance issue by updating two locations in the web app code that were still using the single-user version of bookingrepository.getallacceptedteambookingsofuser instead of the batch version bookingrepository.getallacceptedteambookingsofusers that was introduced in pr #21137.\\nthe problematic sql query was causing database performance issues when checking team booking limits. by using the batch version of the repository function, we can reduce the number of database queries and improve performance.\\nlocations updated:\\n\\npackages/lib/intervallimits/server/getbusytimesfromlimits.ts - updated _getbusytimesfromteamlimits function to use the batch version\\npackages/lib/intervallimits/server/checkbookinglimits.ts - updated checkbookinglimit function to use the batch version\\n\\ntesting\\n\\ntype checking passes with yarn type-check:ci\\nthe changes maintain the same functionality while improving database performance\\n\\nlink to  run:  <URL> \\nrequested by: keith@cal.com\\n this is an auto-generated description by mrge. \\n\\nsummary by mrge\\noptimized team bookings queries by switching to the batch version of the repository function, reducing database load and improving performance. now, team booking limits for multiple users are checked in a single query instead of one per user.\\n end of auto-generated description by mrge.\", \"perf: optimize team bookings query by using batch versionoptimize team bookings query by using batch version\\nwhat's being changed and why\\nthis pr addresses a database performance issue by updating two locations in the web app code that were still using the single-user version of bookingrepository.getallacceptedteambookingsofuser instead of the batch version bookingrepository.getallacceptedteambookingsofusers that was introduced in pr #21137.\\nthe problematic sql query was causing database performance issues when checking team booking limits. by using the batch version of the repository function, we can reduce the number of database queries and improve performance.\\nlocations updated:\\n\\npackages/lib/intervallimits/server/getbusytimesfromlimits.ts - updated _getbusytimesfromteamlimits function to use the batch version\\npackages/lib/intervallimits/server/checkbookinglimits.ts - updated checkbookinglimit function to use the batch version\\n\\ntesting\\n\\ntype checking passes with yarn type-check:ci\\nthe changes maintain the same functionality while improving database performance\\n\\nlink to  run:  <URL> \\nrequested by: keith@cal.com\\n this is an auto-generated description by mrge. \\n\\nsummary by mrge\\noptimized team bookings queries by switching to the batch version, reducing database load and improving performance.\\n\\nrefactors\\nreplaced single-user team bookings queries with batch queries in booking limits and busy times logic.\\n\\n end of auto-generated description by mrge.\"]"
        ],
        [
         "25",
         "25",
         "10",
         "25_git_repo_commit_build_source_files",
         "['git', 'repo', 'commit', 'build_source_files', 'dev', 'release', 'changessee', 'rsyslog', 'releaseref', 'relocs']",
         "['optimize source file tree buildingsummary\\n\\navoid quadratic path lookups in wf_project.build_source_files\\n\\ntesting\\n\\npip install -e . (fails: could not find a version that satisfies the requirement poetry-core)\\n\\n this is an auto-generated comment: release notes by coderabbit.ai \\nsummary by coderabbit\\n\\nbug fixes\\nimproved reliability when handling file paths that contain duplicate directory names, ensuring correct directory checks in all cases.\\n\\n\\n\\n end of auto-generated comment: release notes by coderabbit.ai', 'perf: optimize shared package rebuilds for dev speed b069ellipsis_hidden \\n\\n[!important]\\noptimizes development build speed by removing unnecessary dependencies in turbo.json and updates contributing.md with a note on retrying initial setup command.\\n\\nperformance optimization:\\nremoved @langfuse/shared#build dependency from dev, dev:worker, and dev:web tasks in turbo.json to optimize rebuild speed.\\n\\n\\ndocumentation:\\nupdated contributing.md to note that the initial run of pnpm run dx may fail and should be retried.\\n\\n\\n\\nthis description was created by  for 33551ba272d0264eb1f2cdb7f01aa18e69959373. you can customize this summary. it will automatically update as commits are pushed.\\n\\n ellipsis_hidden', 'ci: skip expensive workflows on doc-only changessee also  <URL> \\n\\nlegal gdpr notice:\\naccording to the european data protection laws (gdpr), we would like to make you\\naware that contributing to rsyslog via git will permanently store the\\nname and email address you provide as well as the actual commit and the\\ntime and date you made it inside git\\'s version history. this is inevitable,\\nbecause it is a main feature git. if you are concerned about your\\nprivacy, we strongly recommend to use\\n\\n--author \"anonymous <gdpr@example.com>\"\\n\\ntogether with your commit. also please do not sign your commit in this case,\\nas that potentially could lead back to you. please note that if you use your\\nreal identity, the gdpr grants you the right to have this information removed\\nlater. however, we have valid reasons why we cannot remove that information\\nlater on. the reasons are:\\n\\n* this would break git history and make future merges unworkable\\n* the rsyslog projects has legitimate interest to keep a permanent record of the\\n  contributor identity, once given, for\\n  - copyright verification\\n  - being able to provide proof should a malicious commit be made\\n\\nplease also note that your commit is public and as such will potentially be\\nprocessed by many third-parties. git\\'s distributed nature makes it impossible\\nto track where exactly your commit, and thus your personal data, will be stored\\nand be processed. if you would not like to accept this risk, please do either\\ncommit anonymously or refrain from contributing to the rsyslog project.']"
        ],
        [
         "26",
         "26",
         "9",
         "26_calendarcache_cachedcalendarservice_getcachedcalendar_calendarservice",
         "['calendarcache', 'cachedcalendarservice', 'getcachedcalendar', 'calendarservice', 'googlecalendar', 'calendar', 'caching', 'cache', 'calendars', 'selectedcalendars']",
         "['feat: optimize calendar cache retrieval with cachedcalendarserviceoptimize calendar cache retrieval with cachedcalendarservice\\nthis pr optimizes calendarcache retrieval by:\\n\\ncreated a new cachedcalendarservice that implements the calendar interface\\nmodified handlenewbooking to fetch all selectedcalendars at once before processing individual users\\nimplemented an in-memory store to hold the calendar cache data\\nensured proper fallback to original calendarservice for cache misses\\n\\nthis reduces database calls and avoids redundant calls to google calendar api during booking processes.\\nchanges\\n\\ncreated cachedcalendarservice in packages/app-store/googlecalendar/lib/cachedcalendarservice.ts\\nimplements the calendar interface\\nonly serves cache hits, falls back to original service for misses\\n\\ndelegates most methods to the original calendar service\\n\\n\\ncreated in-memory cache store in packages/features/calendar-cache/calendar-cache-store.ts\\n\\nstores calendar availability data during a booking process\\n\\ncleared at the start of each new booking\\n\\n\\nadded utility function getcachedcalendar in packages/app-store/_utils/getcachedcalendar.ts\\n\\nreturns cachedcalendarservice for google calendar credentials\\n\\nfalls back to regular calendar service for other types\\n\\n\\nmodified handlenewbooking.ts to:\\n\\nclear the cache at the beginning of a new booking\\n\\nfetch all selected calendars at once before the user loop\\n\\n\\nupdated getcalendarsevents.ts to use cached calendar when appropriate\\n\\n\\ntesting\\ntested by creating a booking with multiple users and verified reduced database calls and no redundant calls to google calendar api.\\nlink to  run:  <URL> \\nrequested by: zomars@cal.com\\n this is an auto-generated description by mrge. \\n\\nsummary by mrge\\nadded cachedcalendarservice to cache calendar availability during bookings, reducing database and google calendar api calls.\\n\\nnew features\\nintroduced an in-memory calendar cache store.\\ncreated cachedcalendarservice to serve cached data and fall back to the original service on cache misses.\\nupdated booking flow to fetch all selected calendars at once and clear the cache at the start of each booking.\\nmodified calendar event retrieval to use the cache when possible.\\n\\n end of auto-generated description by mrge.', \"fix: resolve type errors in calendar cache implementationoptimize calendar cache retrieval with cachedcalendarservice\\nthis pr optimizes calendarcache retrieval by:\\n\\ncreated a new cachedcalendarservice that implements the calendar interface\\nmodified handlenewbooking to fetch all selectedcalendars at once before processing individual users\\nimplemented an in-memory store to hold the calendar cache data\\nadded logic to determine which users have 100% cache hits before selecting the calendar service\\n\\nthis reduces database calls and avoids redundant calls to google calendar api during booking processes.\\nchanges\\n\\ncreated cachedcalendarservice in packages/app-store/googlecalendar/lib/cachedcalendarservice.ts\\nimplements the calendar interface\\nonly serves cache hits, does not use google calendarservice internally\\n\\nthrows errors for write operations as it's read-only\\n\\n\\ncreated in-memory cache store in packages/features/calendar-cache/calendar-cache-store.ts\\n\\nstores calendar availability data during a booking process\\nadded method to determine which users have 100% cache hits\\n\\ncleared at the start of each new booking\\n\\n\\nenhanced utility function getcachedcalendar in packages/app-store/_utils/getcachedcalendar.ts\\n\\nchecks for 100% cache hits before deciding which service to use\\nonly returns cachedcalendarservice for users with complete cache hits\\n\\nfalls back to regular calendar service for other cases\\n\\n\\nmodified handlenewbooking.ts to:\\n\\nclear the cache at the beginning of a new booking\\n\\nfetch all selected calendars at once before the user loop\\n\\n\\nupdated getcalendarsevents.ts to:\\n\\npass necessary parameters to getcachedcalendar for cache hit determination\\nuse the appropriate calendar service based on cache availability\\n\\ntesting\\ntested by creating a booking with multiple users and verified reduced database calls and no redundant calls to google calendar api.\\nlink to  run:  <URL> \\nrequested by: zomars@cal.com\\n this is an auto-generated description by mrge. \\n\\nsummary by mrge\\nfixed type errors in the calendar cache implementation to ensure type safety and prevent runtime issues.\\n end of auto-generated description by mrge.\", 'feat: implement isr for booking pages with google calendar webhook integrationimplement next.js isr for individual booking pages with google calendar webhook integration\\nsummary\\nthis pr implements next.js incremental static regeneration (isr) for individual booking pages (/[user]/[type]) with a 1-hour ttl caching strategy and automatic revalidation triggered by google calendar webhook events.\\nchanges made\\n1. isr implementation for booking pages\\n\\nfile: apps/web/app/(booking-page-wrapper)/[user]/[type]/page.tsx\\nadded unstable_cache with 1-hour (3600 seconds) revalidation\\nfixed app router compatibility by passing individual parameters instead of legacy context object\\nuses cache tags [\"booking-page\"] for targeted invalidation\\n\\n2. server actions for revalidation\\n\\nfile: apps/web/app/(booking-page-wrapper)/[user]/[type]/actions.ts\\ncreated revalidatebookingpage() for specific user/type combinations\\ncreated revalidateuserbookingpages() for all booking pages of a user\\nuses revalidatepath() and revalidatetag() for cache invalidation\\n\\n3. google calendar webhook integration\\n\\nfile: packages/app-store/googlecalendar/api/webhook.ts\\nadded isr revalidation logic triggered by calendar change events\\nimplemented dynamic user identification via userrepository.findbyid()\\nadded comprehensive error handling and logging\\n\\n4. fallback task queue system\\n\\nfile: packages/features/tasker/tasks/revalidate-booking-pages.ts\\ncreated new task handler for isr revalidation as fallback mechanism\\nfile: packages/features/tasker/tasker.ts - added task type definition\\nfile: packages/features/tasker/tasks/index.ts - registered new task handler\\nprovides resilience if direct webhook revalidation fails\\n\\ntechnical implementation details\\nisr caching strategy\\ntypescript\\nconst getcachedbookingdata = unstable_cache(\\n  async (headers, cookies, params, searchparams) => {\\n    const legacyctx = buildlegacyctx(headers, cookies, params, searchparams);\\n    return await getdata(legacyctx);\\n  },\\n  [\"booking-page-data\"],\\n  { \\n    revalidate: 3600, // 1 hour ttl\\n    tags: [\"booking-page\"]\\n  }\\n);\\nwebhook revalidation flow\\n\\ngoogle calendar webhook receives change notification\\nidentifies affected user via credential.userid\\nfetches user profile to get username\\ntriggers isr revalidation for user\\'s booking pages\\nfalls back to task queue if direct revalidation fails\\n\\nerror handling\\n\\ncomprehensive try-catch blocks around revalidation logic\\nfallback to task queue system if direct revalidation fails\\ndetailed logging for debugging and monitoring\\n\\ntesting status\\n local testing limitation: full end-to-end testing was limited due to a database schema issue in the development environment. the error \"the column membership.customroleid does not exist in the current database\" prevented booking pages from loading locally.\\ncompleted testing\\n\\n typescript compilation passes (yarn type-check:ci)\\n pre-commit hooks (prettier, eslint) pass\\n code follows existing patterns and conventions\\n\\ntesting instructions for reviewers\\n\\nisr functionality:\\naccess booking pages like /free/30min or /pro/15min\\nverify pages load quickly (pre-rendered)\\n\\ncheck browser dev tools for cache headers\\n\\n\\nwebhook integration:\\n\\ntrigger google calendar changes for users with cal.com integration\\nverify booking pages update within reasonable time\\n\\ncheck logs for revalidation events\\n\\n\\nfallback mechanism:\\n\\nsimulate webhook revalidation failures\\nverify task queue picks up revalidation jobs\\ncheck task execution logs\\n\\nperformance benefits\\n\\nfaster page loads: pre-rendered pages serve immediately from cache\\nreduced server load: database queries cached for 1 hour\\nautomatic updates: pages stay fresh via webhook-triggered revalidation\\nresilient system: fallback task queue ensures reliability\\n\\nbackwards compatibility\\n\\n no breaking changes to existing booking functionality\\n maintains all existing api contracts\\n preserves metadata generation and internationalization\\n compatible with existing authentication and authorization\\n\\ndatabase requirements\\nthis implementation requires the database schema to be up-to-date. if encountering the customroleid column error, run:\\nbash\\nyarn workspace @calcom/prisma db-migrate\\nlink to  run\\n <URL> \\nrequested by\\nkeith@cal.com (keith williams)\\n this is an auto-generated description by cubic. \\n\\nsummary by cubic\\nadded 1-hour isr caching to individual booking pages and integrated google calendar webhooks to trigger automatic page revalidation. includes a fallback task queue to ensure updates if direct revalidation fails.\\n\\nnew features\\nbooking pages (/[user]/[type]) now use isr with a 1-hour cache.\\ngoogle calendar webhook triggers revalidation when calendar events change.\\nfallback task queue handles revalidation if webhook fails.\\n\\n end of auto-generated description by cubic.']"
        ],
        [
         "27",
         "27",
         "8",
         "27_optimizations_kernels_git_optimization",
         "['optimizations', 'kernels', 'git', 'optimization', 'optimize', 'add', 'benchmark', 'addition', 'matrix', '']",
         "['add benchmark for deserializing large added vocab +  optimizations', 'alpaka kernels for matrix multiplication and addition', '/identify additional shallow store optimizations d0c6']"
        ],
        [
         "28",
         "28",
         "9",
         "28_rpc_metamask_metamaskwallet_endpoints",
         "['rpc', 'metamask', 'metamaskwallet', 'endpoints', 'setlocalrpchealthcache', 'metadata', 'staking', 'precompile', 'lints', 'block_number']",
         "['docs: update trusted hash guide[x] analyzed repository structure and understood the issue\\n[x] confirmed build system works correctly\\n[x] identified that quick-start.md already uses the new command format\\n[x] update celestia-node-trusted-hash.md to use the new single-call command instead of two separate curl calls\\n[x] test that the markdown lints correctly after changes\\n[x] verify build still works after changes\\n\\nthe issue requests updating the trusted hash guide to use a more efficient single api call command instead of making two separate curl requests. this reduces network calls and improves user experience.\\ncurrent state: the file /how-to-guides/celestia-node-trusted-hash.md still uses the old two-call approach on lines 50-51, while /how-to-guides/quick-start.md already uses the new efficient single-call format.\\nfixes #2033.\\n\\n you can make  smarter by setting up custom instructions, customizing its development environment and configuring model context protocol (mcp) servers. learn more  coding agent tips in the docs.', \"docs: add iavl v1 migration guide for consensus node operatorsthis pr adds comprehensive documentation for consensus node operators on how to migrate from iavl v0 to iavl v1 to achieve optimal performance benefits with celestia-app v4.\\nbackground\\nwith celestia-app v4, the state machine uses iavl v1.x which provides roughly 10x performance improvement over v0.x through data locality optimizations. however, the entire database needs to be migrated to the v1 key layout. while this happens lazily by default, it can cause increased i/o load and make state access a performance bottleneck during the migration period.\\nwhat's added\\nthe new migration guide (docs/maintainers/iavl-v1-migration.md) provides:\\nmigration options\\n\\nlazy migration (default): automatic migration during normal operation\\nstate sync migration (recommended): full migration via state sync for immediate optimal performance\\n\\ncomprehensive instructions\\n\\nstep-by-step procedures for both migration approaches\\nspecific commands and configuration examples\\nprerequisites and preparation steps\\nbackup and recovery procedures\\n\\nperformance optimization\\n\\niavl configuration options in app.toml with tuning recommendations\\nperformance monitoring guidance to measure improvements\\nkey performance indicators to track\\n\\noperational support\\n\\ntroubleshooting common migration issues\\nbest practices for safe migration\\nrecovery procedures if rollback is needed\\nlinks to additional resources and support channels\\n\\nkey benefits\\nthis documentation addresses the concern that lazy migration might cause increased i/o load and prevent maximizing throughput. by providing clear guidance on the state sync migration approach, consensus node operators can immediately access the full iavl v1 performance benefits without experiencing a degradation period.\\nthe guide is specifically written for consensus node operators and includes validator-specific considerations like proper backup procedures, downtime planning, and configuration optimization.\\nfixes #4839.\\n\\n you can make  smarter by setting up custom instructions, customizing its development environment and configuring model context protocol (mcp) servers. learn more  coding agent tips in the docs.\", 'add listmetadatakeys and getallmetadata rpc methods for improved metadata discoveryproblem\\nthe existing getmetadata rpc query requires users to know the specific metadata keys beforehand, which are scattered throughout the codebase. this makes it difficult for users to discover what metadata is available or to query all node metadata efficiently.\\nsolution\\nthis pr implements two new rpc methods while maintaining backward compatibility:\\n1. listmetadatakeys() - metadata discovery\\nreturns all available metadata keys with human-readable descriptions:\\ngo\\nkeys, err := client.listmetadatakeys(ctx)\\nfor _, key := range keys {\\n    fmt.printf(\"key: %s - %s\\\\n\", key.key, key.description)\\n}\\n2. getallmetadata() - omni query\\nreturns all node metadata in a single call, reducing round trips:\\ngo\\nmetadata, err := client.getallmetadata(ctx)\\nfor _, entry := range metadata {\\n    fmt.printf(\"%s: %v\\\\n\", entry.key, entry.value)\\n}\\nkey features\\n\\ndiscoverable: users can explore available metadata without prior knowledge\\nefficient: single call to get all metadata reduces network overhead  \\nrobust: gracefully handles missing or unavailable metadata keys\\nbackward compatible: existing getmetadata method unchanged\\nwell-documented: comprehensive api documentation and examples\\n\\nrest endpoints\\nadded convenient http endpoints for web integrations:\\n- get /api/v1/metadata/keys - lists metadata keys with descriptions\\n- get /api/v1/metadata - returns available keys and rpc method info\\nimplementation details\\n\\ncentralized metadata key constants in types package for consistency\\nupdated all references throughout the codebase to use centralized constants\\ngenerated new protobuf definitions with proper message types\\ncomprehensive test coverage for rpc methods, http endpoints, and error cases\\n\\ncurrent metadata keys\\n| key | description |\\n|-----|-------------|\\n| d | da included height - height of the data availability layer that has been included |\\n| l | last batch data - last batch data submitted to the data availability layer |\\n| last-submitted-header-height | last submitted header height to da |\\n| last-submitted-data-height | last submitted data height to da |\\nfixes #2386.\\n\\n[!warning]\\n\\nfirewall rules blocked me from connecting to one or more addresses\\ni tried to connect to the following addresses, but was blocked by firewall rules:\\n\\nbuf.build\\ntriggering command: /usr/local/bin/buf generate --path=./proto/rollkit --template=buf.gen.yaml --config=buf.yaml (dns block)\\nesm.ubuntu.com\\ntriggering command: /usr/lib/apt/methods/ <URL>  (dns block)\\n\\nif you need me to access, download, or install something from one of these locations, you can either:\\n\\nconfigure actions setup steps to set up my environment, which run before the firewall is enabled\\nadd the appropriate urls or hosts to my firewall allow list\\n\\n\\n\\n start  coding agent tips \\n\\n share your feedback on  coding agent for the chance to win a $200 gift card! click here to start the survey.']"
        ],
        [
         "29",
         "29",
         "7",
         "29_scrape_tool_batch_scrape_tool_seleniumscrapingtool_crawl_tool",
         "['scrape_tool', 'batch_scrape_tool', 'seleniumscrapingtool', 'crawl_tool', 'batch_scrape', 'scrapeoptions', 'server', 'firecrawl', 'webdriver', 'scrape']",
         "['implement retry-after header handling for improved throttling in fetch requestscurrently, genaiscript handles throttling situations but does not respect the retry-after header returned by services. this leads to unnecessary load on throttled services and suboptimal user experience with exponential backoff delays that may be longer than needed.\\nchanges made\\nthis pr implements proper retry-after header handling in the fetch retry logic:\\n1. added parseretryafter() function\\n\\nsupports both integer seconds format: retry-after: 120 \\nsupports http date format: retry-after: fri, 31 dec 1999 23:59:59 gmt\\nhandles edge cases: negative values, invalid inputs, past dates, whitespace\\nreturns null for invalid inputs to fallback to exponential backoff\\n\\n2. enhanced retry logic in createfetch()\\n\\nchecks for retry-after header in http responses during retries\\nuses server-specified delay instead of calculated exponential backoff when available\\nfalls back to existing exponential backoff when no retry-after header is present\\nrespects maxdelay configuration to prevent excessive waits\\n\\n3. improved user transparency\\n\\ndisplays retry-after information in log messages\\nformat: retry #1 in 120.0s (retry-after: 120s) vs retry #1 in 2.0s\\nusers can now see the actual reason and duration for delays\\n\\nexample behavior\\nbefore:\\nretry #1 in 2.0s 429 too many requests\\nretry #2 in 3.0s 429 too many requests  \\nretry #3 in 4.5s 429 too many requests\\nafter (with retry-after: 60):\\nretry #1 in 60.0s (retry-after: 60s) 429 too many requests\\nbenefits\\n\\n reduces unnecessary load on throttled services by respecting server timing\\n provides transparency to users about wait times and reasons\\n maintains full backward compatibility with existing retry behavior\\n handles common throttling scenarios (http 429, 503, etc.)\\n rfc 7231 compliant retry-after header parsing\\n\\ntesting\\nadded comprehensive unit tests covering:\\n- integer seconds parsing (\"120\" → 120 seconds)\\n- http date parsing (future dates with proper time calculation)\\n- edge cases (negative values, invalid strings, past dates)\\n- whitespace handling and input validation\\nfixes #1629.\\n\\n[!warning]\\n\\nfirewall rules blocked me from connecting to one or more addresses\\ni tried to connect to the following addresses, but was blocked by firewall rules:\\n\\ncdn.sheetjs.com\\ntriggering command: node /usr/local/bin/yarn install (dns block)\\n\\nif you need me to access, download, or install something from one of these locations, you can either:\\n\\nconfigure actions setup steps to set up my environment, which run before the firewall is enabled\\nadd the appropriate urls or hosts to my firewall allow list\\n\\n\\n\\n start  coding agent tips \\n\\n you can make  smarter by setting up custom instructions, customizing its development environment and configuring model context protocol (mcp) servers. learn more  coding agent tips in the docs.', 'implement maxage fast scraping parameterimplement maxage fast scraping parameter\\nsummary\\nthis pr implements the maxage parameter for the firecrawl-mcp-server to enable faster scraping through caching, addressing github issue #69. the implementation exposes the existing firecrawl api maxage parameter through the mcp server\\'s tool schema.\\nkey changes:\\n- added maxage parameter to scrape_tool inputschema as optional number field\\n- updated tool description and usage examples to highlight caching benefits\\n- added test coverage to verify parameter is passed through to firecrawl api\\n- merged latest main branch changes (version bump to 1.11.0)\\nthe maxage parameter allows users to specify a cache duration in milliseconds. when set, the system will use cached content if available and younger than the specified age, otherwise scrape fresh content.\\nreview & testing checklist for human\\n\\n[ ] test maxage with real firecrawl api calls - verify that setting maxage actually enables caching behavior (most critical)\\n[ ] validate performance claims - test whether maxage actually provides significant speed improvements as claimed\\n[ ] test edge cases - try invalid maxage values (negative, non-numeric) to ensure proper error handling\\n[ ] verify backwards compatibility - ensure existing scrape calls without maxage parameter continue working\\n\\nrecommended test plan: create a test script that scrapes the same url twice with maxage set, verify the second call is faster and returns cached content.\\n\\ndiagram\\n```mermaid\\n%%{ init : { \"theme\" : \"default\" }}%%\\ngraph td\\n    subgraph \"mcp server implementation\"\\n        indexts[\"src/index.ts\"]:::major-edit\\n        indextestts[\"src/index.test.ts\"]:::major-edit\\n    end\\nsubgraph \"tool schema\"\\n    scrape_tool[\"scrape_tool definition\"]:::major-edit\\n    inputschema[\"inputschema.properties\"]:::major-edit\\nend\\n\\nsubgraph \"external dependencies\"\\n    firecrawlsdk[\"@mendable/firecrawl-js\"]:::context\\n    scrapeparams[\"scrapeparams type\"]:::context\\nend\\n\\nindexts --> scrape_tool\\nscrape_tool --> inputschema\\ninputschema --> |\"maxage: number\"| firecrawlsdk\\nindextestts --> |\"tests maxage passing\"| firecrawlsdk\\nfirecrawlsdk --> scrapeparams\\n\\nsubgraph legend\\n    l1[\"major edit\"]:::major-edit\\n    l2[\"minor edit\"]:::minor-edit  \\n    l3[\"context/no edit\"]:::context\\nend\\n\\nclassdef major-edit fill:#90ee90\\nclassdef minor-edit fill:#87ceeb\\nclassdef context fill:#ffffff\\n\\n```\\nnotes\\n\\nimplementation relies on existing firecrawl sdk scrapeparams type to handle maxage validation\\nthe parameter is optional and should default to 0 (always scrape fresh) per firecrawl api behavior\\nperformance improvement claims (500% faster) are based on issue description but not independently verified\\nsession url:  <URL> \\nrequested by: @nickscamara', 'implement maxage fast scraping parameterimplement maxage fast scraping parameter\\nsummary\\nthis pr implements the maxage fast scraping parameter across all scraping-related tools in the firecrawl mcp server, enabling 500% faster scraping through intelligent caching as documented in pr #34 of firecrawl-docs.\\nkey changes:\\n- added maxage parameter (number, defaults to 0) to scrape_tool, crawl_tool, and search_tool schemas\\n- created missing batch_scrape_tool that was referenced in tests but absent from main code\\n- added proper type guard and request handler for batch scraping functionality\\n- updated all tool schemas to include maxage with proper descriptions and defaults\\nthe maxage parameter accepts milliseconds and uses cached content if younger than the specified age, otherwise scrapes fresh content. a value of 0 (default) means always scrape fresh.\\nreview & testing checklist for human\\n\\n[ ] test actual caching behavior: verify maxage parameter works with real firecrawl api calls (make same request twice with maxage > 0, confirm second request uses cache)\\n[ ] test new batch_scrape_tool: verify the previously missing batch scrape functionality now works end-to-end  \\n[ ] verify backward compatibility: test all existing tools still work without maxage specified\\n[ ] test parameter passing: confirm maxage gets properly passed to underlying firecrawl client methods\\n[ ] integration testing: run the mcp server with a real mcp client and test all modified tools\\n\\nrecommended test plan:\\n1. start mcp server locally\\n2. test each tool (scrape, crawl, batch_scrape, search) with and without maxage\\n3. for caching verification: scrape same url twice with maxage=300000 (5min), verify second call is faster\\n4. verify error handling when maxage is invalid (negative, non-number)\\n\\ndiagram\\n```mermaid\\n%%{ init : { \"theme\" : \"default\" }}%%\\ngraph td\\n    subgraph \"mcp server structure\"\\n        index[\"src/index.ts\"]:::major-edit\\n        tests[\"src/index.test.ts\"]:::context\\n    end\\nsubgraph \"tool definitions (updated)\"\\n    scrape[\"scrape_tool<br/>+maxage param\"]:::major-edit\\n    crawl[\"crawl_tool<br/>+maxage in scrapeoptions\"]:::major-edit  \\n    search[\"search_tool<br/>+maxage in scrapeoptions\"]:::minor-edit\\n    batch[\"batch_scrape_tool<br/>**new tool**\"]:::major-edit\\nend\\n\\nsubgraph \"api handlers (updated)\"  \\n    handler[\"calltoolrequestschema<br/>+batch_scrape case\"]:::major-edit\\n    typeguards[\"type guards<br/>+isbatchscrapeoptions\"]:::minor-edit\\nend\\n\\nsubgraph \"firecrawl client calls\"\\n    scrapecall[\"client.scrapeurl()\"]:::context\\n    crawlcall[\"client.asynccrawlurl()\"]:::context  \\n    batchcall[\"client.asyncbatchscrapeurls()\"]:::context\\n    searchcall[\"client.search()\"]:::context\\nend\\n\\nindex --> scrape\\nindex --> crawl  \\nindex --> search\\nindex --> batch\\nindex --> handler\\nindex --> typeguards\\n\\n\\nhandler --> scrapecall\\nhandler --> crawlcall\\nhandler --> batchcall  \\nhandler --> searchcall\\n\\ntests -.-> batch\\n\\nsubgraph legend\\n    l1[\"major edit\"]:::major-edit\\n    l2[\"minor edit\"]:::minor-edit  \\n    l3[\"context/no edit\"]:::context\\nend\\n\\nclassdef major-edit fill:#90ee90\\nclassdef minor-edit fill:#87ceeb  \\nclassdef context fill:#ffffff\\n\\n```\\nnotes\\n\\ncritical discovery: the batch_scrape_tool was completely missing from the main code despite being referenced in tests - this was a significant gap that needed to be filled\\ntypescript issue: had to remove origin: \\'mcp-server\\' parameter from batch scrape call due to type compatibility issues\\ntesting limitation: while all lint/test/build checks pass, the actual caching behavior with real firecrawl api calls couldn\\'t be verified in the development environment\\ndocumentation alignment: implementation follows the fast-scraping documentation from firecrawl-docs pr #34\\n\\nsession info: \\n- requested by: @nickscamara\\n-  session:  <URL> \\n- fixes: mendableai/firecrawl-mcp-server#69']"
        ],
        [
         "30",
         "30",
         "8",
         "30_testracketcompiler_testccompiler_vmvalid_golden_compiler_testccompiler_goldenoutput",
         "['testracketcompiler', 'testccompiler_vmvalid_golden', 'compiler', 'testccompiler_goldenoutput', 'compiling', 'testccompiler_validprograms', 'compile', 'backendsummary', 'backend', 'sum_builtin']",
         "['update c backend to use stack arrayssummary\\n\\nadd helper for contains_array_int and generate it when a list expression is backed by a stack array\\nemit contains_array_int instead of contains_list_int when possible\\nuse listlenexpr and listitemexpr for query loops\\nregenerate c machine outputs\\n\\ntesting\\n\\ngo test ./compiler/x/c -run testccompiler_validprograms -tags slow -count=1\\n\\n\\n <URL> ', 'update c backend aggregatessummary\\n\\nimprove avg, sum, min and max when the length of the input list is known\\nregenerate machine translations for affected examples\\n\\ntesting\\n\\ngo test ./compiler/x/c -run testccompiler_validprograms/avg_builtin -tags slow\\ngo test ./compiler/x/c -run testccompiler_validprograms/sum_builtin -tags slow\\ngo test ./compiler/x/c -run testccompiler_validprograms/min_max_builtin -tags slow\\ngo test ./compiler/x/c -run testccompiler_validprograms/break_continue -tags slow\\n\\n\\n <URL> ', 'add constant string list inference to c backendsummary\\n\\nimprove c compiler by tracking string list constants\\nprecompute len/count/min/max for constant string lists\\nregenerate golden outputs\\nupdate compilation status and task log\\n\\ntesting\\n\\ngo test -tags slow ./compiler/x/c -run testccompiler_vmvalid_golden -update (fails: group_by_multi_join_sort, save_jsonl_stdout, tree_sum, update_stmt)\\n\\n\\n <URL> ']"
        ],
        [
         "31",
         "31",
         "7",
         "31_git_build_options_disable_grpc_modules_build",
         "['git', 'build_options', 'disable_grpc_modules', 'build', 'size_reduction_summary', 'buildbuild', 'test_binary', 'gopsutil', 'tags', 'packages']",
         "['build tag to exclude cloud archiver providerswhat changed?\\nadded a build tag to exclude cloud archiver providers. the filesystem one is unaffected (as it is needed for tests).\\nwhy?\\nreduce binary size by 24mb for when cloud archiver providers are not needed.\\nbefore (without disable_grpc_modules)\\n-rwxr-xr-x@ 1 stephan  staff   119m jun 30 16:35 temporal-server\\nafter\\n-rwxr-xr-x@ 1 stephan  staff    95m jun 30 16:23 temporal-server', '[poc] build tag for lightweight test buildbuild tag that removes the 3 dependencies from the test binary:\\n\\ngithub.com/aws/aws-sdk-go\\ncloud.google.com/go\\ngithub.com/jackc/pgx\\n\\nbefore: ~13.6s build time\\n```\\nhyperfine --warmup 1 \"rm -f test_binary && go test -tags test_dep -count=1 -c -o test_binary ./tests\"    \\nbenchmark 1: rm -f test_binary && go test -tags test_dep -count=1 -c -o test_binary ./tests\\n  time (mean ± σ):     13.616 s ±  0.215 s    [user: 13.757 s, system: 3.726 s]\\n  range (min … max):   13.269 s … 14.046 s    10 runs\\n```\\nafter: ~9.3s build time\\n```\\nhyperfine --warmup 1 \"rm -f test_binary && go test -tags test_dep,lite -count=1 -c -o test_binary ./tests\"\\nbenchmark 1: rm -f test_binary && go test -tags test_dep,lite -count=1 -c -o test_binary ./tests\\n  time (mean ± σ):      9.314 s ±  0.138 s    [user: 9.803 s, system: 3.450 s]\\n  range (min … max):    9.038 s …  9.529 s    10 runs\\n```\\n=> 32% speedup.', 'build: add optional build tags to reduce binary sizethis pr implements conditional compilation using go build tags to make optional features truly optional, allowing users to choose between a smaller binary and full functionality.\\nresults\\n| build configuration | binary size | reduction |\\n|---------------------|-------------|-----------|\\n| original (with go-git) | 16.7 mib | - |\\n| current full build | 8.1m | 51.5% |\\n| new lite build | 7.4m | 55.7% |\\ntotal reduction: 9.3 mib (44% smaller binary)\\nchanges made\\nbuild tags implementation\\n\\nfuzzy tag: controls fuzzy search and path indexing functionality\\ndependencies: goleveldb, sahilm/fuzzy (~500kb)\\n\\nfeatures: --fuzzy flag, path indexing commands\\n\\n\\nmounts tag: controls mount point detection  \\n\\ndependencies: gopsutil (~200kb)\\nfeatures: --mounts flag for mount details\\n\\nbuild options\\nlite build (default) - recommended for most users:\\nbash\\ngo build -ldflags=\"-s -w\" .\\n- size: 7.4m\\n- all core functionality including git status, display options, sorting, filtering\\nfull build - for power users:\\nbash\\ngo build -ldflags=\"-s -w\" -tags=\"fuzzy mounts\" .\\n- size: 8.1m\\n- all features including fuzzy search and mount detection\\ncustom builds:\\n```bash\\nonly fuzzy search\\ngo build -tags=\"fuzzy\" .\\nonly mount details\\ngo build -tags=\"mounts\" .\\n```\\nbackwards compatibility\\n\\ndefault build provides 95% of functionality most users need\\noptional features degrade gracefully when disabled (no errors)\\nall command-line flags remain functional\\ncore git integration via cli preserved in all builds\\n\\ndocumentation\\n\\nadded docs/build_options.md with detailed build instructions\\nadded docs/size_reduction_summary.md with results summary\\nupdated justfile with new build targets (build-lite, build-full, etc.)\\n\\nthis approach allows users to choose between binary size and optional functionality based on their needs, while maintaining full backwards compatibility.\\nfixes #237.\\n start  coding agent tips \\n\\n share your feedback on  coding agent for the chance to win a $200 gift card! click here to start the survey.']"
        ],
        [
         "32",
         "32",
         "9",
         "32_terrainpatch_meshes_rendering_patch",
         "['terrainpatch', 'meshes', 'rendering', 'patch', 'patchsize', 'patches', 'texture', 'terrainpatchsize', 'fsharp', 'umaoptimizedmeshcombinerexample']",
         "['optimize billboard rendering for 3d particles with specialized batched render paththis pr implements an optimized billboard rendering system specifically designed for 3d particle systems, addressing the need for efficient batched billboard rendering with blend and flip support.\\nchanges made\\n1. new render message type\\nadded renderstaticbillboards type that supports batched rendering of multiple billboards with shared material properties but individual:\\n- color and emission values per billboard\\n- blend modes (transparent, additive, overwrite) \\n- flip options (flipnone, fliph, flipv, fliphv)\\n- transform matrices\\n2. fast path api\\nimplemented world.renderstaticbillboardfast method that provides a high-performance rendering path by:\\n- bypassing normal message allocation overhead\\n- batching billboards with the same material\\n- supporting individual per-billboard properties\\nfsharp\\nworld.renderstaticbillboardfast(\\n    &modelmatrix, castshadow, presence, insetopt, orientup, planar,\\n    &materialproperties, &material, &color, &emission, \\n    blend, flip, shadowoffset, depthtest, rendertype, renderpass, world)\\n3. blend mode support\\nadded infrastructure for blend modes in 3d billboard rendering:\\n- transparent: standard alpha blending for particles\\n- additive: bright glowing effects for sparks/fire\\n- overwrite: solid billboard rendering\\n4. flip support\\nfull texture flipping support for billboard variety:\\n- individual flip control per billboard\\n- supports horizontal, vertical, and combined flipping\\nperformance benefits\\n\\nbatched rendering: multiple billboards sharing materials are rendered efficiently\\nfast path: eliminates message allocation overhead for high-frequency particle updates\\nparticle optimized: designed specifically for particle system use cases with hundreds/thousands of billboards\\n\\nexample usage\\n```fsharp\\n// fire particle with additive blending and emission\\nlet modelmatrix = matrix4x4.createtranslation(particleposition)\\nlet color = color(1.0f, 0.5f, 0.0f, alpha) // orange with fade\\nlet emission = color(1.0f, 0.3f, 0.0f, 1.0f) // glowing orange\\nworld.renderstaticbillboardfast(\\n    &modelmatrix, false, presence.present, valuenone, true, false,\\n    &materialprops, &material, &color, &emission, \\n    additive, flipnone, 0.0f, lessthanorequaltest,\\n    forwardrendertype(0.0f, 0.0f), normalpass, world)\\n```\\nthis implementation provides the foundation for high-performance 3d particle systems while maintaining compatibility with existing billboard rendering infrastructure.\\nfixes #700.\\n start  coding agent tips \\n\\n share your feedback on  coding agent for the chance to win a $200 gift card! click here to start the survey.', 'implement terrain patches for improved gpu performancethis pr implements terrain patches to address gpu performance issues when rendering large terrains. the solution automatically subdivides large terrains into smaller, cullable chunks without breaking existing apis.\\nproblem\\ncurrently, rendering large terrains consumes excessive gpu resources because the entire terrain is rendered as a single mesh. this prevents efficient frustum culling and makes proximity debug drawing for height maps impractical due to performance concerns.\\nsolution\\nthe implementation adds an automatic terrain patching system that:\\n\\nautomatically subdivides large terrains (>64x64 vertices) into 64x64 vertex patches\\nmaintains full backwards compatibility - existing terrain code works unchanged\\nenables per-patch frustum culling for better gpu utilization\\nuses configurable patch size via constants.render.terrainpatchsize\\n\\nkey changes\\ncore infrastructure\\n\\nadded terrainpatchsize constant (64 vertices) in constants.fs\\nadded terrainpatch type to represent terrain subdivisions\\nextended terraingeometrydescriptor and terraindescriptor with optional patch information\\n\\nsmart patch system\\n\\nterrainpatch.calculateterrainpatches function handles subdivision logic\\nedge patches correctly handle non-even divisions (e.g., 200x200 terrain creates 4x4 patches with 8-vertex edge patches)\\nsmall terrains (<64x64) continue using single-mesh rendering\\n\\nrendering pipeline updates\\n\\nmodified categorizeterrain to detect when patches should be used\\nupdated terrain geometry creation to handle both single-mesh and patch-based rendering\\neach patch is treated as independent geometry with its own bounds for culling\\n\\ntesting\\ncreated validation scripts that confirm:\\n- 200x200 terrain correctly generates 16 patches (4x4 grid)\\n- edge patches handle remainder vertices properly (8x64, 64x8, 8x8 for edges/corners)\\n- small terrains (32x32) use single patch as expected\\n- all patch bounds calculations are accurate\\nperformance benefits\\n\\nreduced gpu load: only visible terrain patches are rendered\\nbetter memory utilization: patches enable more efficient gpu processing\\nenables proximity debug drawing: height map debugging can now be used without performance concerns\\nautomatic optimization: no code changes needed - patches activate automatically for large terrains\\n\\nexample\\nfsharp\\n// before: 512x512 terrain renders as single 262,144 vertex mesh\\n// after: same terrain automatically splits into 64 patches of 64x64 vertices each\\n//        only visible patches are rendered based on frustum culling\\nthis change directly addresses the core issue while maintaining complete api compatibility. developers can now enable proximity debug drawing for height maps without performance penalties.\\nfixes #747.\\n start  coding agent tips \\n\\n you can make  smarter by setting up custom instructions, customizing its development environment and configuring model context protocol (mcp) servers. learn more  coding agent tips in the docs.', 'implement terrain patches to reduce gpu load for large terrainsthis pr implements a terrain patch system that splits large terrains into smaller, manageable chunks to significantly reduce gpu performance issues when rendering large terrains.\\nproblem\\ncurrently, rendering large terrains generates massive vertex/index arrays that overwhelm the gpu, causing performance bottlenecks. the entire terrain was rendered as a single geometry object regardless of size.\\nsolution\\nthe patch system divides terrains into configurable sub-sections (patches) that are rendered individually:\\ncore changes\\n\\nnew terrainpatch type - represents a subdivision of terrain with its own bounds and resolution\\nextended terrain descriptors - added patchsize property to terraingeometrydescriptor and terraindescriptor\\npatch-based geometry generation - new trycreatephysicallybasedterrainpatchgeometry function creates geometry for individual patches\\nupdated rendering pipeline - modified terrain categorization, shadow rendering, and main rendering to handle patch arrays\\n\\nterrainfacet integration\\n\\nadded patchsize property to terrainfacet with default value of 64×64 height map samples\\nseamless integration with existing terrain creation workflow\\nbackward compatibility maintained\\n\\nmaterial system support\\nthe patch system fully supports the existing terrain material pipeline:\\n- normal map extraction for patch regions\\n- tint image processing per patch\\n- blend weight computation\\n- proper vertex format matching\\nperformance benefits\\n\\nreduced gpu load - smaller geometry chunks instead of massive single meshes\\nfuture optimization ready - enables per-patch frustum culling and lod\\nconfigurable granularity - adjustable patch sizes for different use cases\\nfallback support - automatically uses single patch for small terrains\\n\\nexample usage\\n```fsharp\\n// default patch size (64x64 samples)\\nterrain \"myterrain\" [\\n    terrainmaterial <| blendmaterial { / material config / }\\n    heightmap <| rawheightmap { resolution = v2i 1025 1025; / config / }\\n]\\n// custom patch size for very large terrains\\nterrain \"largeterrain\" [\\n    terrainmaterial <| blendmaterial { / material config / }\\n    heightmap <| rawheightmap { resolution = v2i 2049 2049; / config / }\\n    patchsize <| v2i 128 128  // larger patches for huge terrains\\n]\\n```\\nthis addresses the performance concern mentioned in the issue about enabling proximity debug drawing for height maps without killing performance.\\nfixes #747.\\n\\n[!warning]\\n\\nfirewall rules blocked me from connecting to one or more addresses\\ni tried to connect to the following addresses, but was blocked by firewall rules:\\n\\nesm.ubuntu.com\\ntriggering command: /usr/lib/apt/methods/ <URL>  (dns block)\\n\\nif you need me to access, download, or install something from one of these locations, you can either:\\n\\nconfigure actions setup steps to set up my environment, which run before the firewall is enabled\\nadd the appropriate urls or hosts to my firewall allow list\\n\\n\\n\\n start  coding agent tips \\n\\n share your feedback on  coding agent for the chance to win a $200 gift card! click here to start the survey.']"
        ],
        [
         "33",
         "33",
         "8",
         "33_maven_azure_build_macos_build_windows",
         "['maven', 'azure', 'build_macos', 'build_windows', 'dockerfiles', 'build_linux', 'build_documentation', 'detectbuildtype', 'provisioning', 'pip']",
         "['[azure pipelines] implement  build detection with x64-linux first executionproblem\\n-triggered workflows were overloading the azure pipelines pool by running all triplets in parallel, causing resource contention and slower feedback for both  and regular users.\\nsolution\\nthis pr implements automatic  build detection and conditional execution logic:\\n  detection\\ndetects  builds via multiple indicators:\\n- build.requestedforemail contains \"\" or \"github.com\"\\n- build.sourceversionmessage contains \"\" \\n- build.requestedfor contains \"\"\\n execution strategy\\nfor  builds:\\n1. run x64-linux first\\n2. only run other triplets if x64-linux succeeds\\n3. cancel remaining jobs if x64-linux fails\\nfor regular builds:\\n- unchanged behavior - all triplets run in parallel\\n implementation\\nrestructured the pipeline into 3 stages:\\nyaml\\nstages:\\n- detectbuildtype    # determines if build is -triggered\\n- buildx64linux      # always runs x64-linux first  \\n- buildallothers     # conditionally runs other triplets\\nthe key logic uses azure pipelines stage conditions:\\nyaml\\ncondition: or(\\n  eq(dependencies.detectbuildtype.outputs[\\'detect.detect.isbuild\\'], \\'false\\'),\\n  succeeded(\\'buildx64linux\\')\\n)\\n benefits\\n\\nresource efficiency: prevents pool overload for  builds\\nfast feedback: early failure detection saves time and resources  \\nzero impact: regular user workflows completely unchanged\\nrobust detection: multiple fallback mechanisms catch various  scenarios\\n\\n testing scenarios\\n| scenario | x64-linux | other jobs | behavior |\\n|----------|-----------|------------|----------|\\n|  + success |  runs first |  run after x64-linux | serial execution |\\n|  + failure |  fails |  cancelled | early termination |\\n| regular build |  runs |  run in parallel | existing behavior |\\nfixes #45945.\\n start  coding agent tips \\n\\n you can make  smarter by setting up custom instructions, customizing its development environment and configuring model context protocol (mcp) servers. learn more  coding agent tips in the docs.', '[gh-flow] add minimal azure provisioning for local developmentthis pr implements a split provisioning solution for the gh-flow sample, allowing developers to provision only essential azure resources when developing locally instead of the full production infrastructure.\\nproblem\\npreviously, developers working locally on gh-flow had to run azd provision which creates the complete azure infrastructure including:\\n- container apps environment & registry\\n- azure-hosted qdrant on container apps\\n- cosmos db\\n- application insights & log analytics\\n- monitoring dashboards\\n- the gh-flow container app service\\nthis resulted in:\\n- slow provisioning (20+ minutes)\\n- high azure costs during development ($50+/month)\\n- unnecessary complexity for local development\\nsolution\\nadded a minimal provisioning option that only creates essential azure resources while leveraging the existing containerized services in the devcontainer:\\nnew files\\n\\ninfra/main.local.bicep - minimal bicep template (49 lines vs 160 lines in full template)\\ninfra/main.local.parameters.json - parameters for local development\\nprovision-local.sh - helper script for easy provisioning\\nlocal-development.md - detailed documentation\\n.gitignore - ignore generated bicep artifacts\\n\\nupdated files\\n\\ndocs/github-flow-getting-started.md - added option 2 for minimal provisioning\\n\\nwhat gets provisioned\\n| deployment type | resources | estimated cost |\\n|---|---|---|\\n| full (azd provision) | storage + container apps + qdrant + cosmos + monitoring + app service | $50+/month |\\n| local (./provision-local.sh) | storage account only | ~$1/month |\\nusage\\n```bash\\nfull deployment (unchanged)\\nazd provision\\nlocal development (new)\\n./provision-local.sh my-local-env\\n```\\nbenefits\\n\\n90%+ resource reduction: 1 storage account vs 10+ resources\\nfaster provisioning: minutes instead of 20+ minutes  \\nlower cost: ~$1/month vs $50+/month\\nbetter developer experience: local debugging, containerized qdrant, faster iteration\\neasy cleanup: minimal resources to delete when done\\n\\ntechnical details\\nthe local template reuses existing core bicep modules but only provisions:\\n- resource group\\n- storage account with file shares (needed for azure container instances)\\nlocal development uses:\\n- containerized qdrant ( <URL>  from devcontainer\\n- local application debugging via vs code\\n- user-configured  and github app settings\\nboth templates are validated and compile successfully. the solution maintains full backward compatibility with existing workflows.\\nfixes #72.\\n\\n[!warning]\\n\\nfirewall rules blocked me from connecting to one or more addresses\\ni tried to connect to the following addresses, but was blocked by firewall rules:\\n\\naka.ms\\ntriggering command: curl -fssl redacted (dns block)\\ntriggering command: /usr/bin/../../opt/az/bin/python3 -im azure.cli bicep validate --file main.local.bicep (dns block)\\ntriggering command: bicep lint main.local.bicep (dns block)\\napp.aladdin.microsoft.com\\ntriggering command: /usr/bin/../../opt/az/bin/python3 -im azure.cli bicep validate --file main.local.bicep (dns block)\\n\\nif you need me to access, download, or install something from one of these locations, you can either:\\n\\nconfigure actions setup steps to set up my environment, which run before the firewall is enabled\\nadd the appropriate urls or hosts to my firewall allow list', 'optimize azure sdk ci pipeline with uv tool for faster package managementsummary\\nthis pr optimizes the azure sdk for python ci pipeline by replacing traditional pip and python -m venv commands with the faster uv tool for package installation and virtual environment management. this change aims to significantly reduce ci pipeline setup time across all build jobs.\\nchanges made\\n1. added uv installation to all ci jobs\\nadded global uv installation steps to all 6 ci jobs (build_linux, build_windows, build_macos, build_extended, build_documentation, and analyze) in eng/pipelines/templates/jobs/ci.yml:\\n```yaml\\ninstall uv tool globally\\n\\n\\ntask: bash@3\\n  displayname: \\'install uv (linux/macos)\\'\\n  inputs:\\n    targettype: inline\\n    script: |\\n      curl -lssf  <URL>  | sh\\n  condition: or(eq(variables[\\'agent.os\\'], \\'linux\\'), eq(variables[\\'agent.os\\'], \\'darwin\\'))\\n\\n\\ntask: powershell@2\\n  displayname: \\'install uv (windows)\\'\\n  inputs:\\n    targettype: inline\\n    script: |\\n      iex (irm  <URL> \\n  condition: eq(variables[\\'agent.os\\'], \\'windows_nt\\')\\n```\\n\\n\\n2. replaced pip commands with uv equivalents\\nupdated package installation commands across template files:\\n\\nbuild-package-artifacts.yml: replaced 3 pip commands\\n  ```bash\\n  # before\\n  python -m pip install --force -r eng/ci_tools.txt\\n  python -m pip install -r eng/release_requirements.txt\\n  python -m pip freeze --all\\n\\n# after\\n  uv pip install --force -r eng/ci_tools.txt\\n  uv pip install -r eng/release_requirements.txt\\n  uv pip freeze --all\\n  ```\\n\\nbuild-extended-artifacts.yml: replaced 1 pip command\\nanalyze.yml: replaced 1 pip command\\n\\n3. updated virtual environment management\\nreplaced powershell script-based virtual environment creation in use-venv.yml with native uv venv commands while preserving all existing parameters (virtualenvironmentname, activate, condition):\\n```powershell\\nnow uses uv venv instead of create-venv.ps1\\nuv venv \"${{ parameters.virtualenvironmentname }}\"\\n```\\nbenefits\\n\\nfaster ci builds: uv is significantly faster than pip for package installation and resolution\\nimproved reliability: more consistent dependency resolution\\ncross-platform compatibility: maintained support for windows, linux, and macos\\nno breaking changes: all existing functionality and parameters preserved\\n\\ntesting\\n\\n all yaml files pass syntax validation\\n preserved existing azure devops pipeline formatting conventions\\n cross-platform installation scripts tested for all supported operating systems\\n no remaining python -m pip references in modified template files\\n\\nfiles modified\\n\\neng/pipelines/templates/jobs/ci.yml - added uv installation to 6 jobs\\neng/pipelines/templates/steps/build-package-artifacts.yml - replaced 3 pip commands  \\neng/pipelines/templates/steps/build-extended-artifacts.yml - replaced 1 pip command\\neng/pipelines/templates/steps/analyze.yml - replaced 1 pip command\\neng/pipelines/templates/steps/use-venv.yml - updated virtual environment management\\n\\nthis optimization will reduce ci pipeline execution time while maintaining full backward compatibility with existing workflows.\\nthis pull request was created as a result of the following prompt from  chat.\\n\\nwe need to optimize the azure sdk for python ci pipeline to use the uv tool for installing packages and creating virtual environments, instead of directly using pip or python -m venv. specifically:\\n\\nin eng/pipelines/templates/jobs/ci.yml, add a global uv installation step at the top of each job (build_linux, build_windows, build_macos, build_extended, build_documentation, and analyze). insert the following yaml block as the first step under steps: for each job:\\n\\n```yaml\\ninstall uv tool globally\\n\\n\\ntask: bash@3\\n  displayname: \\'install uv (linux/macos)\\'\\n  inputs:\\n    targettype: inline\\n    script: |\\n      curl -lssf  <URL>  | sh\\n  condition: or(\\n    eq(variables[\\'agent.os\\'], \\'linux\\'),\\n    eq(variables[\\'agent.os\\'], \\'darwin\\')\\n  )\\n\\n\\ntask: powershell@2\\n  displayname: \\'install uv (windows)\\'\\n  inputs:\\n    targettype: inline\\n    script: |\\n      iex (irm  <URL> \\n  condition: eq(variables[\\'agent.os\\'], \\'windows_nt\\')\\n```\\n\\n\\nreplace all direct python -m pip install and python -m pip freeze invocations in yaml under eng/pipelines/templates (specifically build-package-artifacts.yml, build-extended-artifacts.yml, analyze.yml) with uv pip install and uv pip freeze respectively.\\n\\n\\nupdate eng/pipelines/templates/steps/use-venv.yml to create and activate virtual environments using uv instead of the existing create-venv.ps1 script. replace the existing pwsh steps with uv venv create and uv venv activate commands, preserving the virtualenvironmentname, activate, and condition parameters.\\n\\n\\nthese changes aim to reduce setup time by leveraging the uv tool\\'s optimized environment and package management capabilities throughout the ci pipeline.\\n\\n start  coding agent tips \\n\\n you can make  smarter by setting up custom instructions, customizing its development environment and configuring model context protocol (mcp) servers. learn more  coding agent tips in the docs.']"
        ],
        [
         "34",
         "34",
         "7",
         "34_calculate_spring_pose_bone_rotations_calculate_object_pose_bone_rotations_calculate_joint_pair_head_pose_bone_rotations_get_bone_namespring",
         "['calculate_spring_pose_bone_rotations', 'calculate_object_pose_bone_rotations', 'calculate_joint_pair_head_pose_bone_rotations', 'get_bone_namespring', 'get_bone_name', 'spring_bone1', 'get_bone_extension', 'io_scene_vrm', 't_pose', 'blender']",
         "['spring bone: get_bone_namespring bone: get_bone_name\\n\\nget_bone_name\\n\\n\\nuuid\\n\\nget_bone_extension(bone).uuid\\n\\n\\n\\n```\\n         3607230 function calls in 2.629 seconds\\nordered by: internal time\\nncalls  tottime  percall  cumtime  percall filename:lineno(function)\\n    72100    0.831    0.000    1.097    0.000 /home/ubuntu/repos/vrm-addon-for-blender/src/io_scene_vrm/editor/spring_bone1/handler.py:526(calculate_joint_pair_head_pose_bone_rotations)\\n   158900    0.630    0.000    0.917    0.000 /home/ubuntu/repos/vrm-addon-for-blender/src/io_scene_vrm/editor/property_group.py:304(get_bone_name)\\n     2450    0.349    0.000    2.356    0.001 /home/ubuntu/repos/vrm-addon-for-blender/src/io_scene_vrm/editor/spring_bone1/handler.py:408(calculate_spring_pose_bone_rotations)\\n```\\n\\n```\\n         3607230 function calls in 2.593 seconds\\nordered by: internal time\\nncalls  tottime  percall  cumtime  percall filename:lineno(function)\\n    72100    0.835    0.000    1.096    0.000 /home/ubuntu/repos/vrm-addon-for-blender/src/io_scene_vrm/editor/spring_bone1/handler.py:526(calculate_joint_pair_head_pose_bone_rotations)\\n   158900    0.611    0.000    0.897    0.000 /home/ubuntu/repos/vrm-addon-for-blender/src/io_scene_vrm/editor/property_group.py:306(get_bone_name)\\n     2450    0.347    0.000    2.330    0.001 /home/ubuntu/repos/vrm-addon-for-blender/src/io_scene_vrm/editor/spring_bone1/handler.py:408(calculate_spring_pose_bone_rotations)\\n```\\n\\n\\nget_bone_name: 0.630 → 0.611 (3.0%)\\n: 2.629 → 2.593 (1.4%)\\n\\n\\nlink to  run:  <URL> ', \"spring bone: spring bone\\n\\ncalculate_joint_pair_head_pose_bone_rotations\\n\\ninverted_safe()\\n\\n\\n\\n\\n\\n\\n```\\n         3607230 function calls in 2.689 seconds\\nordered by: internal time\\nncalls  tottime  percall  cumtime  percall filename:lineno(function)\\n    72100    0.875    0.000    1.139    0.000 /home/ubuntu/repos/vrm-addon-for-blender/src/io_scene_vrm/editor/spring_bone1/handler.py:526(calculate_joint_pair_head_pose_bone_rotations)\\n   158900    0.635    0.000    0.923    0.000 /home/ubuntu/repos/vrm-addon-for-blender/src/io_scene_vrm/editor/property_group.py:304(get_bone_name)\\n     2450    0.360    0.000    2.411    0.001 /home/ubuntu/repos/vrm-addon-for-blender/src/io_scene_vrm/editor/spring_bone1/handler.py:408(calculate_spring_pose_bone_rotations)\\n   290850    0.082    0.000    0.082    0.000 {method 'inverted_safe' of 'matrix' objects}\\n```\\n\\n```\\n         3535130 function calls in 2.553 seconds\\nordered by: internal time\\nncalls  tottime  percall  cumtime  percall filename:lineno(function)\\n    72100    0.766    0.000    1.016    0.000 /home/ubuntu/repos/vrm-addon-for-blender/src/io_scene_vrm/editor/spring_bone1/handler.py:526(calculate_joint_pair_head_pose_bone_rotations)\\n   158900    0.621    0.000    0.907    0.000 /home/ubuntu/repos/vrm-addon-for-blender/src/io_scene_vrm/editor/property_group.py:304(get_bone_name)\\n     2450    0.367    0.000    2.280    0.001 /home/ubuntu/repos/vrm-addon-for-blender/src/io_scene_vrm/editor/spring_bone1/handler.py:408(calculate_spring_pose_bone_rotations)\\n   218750    0.063    0.000    0.063    0.000 {method 'inverted_safe' of 'matrix' objects}\\n```\\n\\n\\n: 2.689 → 2.553 (5.1%)\\ncalculate_joint_pair_head_pose_bone_rotations: 0.875 → 0.766 (12.5%)\\ninverted_safe: 290,850 → 218,750 (24.8%)\\ninverted_safe: 0.082 → 0.063 (23.2%)\\n\\n\\nlink to  run:  <URL> \", \"spring bonespring bone\\n\\nspring bone\\n\\nupdate_pose_bone_rotationsarmature\\nget_bone_name\\n\\n\\n\\n\\n\\n\\n```\\n         3607230 function calls in 2.715 seconds\\nordered by: internal time\\nncalls  tottime  percall  cumtime  percall filename:lineno(function)\\n    72100    0.867    0.000    1.134    0.000 /home/ubuntu/repos/vrm-addon-for-blender/src/io_scene_vrm/editor/spring_bone1/handler.py:526(calculate_joint_pair_head_pose_bone_rotations)\\n   158900    0.648    0.000    0.944    0.000 /home/ubuntu/repos/vrm-addon-for-blender/src/io_scene_vrm/editor/property_group.py:304(get_bone_name)\\n     2450    0.364    0.000    2.432    0.001 /home/ubuntu/repos/vrm-addon-for-blender/src/io_scene_vrm/editor/spring_bone1/handler.py:408(calculate_spring_pose_bone_rotations)\\n   387450    0.102    0.000    0.102    0.000 {method 'get' of 'bpy_prop_collection' objects}\\n   290850    0.082    0.000    0.082    0.000 {method 'inverted_safe' of 'matrix' objects}\\n   288400    0.039    0.000    0.039    0.000 {method 'to_translation' of 'matrix' objects}\\n```\\n\\n```\\n         3833340 function calls in 2.748 seconds\\nordered by: internal time\\nncalls  tottime  percall  cumtime  percall filename:lineno(function)\\n    72100    0.847    0.000    1.112    0.000 /home/ubuntu/repos/vrm-addon-for-blender/src/io_scene_vrm/editor/spring_bone1/handler.py:552(calculate_joint_pair_head_pose_bone_rotations)\\n   158900    0.638    0.000    0.927    0.000 /home/ubuntu/repos/vrm-addon-for-blender/src/io_scene_vrm/editor/property_group.py:304(get_bone_name)\\n     2450    0.422    0.000    2.469    0.001 /home/ubuntu/repos/vrm-addon-for-blender/src/io_scene_vrm/editor/spring_bone1/handler.py:422(calculate_spring_pose_bone_rotations)\\n   387450    0.097    0.000    0.097    0.000 {method 'get' of 'bpy_prop_collection' objects}\\n   290850    0.081    0.000    0.081    0.000 {method 'inverted_safe' of 'matrix' objects}\\n   288400    0.027    0.000    0.027    0.000 {method 'to_translation' of 'matrix' objects}\\n```\\n\\n\\n- calculate_joint_pair_head_pose_bone_rotations: 0.867 → 0.847 (2.3%)\\n- get_bone_name: 0.648 → 0.638 (1.5%)\\n- to_translation: 0.039 → 0.027 (30.8%)\\nto_translation30.8%calculate_joint_pair_head_pose_bone_rotations2.3%\\nspring bone\\nlink to  run\\n <URL> \"]"
        ],
        [
         "35",
         "35",
         "8",
         "35_compile_lua_testluacompiler_vmvalid_golden_testpythoncompiler_vmvalid_golden_lua",
         "['compile_lua', 'testluacompiler_vmvalid_golden', 'testpythoncompiler_vmvalid_golden', 'lua', 'compiler', 'testluacompiler_validprograms', 'compilersummary', 'testpycompiler_subsetprograms', 'improve', 'improvementssummary']",
         "['lua compiler improvementssummary\\n\\nmake the lua compiler inline append operations when a list literal is given\\nregenerate lua machine outputs with updated compiler\\n\\ntesting\\n\\ngo test -tags slow ./compiler/x/lua -run testluacompiler_validprograms -count=1\\n\\n\\n <URL> ', 'lua compiler improvementssummary\\n\\noptimize lua compiler to skip helper calls when indexing simple lists or strings\\nupdate runtime boolean printing to match golden expectations\\nregenerate lua machine outputs\\nnote new capabilities in lua machine readme\\n\\ntesting\\n\\ngo test -tags slow ./compiler/x/lua -run vmvalid_golden (fails: 73 passed, 27 failed)\\n\\n\\n <URL> ', 'add main handling and faster list append for lua transpilersummary\\n\\nsupport programs with main by emitting a call at the end of generated lua\\noptimize append translation to mutate the list instead of copying\\nadd lua translation for pow builtin\\nmark brilliant-numbers as transpiled\\nadd transpiled lua and expected output for brilliant-numbers\\n\\ntesting\\n\\ngo test ./transpiler/x/lua -run rosetta -index 137 -tags slow -update -v (fails: took too long, interrupted)\\n\\n\\n <URL> ']"
        ],
        [
         "36",
         "36",
         "8",
         "36_fixes_cacheinspired_adapters_tasksfixe",
         "['fixes', 'cacheinspired', 'adapters', 'tasksfixe', 'gpt', 'optimize', 'prebid', 'adapter', 'cache', 'greedy']",
         "['prebid 10: disable greedy feature by default, break up long tasksfixe #10062', 'prebid 10: add yield helper and disable greedy by defaultredo  <URL>  for typescript base and fixes #10062\\nsee  <URL> ', 'ix bid adapter: cache gpt slot lookupstrying to spped up the enormous ix bid adapter']"
        ],
        [
         "37",
         "37",
         "8",
         "37_qme_caches_memoization_caches_bench_memo",
         "['qme_caches', 'memoization', 'caches', 'bench_memo', 'memoize', 'mochi_memo', 'memoized', 'cached', 'memoizationsummary', 'computationsummary']",
         "['add memoized fibonacci implementationsummary\\n\\ndownload task 365 from rosetta code (fibonacci sequence)\\nimplement memoized fibonacci example in mochi\\nprovide golden output running under vm\\n\\ntesting\\n\\nmochi_rosetta_only=fibonacci-sequence-5 go test ./runtime/vm -tags=slow -run rosetta -count=1\\n\\n\\n <URL> ', 'add optional memoization in interpretersummary\\n\\nadd a flag to toggle memoization of pure functions\\nwire the flag into mochi run and mochi test\\nskip memoization logic unless enabled\\n\\ntesting\\n\\ngo test ./...\\ngo run ./cmd/mochi-bench > /tmp/bench_default.txt\\nmochi_memo=1 go run ./cmd/mochi-bench > /tmp/bench_memo.txt\\n\\n\\n <URL> ', \"refactor(twap): implement strategy pattern for accumulator updatesthis commit refactors the twap module to use the strategy pattern more thoroughly for its accumulator updates. previously, the strategy pattern was only used for twap computation but not for the accumulator updates.\\nkey changes:\\n- add updateaccumulators method to the twapstrategy interface\\n- implement strategy-specific accumulator update logic for both arithmetic and geometric strategies\\n- modify getinterpolatedrecord to use the provided strategy's accumulator update method\\n- update remaining code to use the appropriate strategy for accumulator updates\\n- maintain backward compatibility in exported functions and existing code paths\\nwith this change, geometric accumulator calculations are now only performed when using the geometric strategy, making the system more efficient by avoiding unnecessary calculations for the arithmetic strategy.\\n generated with  code\\nco-authored-by:  noreply@anthropic.com\\ncloses: #7113\"]"
        ],
        [
         "38",
         "38",
         "10",
         "38_compilationfixes_compiler_compilations_rustdoc",
         "['compilationfixes', 'compiler', 'compilations', 'rustdoc', 'compilation', 'compile_with_diagnostics', 'rust', 'recomputes', 'write_offset', '_collect_indices']",
         "['fix memory usage by calling comemo::evict(10) after compilationfixes #104 \\nthis pr addresses excessive memory usage during compilation by calling comemo::evict(10) after each typst::compile() invocation, as suggested in the issue.\\nproblem\\nwhen generating large documents (e.g., reports with hundreds of pages), typst-py can consume excessive memory due to the comemo cache not being cleared between compilations. this is particularly problematic for long-running applications that perform multiple compilations.\\nsolution\\nadded comemo::evict(10) calls after each compilation to limit cache size and manage memory usage:\\n\\nafter the main typst::compile(self) call in compile_with_diagnostics\\nafter the html typst::compile::<htmldocument>(self) call for html format\\n\\nchanges\\n\\nadded use comemo; import to src/compiler.rs\\nadded memory eviction calls with explanatory comments\\nno breaking changes - all existing functionality preserved\\n\\n```rust\\nlet warned { output, warnings } = typst::compile(self);\\n// evict comemo cache to limit memory usage after compilation\\ncomemo::evict(10);\\n```\\ntesting\\n\\n all compilation formats (pdf, png, svg, html) work correctly\\n both function-based and compiler class usage patterns tested\\n memory-intensive scenarios with multiple large document compilations verified\\n build and functionality tests pass\\n\\nimpact\\nthis is a minimal, surgical fix that addresses the memory usage concern without affecting performance or functionality. the change helps applications that perform many compilations avoid memory leaks while maintaining fast compilation times for subsequent runs.\\n start  coding agent tips \\n\\n you can make  smarter by setting up custom instructions, customizing its development environment and configuring model context protocol (mcp) servers. learn more  coding agent tips in the docs.', 'refactor repl to eliminate temporary file usagethe repl implementation was using a \"huge hack\" of creating temporary files to conform with the existing compilation interface that expects a bufread object. this approach was inefficient and unnecessary.\\nchanges made\\nthis pr replaces the temporary file usage with std::io::<vec<u8>> which implements bufread and can be created directly from string content:\\nbefore:\\nrust\\n// create temporary file\\nlet file_res = tempfile();\\nlet mut file = file_res.unwrap();\\nfile.write_all(line.as_bytes());\\nfile.seek(seekfrom::start(0));\\nlet mut bufread: box<dyn bufread> = box::new(bufreader::new(file));\\nafter:\\nrust\\n// create in-memory buffer\\nlet mut bufread: box<dyn bufread> = box::new(::new(line.into_bytes()));\\nimpact\\n\\nrepl input processing: no longer creates temporary files for each command\\nexpression evaluation (-e option): no longer creates temporary files for expressions\\ncode reduction: removed 43 lines of error-prone file i/o code, added only 6 lines\\nperformance: eliminates filesystem operations for every repl interaction\\nreliability: removes potential issues with temporary file creation/cleanup\\n\\ntesting\\nverified that:\\n- repl functionality works identically to before\\n- expression evaluation with -e flag works correctly\\n- error handling remains functional\\n- no temporary files are created during operation\\n- edge cases (empty input, whitespace) are handled properly\\nthe refactoring successfully maintains compatibility with the existing compilation interface while eliminating the temporary file dependency.\\nfixes #8.\\n start  coding agent tips \\n\\n share your feedback on  coding agent for the chance to win a $200 gift card! click here to start the survey.', \"implement salsa-based incremental compilationsummary\\nthis pr implements salsa-based incremental compilation infrastructure for the rue compiler, enabling ide-friendly incremental updates.\\nchanges\\n\\nsalsa database setup: added basic ruedatabase type alias using salsa's databaseimpl\\nincremental file parsing: implemented parse_file as a salsa tracked function that automatically caches results\\ncomprehensive testing: added tests to verify incremental behavior works correctly (caching unchanged results)\\napi fixes: updated example file to use current api and added partialeq to parseerror for salsa compatibility\\n\\nkey benefits\\n\\nfast recompilation: only recomputes changed files and their dependents\\nide support: foundation for language server protocol implementation\\nmemory efficient: automatic result caching and invalidation\\nexpression-level granularity: future support for fine-grained incremental computation\\n\\ntesting\\n\\nall existing tests continue to pass\\nnew incremental compilation tests verify caching behavior\\nboth buck2 and cargo builds work correctly\\n\\narchitecture\\n```rust\\n// salsa input (can be modified)\\n[salsa::input]\\npub struct sourcefile { / path, text / }\\n// salsa tracked function (automatically cached)\\n[salsa::tracked]\\npub fn parse_file(db: &dyn database, file: sourcefile) -> result, arc\\\\>\\n// usage - salsa handles caching automatically\\nlet result = parse_file(&db, file);\\nfile.set_text(&mut db).to(new_content); // invalidates cache\\nlet new_result = parse_file(&db, file); // recomputes only if needed\\n```\\nnext steps\\nthis establishes the foundation for:\\n- semantic analysis queries\\n- type checking\\n- name resolution\\n- code generation\\n- lsp implementation\\n generated with  code\\nco-authored-by:  noreply@anthropic.com\"]"
        ],
        [
         "39",
         "39",
         "12",
         "39_pytest_tests_testing_test_sft",
         "['pytest', 'tests', 'testing', 'test_sft', 'test_client', 'test_common', 'unittest', 'test_supervised', 'lint', 'npm']",
         "['split slow tests into separate filessummary\\n\\nseparate field-error tests into individual files\\nsplit zod-effects test into a second refinement test file\\n\\ntesting\\n\\nnpm run lint-fix\\nnpm run lint\\nnpm run tsc\\nnpm run test (fails: slow test file, some tests failed)', 'disable uv-runsummary\\n\\ndisable uv-run since it causes oom by reinstalling cuda torch every run\\n\\ntesting\\n\\nmake test (fails: plugin errors and network calls to huggingface.co blocked)\\n\\n\\n <URL> ', 'avoid network downloads in testssummary\\n\\nadd local_gpt2_tokenizer fixture to load gpt2 tokenizer from local files\\nswitch gpt2-using tests to rely on this fixture\\nuse passthrough tokenizer for validation-set test\\n\\ntesting\\n\\npre-commit run --all-files\\npytest tests/test_supervised.py tests/test_sft.py tests/test_text.py -m \"not entry and not slow and not ray\"\\n\\n\\n <URL> ']"
        ],
        [
         "40",
         "40",
         "8",
         "40_bun_options_test_concurrency_test_transaction_test",
         "['bun_options', 'test_concurrency', 'test', 'transaction_test', 'run_with_redis', 'node_modules', 'run_scriptllmredis', 'redis', 'webkit', 'parseoptionsenv']",
         "['fix net server nodelay handlingsummary\\n\\nadd node.js test test-net-server-nodelay\\ncall socket.setnodelay() when server nodelay option is enabled\\n\\ntesting\\n\\nbun bd --silent node:test test-net-server-nodelay (fails: missing webkit build files)', 'speculative test for lockfile install hangsummary\\n\\nschedule dependency downloads when creating a new lockfile so async tasks run\\nadd regression test for new lockfile installs\\n\\ntesting\\n\\nnode_modules/.bin/prettier -w test/regression/issue/020850.test.ts\\nbun run clang-format (fails: could not download build dependencies)\\nbun run zig-format (fails: could not download build dependencies)\\nbun bd test test/regression/issue/020850.test.ts (fails: could not download build dependencies)\\n\\n\\n <URL> ', 'fix: \\n\\n src/api study/run \\n\\n\\ndb.session.begin_nested()db.session.commit()\\nredis\\nrun_scriptllmredis\\n\\n\\n\\nrun_with_redis\\n30\\n3\\n\\n\\n\\n\\n\\nreset_user_study_info_by_lesson\\n\\n\\n\\n\\n\\n\\n\\nget_script\\n\\n\\n\\n\\n\\n\\n\\nrun_script_inner\\n\\n\\n\\n\\n\\n\\n\\n1. test_concurrency.pystudy/runreset-study-progress\\n2. monitor_db.py\\n3. transaction_test.py\\n\\n\\n/\\nmonitor_db.py\\njmeterlocust\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nlink to  run:  <URL> \\nrequested by: geyunfei@gmail.com']"
        ],
        [
         "41",
         "41",
         "8",
         "41_caches_github_check_python_deps_cache",
         "['caches', 'github', 'check_python_deps', 'cache', 'dockerfile', 'caching', 'pnpm', 'testing', 'docker', 'npm']",
         "['ci: cache node setup in lint jobsummary\\n\\nupdate lint job to install node.js with caching before using pre-commit\\n\\ntesting\\n\\npre-commit run --files .github/workflows/ci.yml (failed: keyboardinterrupt during environment initialization)\\n\\n\\n <URL> ', ' enable docker buildkit cachingsummary\\n\\nenable docker buildx cache via github actions\\nrevert dockerfile cache mounts\\ndocument the workflow improvement in a changeset\\n\\ntesting\\n\\npnpm lint\\npnpm test:unit (fails: fetch failed)\\n\\n\\n <URL> ', 'ci: cache docs sandbox buildsummary\\n\\navoid repeated installs by installing docs deps first\\ncache sandbox docker build layers with buildx\\n\\ntesting\\n\\npre-commit run --files .github/workflows/docs.yml\\npython scripts/check_python_deps.py\\npytest tests/test_ping_agent.py tests/test_af_requests.py -q\\n\\n\\n <URL> ']"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 42
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>463</td>\n",
       "      <td>0_coding_fixes_cache_optimizations</td>\n",
       "      <td>[coding, fixes, cache, optimizations, implemen...</td>\n",
       "      <td>[feat: implement async notification and teleme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>137</td>\n",
       "      <td>1_compiler_testvm_ir_compile_onnxscript</td>\n",
       "      <td>[compiler, testvm_ir, compile, onnxscript, x86...</td>\n",
       "      <td>[convert wormholecontract to sol_storage! macr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>47</td>\n",
       "      <td>2_alpha_agi_insight_v1_insight_browser_v1_alph...</td>\n",
       "      <td>[alpha_agi_insight_v1, insight_browser_v1, alp...</td>\n",
       "      <td>[[alpha_factory] tighten insight bundle size c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>46</td>\n",
       "      <td>3_mochi_benchmark_testswifttranspiler_rosetta_...</td>\n",
       "      <td>[mochi_benchmark, testswifttranspiler_rosetta_...</td>\n",
       "      <td>[add benchmark support to c++ transpiler tests...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>31</td>\n",
       "      <td>4_github_caching_cache_git</td>\n",
       "      <td>[github, caching, cache, git, circleci, compil...</td>\n",
       "      <td>[add vcpkg dependency caching to windows ci wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>5_compilehashouterjoin_compilejoinquery_outer_...</td>\n",
       "      <td>[compilehashouterjoin, compilejoinquery, outer...</td>\n",
       "      <td>[improve join performance with hashed left joi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "      <td>6_gpu_model_runner_webgpu_test_paged_attention...</td>\n",
       "      <td>[gpu_model_runner, webgpu, test_paged_attentio...</td>\n",
       "      <td>[[core] freeze gc during cuda graph capture to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>7_buildslotswithdateranges_scheduling_calendar...</td>\n",
       "      <td>[buildslotswithdateranges, scheduling, calenda...</td>\n",
       "      <td>[feat: optimize slot generation with inverted ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>8_mvn_wget_java_jsonparser</td>\n",
       "      <td>[mvn, wget, java, jsonparser, parsersummary, t...</td>\n",
       "      <td>[implement kv batch putmanysummary\\n\\nadd putm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>9_dotnet_csproj_tests_inlining</td>\n",
       "      <td>[dotnet, csproj, tests, inlining, net9, struct...</td>\n",
       "      <td>[apply aggressiveinlining attributessummary\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>10_gradle_gradlew_kotlinx_asset_conversion_ref...</td>\n",
       "      <td>[gradle, gradlew, kotlinx, asset_conversion_re...</td>\n",
       "      <td>[fix heavy ui updates on main threadsummary\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>11_dotnetbuild_dotnet_dotnetbuildfromvmr_csproj</td>\n",
       "      <td>[dotnetbuild, dotnet, dotnetbuildfromvmr, cspr...</td>\n",
       "      <td>[make generatedepsfile and generateruntimeconf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>12_github_git_gitlab_bitbucket</td>\n",
       "      <td>[github, git, gitlab, bitbucket, writechangese...</td>\n",
       "      <td>[fix github api rate limiting in chess workflo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>13_pharmacyincomecostbillitemdto_opd_income_re...</td>\n",
       "      <td>[pharmacyincomecostbillitemdto, opd_income_rep...</td>\n",
       "      <td>[implement bill item dto for pharmacy income r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>14_testfortrancompiler_vmvalid_golden_fortran_...</td>\n",
       "      <td>[testfortrancompiler_vmvalid_golden, fortran, ...</td>\n",
       "      <td>[improve fortran compiler constant foldingsumm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15_left_join_outer_join_testclojurecompiler_vm...</td>\n",
       "      <td>[left_join, outer_join, testclojurecompiler_vm...</td>\n",
       "      <td>[improve go join compilationsummary\\n\\nimprove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>11</td>\n",
       "      <td>16_messagecache_chat_messageids_useaiassistant...</td>\n",
       "      <td>[messagecache, chat, messageids, useaiassistan...</td>\n",
       "      <td>[feat: optimize duplicate checking in userealt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>14</td>\n",
       "      <td>17_csharp_linq_debuggability_cs</td>\n",
       "      <td>[csharp, linq, debuggability, cs, compatibilit...</td>\n",
       "      <td>[optimize trimstacktrace to use spans instead ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>11</td>\n",
       "      <td>18_testprologcompiler_testerlangcompiler_golde...</td>\n",
       "      <td>[testprologcompiler, testerlangcompiler_golden...</td>\n",
       "      <td>[improve prolog transpilersummary\\n\\ntranspile...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>11</td>\n",
       "      <td>19_sitemap_pagination_frontend_sitemaps</td>\n",
       "      <td>[sitemap, pagination, frontend, sitemaps, pagi...</td>\n",
       "      <td>[add yearly and monthly sitemap organization f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>20_preview_docs_plotly_debounce_github</td>\n",
       "      <td>[preview_docs, plotly, debounce, github, sync_...</td>\n",
       "      <td>[fix lag in pyplot example by implementing pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>10</td>\n",
       "      <td>21_optimized_blas_kernels_fp16_openmp_opencl_onnx</td>\n",
       "      <td>[optimized_blas_kernels_fp16, openmp, opencl, ...</td>\n",
       "      <td>[[wip] improve dft implementationdft implement...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>14</td>\n",
       "      <td>22_testdartcompiler_vmvalid_golden_testgocompi...</td>\n",
       "      <td>[testdartcompiler_vmvalid_golden, testgocompil...</td>\n",
       "      <td>[improve go compiler struct reusesummary\\n\\nen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>11</td>\n",
       "      <td>23_rustc_hash_rustc_hashing_lib_rs</td>\n",
       "      <td>[rustc_hash, rustc, hashing, lib_rs, hash, fxh...</td>\n",
       "      <td>[add database caching for folder scan results ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "      <td>24__getbusytimesfromteamlimits_refactors_booki...</td>\n",
       "      <td>[_getbusytimesfromteamlimits, refactors, booki...</td>\n",
       "      <td>[perf: optimize team bookings query by fetchin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>10</td>\n",
       "      <td>25_git_repo_commit_build_source_files</td>\n",
       "      <td>[git, repo, commit, build_source_files, dev, r...</td>\n",
       "      <td>[optimize source file tree buildingsummary\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>9</td>\n",
       "      <td>26_calendarcache_cachedcalendarservice_getcach...</td>\n",
       "      <td>[calendarcache, cachedcalendarservice, getcach...</td>\n",
       "      <td>[feat: optimize calendar cache retrieval with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>8</td>\n",
       "      <td>27_optimizations_kernels_git_optimization</td>\n",
       "      <td>[optimizations, kernels, git, optimization, op...</td>\n",
       "      <td>[add benchmark for deserializing large added v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>9</td>\n",
       "      <td>28_rpc_metamask_metamaskwallet_endpoints</td>\n",
       "      <td>[rpc, metamask, metamaskwallet, endpoints, set...</td>\n",
       "      <td>[docs: update trusted hash guide[x] analyzed r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>7</td>\n",
       "      <td>29_scrape_tool_batch_scrape_tool_seleniumscrap...</td>\n",
       "      <td>[scrape_tool, batch_scrape_tool, seleniumscrap...</td>\n",
       "      <td>[implement retry-after header handling for imp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>30_testracketcompiler_testccompiler_vmvalid_go...</td>\n",
       "      <td>[testracketcompiler, testccompiler_vmvalid_gol...</td>\n",
       "      <td>[update c backend to use stack arrayssummary\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>7</td>\n",
       "      <td>31_git_build_options_disable_grpc_modules_build</td>\n",
       "      <td>[git, build_options, disable_grpc_modules, bui...</td>\n",
       "      <td>[build tag to exclude cloud archiver providers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>9</td>\n",
       "      <td>32_terrainpatch_meshes_rendering_patch</td>\n",
       "      <td>[terrainpatch, meshes, rendering, patch, patch...</td>\n",
       "      <td>[optimize billboard rendering for 3d particles...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>8</td>\n",
       "      <td>33_maven_azure_build_macos_build_windows</td>\n",
       "      <td>[maven, azure, build_macos, build_windows, doc...</td>\n",
       "      <td>[[azure pipelines] implement  build detection ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>7</td>\n",
       "      <td>34_calculate_spring_pose_bone_rotations_calcul...</td>\n",
       "      <td>[calculate_spring_pose_bone_rotations, calcula...</td>\n",
       "      <td>[spring bone: get_bone_namespring bone: get_bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>8</td>\n",
       "      <td>35_compile_lua_testluacompiler_vmvalid_golden_...</td>\n",
       "      <td>[compile_lua, testluacompiler_vmvalid_golden, ...</td>\n",
       "      <td>[lua compiler improvementssummary\\n\\nmake the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>8</td>\n",
       "      <td>36_fixes_cacheinspired_adapters_tasksfixe</td>\n",
       "      <td>[fixes, cacheinspired, adapters, tasksfixe, gp...</td>\n",
       "      <td>[prebid 10: disable greedy feature by default,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>8</td>\n",
       "      <td>37_qme_caches_memoization_caches_bench_memo</td>\n",
       "      <td>[qme_caches, memoization, caches, bench_memo, ...</td>\n",
       "      <td>[add memoized fibonacci implementationsummary\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>10</td>\n",
       "      <td>38_compilationfixes_compiler_compilations_rustdoc</td>\n",
       "      <td>[compilationfixes, compiler, compilations, rus...</td>\n",
       "      <td>[fix memory usage by calling comemo::evict(10)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>12</td>\n",
       "      <td>39_pytest_tests_testing_test_sft</td>\n",
       "      <td>[pytest, tests, testing, test_sft, test_client...</td>\n",
       "      <td>[split slow tests into separate filessummary\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "      <td>40_bun_options_test_concurrency_test_transacti...</td>\n",
       "      <td>[bun_options, test_concurrency, test, transact...</td>\n",
       "      <td>[fix net server nodelay handlingsummary\\n\\nadd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>8</td>\n",
       "      <td>41_caches_github_check_python_deps_cache</td>\n",
       "      <td>[caches, github, check_python_deps, cache, doc...</td>\n",
       "      <td>[ci: cache node setup in lint jobsummary\\n\\nup...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic  Count                                               Name  \\\n",
       "0       0    463                 0_coding_fixes_cache_optimizations   \n",
       "1       1    137            1_compiler_testvm_ir_compile_onnxscript   \n",
       "2       2     47  2_alpha_agi_insight_v1_insight_browser_v1_alph...   \n",
       "3       3     46  3_mochi_benchmark_testswifttranspiler_rosetta_...   \n",
       "4       4     31                         4_github_caching_cache_git   \n",
       "5       5     21  5_compilehashouterjoin_compilejoinquery_outer_...   \n",
       "6       6     21  6_gpu_model_runner_webgpu_test_paged_attention...   \n",
       "7       7     18  7_buildslotswithdateranges_scheduling_calendar...   \n",
       "8       8     17                         8_mvn_wget_java_jsonparser   \n",
       "9       9     16                     9_dotnet_csproj_tests_inlining   \n",
       "10     10     16  10_gradle_gradlew_kotlinx_asset_conversion_ref...   \n",
       "11     11     22    11_dotnetbuild_dotnet_dotnetbuildfromvmr_csproj   \n",
       "12     12     17                     12_github_git_gitlab_bitbucket   \n",
       "13     13     14  13_pharmacyincomecostbillitemdto_opd_income_re...   \n",
       "14     14     12  14_testfortrancompiler_vmvalid_golden_fortran_...   \n",
       "15     15     15  15_left_join_outer_join_testclojurecompiler_vm...   \n",
       "16     16     11  16_messagecache_chat_messageids_useaiassistant...   \n",
       "17     17     14                    17_csharp_linq_debuggability_cs   \n",
       "18     18     11  18_testprologcompiler_testerlangcompiler_golde...   \n",
       "19     19     11            19_sitemap_pagination_frontend_sitemaps   \n",
       "20     20     11             20_preview_docs_plotly_debounce_github   \n",
       "21     21     10  21_optimized_blas_kernels_fp16_openmp_opencl_onnx   \n",
       "22     22     14  22_testdartcompiler_vmvalid_golden_testgocompi...   \n",
       "23     23     11                 23_rustc_hash_rustc_hashing_lib_rs   \n",
       "24     24     10  24__getbusytimesfromteamlimits_refactors_booki...   \n",
       "25     25     10              25_git_repo_commit_build_source_files   \n",
       "26     26      9  26_calendarcache_cachedcalendarservice_getcach...   \n",
       "27     27      8          27_optimizations_kernels_git_optimization   \n",
       "28     28      9           28_rpc_metamask_metamaskwallet_endpoints   \n",
       "29     29      7  29_scrape_tool_batch_scrape_tool_seleniumscrap...   \n",
       "30     30      8  30_testracketcompiler_testccompiler_vmvalid_go...   \n",
       "31     31      7    31_git_build_options_disable_grpc_modules_build   \n",
       "32     32      9             32_terrainpatch_meshes_rendering_patch   \n",
       "33     33      8           33_maven_azure_build_macos_build_windows   \n",
       "34     34      7  34_calculate_spring_pose_bone_rotations_calcul...   \n",
       "35     35      8  35_compile_lua_testluacompiler_vmvalid_golden_...   \n",
       "36     36      8          36_fixes_cacheinspired_adapters_tasksfixe   \n",
       "37     37      8        37_qme_caches_memoization_caches_bench_memo   \n",
       "38     38     10  38_compilationfixes_compiler_compilations_rustdoc   \n",
       "39     39     12                   39_pytest_tests_testing_test_sft   \n",
       "40     40      8  40_bun_options_test_concurrency_test_transacti...   \n",
       "41     41      8           41_caches_github_check_python_deps_cache   \n",
       "\n",
       "                                       Representation  \\\n",
       "0   [coding, fixes, cache, optimizations, implemen...   \n",
       "1   [compiler, testvm_ir, compile, onnxscript, x86...   \n",
       "2   [alpha_agi_insight_v1, insight_browser_v1, alp...   \n",
       "3   [mochi_benchmark, testswifttranspiler_rosetta_...   \n",
       "4   [github, caching, cache, git, circleci, compil...   \n",
       "5   [compilehashouterjoin, compilejoinquery, outer...   \n",
       "6   [gpu_model_runner, webgpu, test_paged_attentio...   \n",
       "7   [buildslotswithdateranges, scheduling, calenda...   \n",
       "8   [mvn, wget, java, jsonparser, parsersummary, t...   \n",
       "9   [dotnet, csproj, tests, inlining, net9, struct...   \n",
       "10  [gradle, gradlew, kotlinx, asset_conversion_re...   \n",
       "11  [dotnetbuild, dotnet, dotnetbuildfromvmr, cspr...   \n",
       "12  [github, git, gitlab, bitbucket, writechangese...   \n",
       "13  [pharmacyincomecostbillitemdto, opd_income_rep...   \n",
       "14  [testfortrancompiler_vmvalid_golden, fortran, ...   \n",
       "15  [left_join, outer_join, testclojurecompiler_vm...   \n",
       "16  [messagecache, chat, messageids, useaiassistan...   \n",
       "17  [csharp, linq, debuggability, cs, compatibilit...   \n",
       "18  [testprologcompiler, testerlangcompiler_golden...   \n",
       "19  [sitemap, pagination, frontend, sitemaps, pagi...   \n",
       "20  [preview_docs, plotly, debounce, github, sync_...   \n",
       "21  [optimized_blas_kernels_fp16, openmp, opencl, ...   \n",
       "22  [testdartcompiler_vmvalid_golden, testgocompil...   \n",
       "23  [rustc_hash, rustc, hashing, lib_rs, hash, fxh...   \n",
       "24  [_getbusytimesfromteamlimits, refactors, booki...   \n",
       "25  [git, repo, commit, build_source_files, dev, r...   \n",
       "26  [calendarcache, cachedcalendarservice, getcach...   \n",
       "27  [optimizations, kernels, git, optimization, op...   \n",
       "28  [rpc, metamask, metamaskwallet, endpoints, set...   \n",
       "29  [scrape_tool, batch_scrape_tool, seleniumscrap...   \n",
       "30  [testracketcompiler, testccompiler_vmvalid_gol...   \n",
       "31  [git, build_options, disable_grpc_modules, bui...   \n",
       "32  [terrainpatch, meshes, rendering, patch, patch...   \n",
       "33  [maven, azure, build_macos, build_windows, doc...   \n",
       "34  [calculate_spring_pose_bone_rotations, calcula...   \n",
       "35  [compile_lua, testluacompiler_vmvalid_golden, ...   \n",
       "36  [fixes, cacheinspired, adapters, tasksfixe, gp...   \n",
       "37  [qme_caches, memoization, caches, bench_memo, ...   \n",
       "38  [compilationfixes, compiler, compilations, rus...   \n",
       "39  [pytest, tests, testing, test_sft, test_client...   \n",
       "40  [bun_options, test_concurrency, test, transact...   \n",
       "41  [caches, github, check_python_deps, cache, doc...   \n",
       "\n",
       "                                  Representative_Docs  \n",
       "0   [feat: implement async notification and teleme...  \n",
       "1   [convert wormholecontract to sol_storage! macr...  \n",
       "2   [[alpha_factory] tighten insight bundle size c...  \n",
       "3   [add benchmark support to c++ transpiler tests...  \n",
       "4   [add vcpkg dependency caching to windows ci wo...  \n",
       "5   [improve join performance with hashed left joi...  \n",
       "6   [[core] freeze gc during cuda graph capture to...  \n",
       "7   [feat: optimize slot generation with inverted ...  \n",
       "8   [implement kv batch putmanysummary\\n\\nadd putm...  \n",
       "9   [apply aggressiveinlining attributessummary\\n\\...  \n",
       "10  [fix heavy ui updates on main threadsummary\\n\\...  \n",
       "11  [make generatedepsfile and generateruntimeconf...  \n",
       "12  [fix github api rate limiting in chess workflo...  \n",
       "13  [implement bill item dto for pharmacy income r...  \n",
       "14  [improve fortran compiler constant foldingsumm...  \n",
       "15  [improve go join compilationsummary\\n\\nimprove...  \n",
       "16  [feat: optimize duplicate checking in userealt...  \n",
       "17  [optimize trimstacktrace to use spans instead ...  \n",
       "18  [improve prolog transpilersummary\\n\\ntranspile...  \n",
       "19  [add yearly and monthly sitemap organization f...  \n",
       "20  [fix lag in pyplot example by implementing pro...  \n",
       "21  [[wip] improve dft implementationdft implement...  \n",
       "22  [improve go compiler struct reusesummary\\n\\nen...  \n",
       "23  [add database caching for folder scan results ...  \n",
       "24  [perf: optimize team bookings query by fetchin...  \n",
       "25  [optimize source file tree buildingsummary\\n\\n...  \n",
       "26  [feat: optimize calendar cache retrieval with ...  \n",
       "27  [add benchmark for deserializing large added v...  \n",
       "28  [docs: update trusted hash guide[x] analyzed r...  \n",
       "29  [implement retry-after header handling for imp...  \n",
       "30  [update c backend to use stack arrayssummary\\n...  \n",
       "31  [build tag to exclude cloud archiver providers...  \n",
       "32  [optimize billboard rendering for 3d particles...  \n",
       "33  [[azure pipelines] implement  build detection ...  \n",
       "34  [spring bone: get_bone_namespring bone: get_bo...  \n",
       "35  [lua compiler improvementssummary\\n\\nmake the ...  \n",
       "36  [prebid 10: disable greedy feature by default,...  \n",
       "37  [add memoized fibonacci implementationsummary\\...  \n",
       "38  [fix memory usage by calling comemo::evict(10)...  \n",
       "39  [split slow tests into separate filessummary\\n...  \n",
       "40  [fix net server nodelay handlingsummary\\n\\nadd...  \n",
       "41  [ci: cache node setup in lint jobsummary\\n\\nup...  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_topics = topic_model.reduce_outliers(docs, topics, embeddings=embs, strategy=\"embeddings\")\n",
    "#new_topics = topic_model.reduce_outliers(docs, topics, probabilities=probs, strategy=\"probabilities\")\n",
    "topic_model.update_topics(docs, topics=new_topics,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    representation_model=representation_model,\n",
    "    ctfidf_model=ctfidf_model )\n",
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "57ba88ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-07 13:04:48,992 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-12-07 13:04:49,313 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-12-07 13:04:49,314 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-12-07 13:04:49,329 - BERTopic - Cluster - Completed ✓\n",
      "2025-12-07 13:04:49,330 - BERTopic - Representation - Extracting topics using c-TF-IDF for topic reduction.\n",
      "2025-12-07 13:04:49,366 - BERTopic - Representation - Completed ✓\n",
      "2025-12-07 13:04:49,366 - BERTopic - Topic reduction - Reducing number of topics\n",
      "2025-12-07 13:04:49,371 - BERTopic - Representation - Fine-tuning topics using representation models.\n",
      "2025-12-07 13:05:00,761 - BERTopic - Representation - Completed ✓\n",
      "2025-12-07 13:05:00,762 - BERTopic - Topic reduction - Reduced number of topics from 14 to 14\n"
     ]
    }
   ],
   "source": [
    "topic_model = BERTopic(\n",
    "    embedding_model=model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    representation_model=representation_model,\n",
    "    ctfidf_model=ctfidf_model,\n",
    "    calculate_probabilities=True,\n",
    "    top_n_words=20,\n",
    "    verbose=True,\n",
    "    nr_topics=\"auto\"\n",
    ")\n",
    "docs = data_human\n",
    "embs = embeddings_human\n",
    "\n",
    "topics, probs = topic_model.fit_transform(docs, embeddings=embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3a73dfdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Topic",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Representation",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Representative_Docs",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "7730bacf-8543-4b07-9947-c0abf89f9965",
       "rows": [
        [
         "0",
         "-1",
         "58",
         "-1_fixes_dotnet_implementation_benchmark",
         "['fixes', 'dotnet', 'implementation', 'benchmark', 'invoices', 'java', 'validation', 'performance', '_headerstoclone', 'optimization']",
         "['reduce http headers validation overheadwhen adding/reading headers where we don\\'t have a special parser, \"parsing\" only validates that there are no new lines in the value. this change special-cases this (common) case and avoids allocating the headerstoreiteminfo.\\nexisting code paths where only non-validating apis are used stay the same.\\noverhead for cases of reading headers with validation, where they were added without validation, and we do have a known parser (still common) is minimal and an acceptable tradeoff imo (extra branch).\\n| method          | toolchain | mean      | error    | ratio | allocated | alloc ratio |\\n|---------------- |---------- |----------:|---------:|------:|----------:|------------:|\\n| add             | main      |  44.31 ns | 0.386 ns |  1.00 |      32 b |        1.00 |\\n| add             | pr        |  21.35 ns | 0.018 ns |  0.48 |         - |        0.00 |\\n|                 |           |           |          |       |           |             |\\n| addenumerable   | main      |  36.27 ns | 0.592 ns |  1.00 |      32 b |        1.00 |\\n| addenumerable   | pr        |  28.80 ns | 0.265 ns |  0.79 |         - |        0.00 |\\n|                 |           |           |          |       |           |             |\\n| getvalues       | main      |  92.39 ns | 0.302 ns |  1.00 |      64 b |        1.00 |\\n| getvalues       | pr        |  40.35 ns | 0.090 ns |  0.44 |      32 b |        0.50 |\\n|                 |           |           |          |       |           |             |\\n| addandgetvalues | main      |  94.01 ns | 0.192 ns |  1.00 |      64 b |        1.00 |\\n| addandgetvalues | pr        |  42.92 ns | 0.229 ns |  0.46 |      32 b |        0.50 |\\n|                 |           |           |          |       |           |             |\\n| cloneheaders    | main      | 905.95 ns | 1.891 ns |  1.00 |    1112 b |        1.00 |\\n| cloneheaders    | pr        | 490.74 ns | 1.064 ns |  0.54 |     600 b |        0.54 |\\n\\nbenchmark code\\n\\n```c#\\nbenchmarkrunner.run(args: args);\\n\\n[memorydiagnoser(false)]\\npublic class headersbench\\n{\\n    private readonly  <URL>  _headers = new  <URL> \\n    private readonly  <URL>  _headerstoclone = new  <URL> \\n    private readonly string[] _fooasarray = [\"foo\"];\\n\\n    public headersbench()\\n    {\\n        _headerstoclone.tryaddwithoutvalidation(\"priority\", \"u=0, i\");\\n        _headerstoclone.tryaddwithoutvalidation(\"sec-ch-ua-mobile\", \"?0\");\\n        _headerstoclone.tryaddwithoutvalidation(\"sec-ch-ua-platform\", \"\\\\\"windows\\\\\"\");\\n        _headerstoclone.tryaddwithoutvalidation(\"sec-fetch-dest\", \"document\");\\n        _headerstoclone.tryaddwithoutvalidation(\"sec-fetch-mode\", \"navigate\");\\n        _headerstoclone.tryaddwithoutvalidation(\"sec-fetch-site\", \"none\");\\n        _headerstoclone.tryaddwithoutvalidation(\"sec-fetch-user\", \"?1\");\\n        _headerstoclone.tryaddwithoutvalidation(\"upgrade-insecure-requests\", \"1\");\\n    }\\n\\n    [benchmark]\\n    public void add()\\n    {\\n        _headers.add(\"x-custom\", \"foo\");\\n        _headers.clear();\\n    }\\n\\n    [benchmark]\\n    public void addenumerable()\\n    {\\n        _headers.add(\"x-custom\", _fooasarray);\\n        _headers.clear();\\n    }\\n\\n    [benchmark]\\n    public object getvalues()\\n    {\\n        _headers.tryaddwithoutvalidation(\"x-custom\", \"foo\");\\n        ienumerable values = _headers.getvalues(\"x-custom\");\\n        _headers.clear();\\n        return values;\\n    }\\n\\n    [benchmark]\\n    public object addandgetvalues()\\n    {\\n        _headers.add(\"x-custom\", \"foo\");\\n        ienumerable values = _headers.getvalues(\"x-custom\");\\n        _headers.clear();\\n        return values;\\n    }\\n\\n    [benchmark]\\n    public  <URL>  cloneheaders()\\n    {\\n         <URL>  newheaders = new  <URL> \\n\\n        foreach (keyvaluepair> header in _headerstoclone)\\n        {\\n            newheaders.add(header.key, header.value);\\n        }\\n\\n        return newheaders;\\n    }\\n}\\n```', 'optimize invoices page to load only invoices needing approval by defaultoptimize invoices page to load only invoices needing approval by default\\nsummary\\nthis pr optimizes the invoices page performance by implementing default status filtering, similar to how the documents page loads only \"awaiting signature\" documents by default. the invoices page now loads only invoices with \"received\" and \"approved\" status by default, significantly reducing the initial dataset and improving page load times.\\nchanges made\\n\\nadded localstorage-based status filtering using the same pattern as the documents page\\ndefault filter shows only received and approved invoices (both map to \"awaiting approval\" in the ui)\\ntoggle functionality allows users to switch between \"show awaiting approval only\" and \"show all invoices\"\\ntype-safe implementation using proper typescript types and zod schema validation\\npreserved existing functionality including admin/contractor access controls\\n\\ntechnical details\\n\\nmodified frontend/app/invoices/page.tsx to add status filtering logic\\nadded invoicestatusfilterschema using zod with proper enum validation\\nimplemented localstorage persistence for user filter preferences\\nadded toggle button in the datatable actions for administrators\\nused existing trpc.invoices.list query with status parameter (no backend changes needed)\\n\\nperformance impact\\n\\nreduced initial load time by limiting the default dataset to only invoices needing approval\\nmaintained user flexibility by allowing access to all invoices via toggle\\nimproved user experience by showing the most relevant invoices first (similar to documents page)\\n\\ntesting transparency\\nwhat i actually checked\\n code linting passed - all typescript and eslint checks passed successfully\\n code patterns verified - implementation follows the exact same pattern used in the documents page\\n type safety confirmed - proper typescript types and zod schema validation implemented\\n import validation - all required imports (invoicestatuses from @/db/enums) are correctly added\\n query structure verified - the existing trpc.invoices.list query already supports status filtering\\n ui integration confirmed - toggle button properly integrated into existing datatable actions  \\nwhat i did not check\\n browser functionality - could not test the actual page behavior due to database migration issues in local dev environment\\n filter toggle behavior - could not verify the toggle button works correctly in the browser\\n localstorage persistence - could not test that filter preferences are properly saved and restored\\n performance improvement - could not measure actual load time improvements  \\nenvironment issues encountered\\n\\nlocal development server (bin/dev) failed due to database migration error: pg::undefinedtable: error: table \"board_consents\" does not exist\\nthis prevented browser testing but does not affect the frontend code changes\\n\\nreview checklist for human reviewer\\nplease verify the following when testing:\\n critical functionality to test:\\n- [ ] navigate to /invoices and verify only \"received\" and \"approved\" invoices are shown by default\\n- [ ] click the toggle button and verify it switches between filtered and all invoices views\\n- [ ] refresh the page and verify the filter preference is remembered via localstorage\\n- [ ] test both admin and contractor user roles to ensure access controls still work\\n- [ ] verify existing invoice actions (approve, reject, etc.) still function correctly\\n performance to verify:\\n- [ ] page loads faster with fewer invoices initially displayed\\n- [ ] no regressions in existing invoice functionality\\nlink to  run\\n <URL> \\nrequested by: sahil.lavingia@gmail.com\\nnotes\\nthis change addresses the performance issues mentioned in the slack thread where the invoices page was \"super slow to resolve\" by reducing the initial dataset size, similar to the successful approach used on the documents page.', 'feat: optimize slot calculation performance for team event typesoptimize slot calculation performance with binary search algorithm\\nsummary\\nthis pr addresses the performance bottleneck in cal.com\\'s team event scheduling where loading 4 weeks of data takes 5-7.5 seconds instead of the expected 2 seconds. the root cause was an o(n²) linear search through slot boundaries during slot generation.\\nkey changes:\\n- binary search optimization: replaced linear search with binary search in buildslotswithdateranges() function, reducing time complexity from o(n²) to o(n log n)\\n- caching mechanism: added sortedboundariescache with boundariescachevalid flag to avoid redundant sorting operations\\n- comprehensive test suite: added 4 new stress tests with exact slot value validation to verify algorithmic correctness across 2000+ overlapping date ranges\\n- performance validation: demonstrated 20% performance improvement (209.5ms → 167.5ms) on intensive stress tests\\nthe optimization specifically targets scenarios with overlapping availability windows (common in team scheduling) where multiple slot boundaries need to be checked during generation.\\nreview & testing checklist for human\\n critical - 5 items\\n\\n[ ] verify binary search logic: manually trace through the while loop in lines 98-109 of slots.ts with test data to ensure boundary conditions are correct and no off-by-one errors exist\\n[ ] test with production data: run the optimization against real cal.com team event data to verify no scheduling regressions occur in complex scenarios (different timezones, various event lengths, team availability patterns)  \\n[ ] cache invalidation verification: confirm that boundariescachevalid flag is properly managed - especially verify it\\'s set to false on line 132 when new boundaries are added\\n[ ] performance measurement: use actual cal.com 4-week data loads to confirm the performance improvement from 5-7.5s to closer to 2s target\\n[ ] algorithmic correctness: run the new stress tests on both main branch and this branch to verify identical slot generation results (i tested this, but independent verification is critical)\\n\\nrecommended test plan:\\n1. create a team event with 3-4 team members having overlapping but slightly offset availability\\n2. load 4 weeks of scheduling data and measure load time\\n3. verify generated slots match exactly between old and new algorithms\\n4. test edge cases: single team member, no overlapping availability, complex timezone scenarios\\n\\ndiagram\\n```mermaid\\ngraph td\\n    a[\"packages/lib/slots.ts\"]:::major-edit --> b[\"buildslotswithdateranges()\"]\\n    b --> c[\"binary search logic\\\\nlines 98-109\"]:::major-edit\\n    b --> d[\"cache management\\\\nsortedboundariescache\"]:::major-edit\\ne[\"packages/lib/slots.test.ts\"]:::major-edit --> f[\"4 new stress tests\"]\\nf --> g[\"exact slot validation\"]:::major-edit\\nf --> h[\"2000 overlapping ranges\"]:::major-edit\\nf --> i[\"performance comparison\"]:::major-edit\\n\\nj[\"packages/trpc/server/routers/viewer/slots/util.ts\"]:::context\\nk[\"team event scheduling\"]:::context --> a\\n\\nsubgraph legend\\n    l1[\"major edit\"]:::major-edit\\n    l2[\"minor edit\"]:::minor-edit  \\n    l3[\"context/no edit\"]:::context\\nend\\n\\nclassdef major-edit fill:#90ee90\\nclassdef minor-edit fill:#87ceeb\\nclassdef context fill:#ffffff\\n\\n```\\nnotes\\n\\nbackward compatibility: the optimization only activates when slotboundaries.size > 0, so scenarios without boundary conflicts continue using the original logic path\\nperformance scope: this optimization specifically targets the slot boundary checking bottleneck identified in team event scheduling, not database or api call performance\\ntest coverage: new tests include predictable overlapping ranges with exact expected slot values to catch any algorithmic differences between linear and binary search implementations\\nrisk mitigation: all existing tests continue to pass, and new stress tests validate correctness with intensive boundary scenarios that exercise the optimization code paths\\n\\nlink to  run:  <URL> ']"
        ],
        [
         "1",
         "0",
         "41",
         "0_github_lint_fixes_mlflow",
         "['github', 'lint', 'fixes', 'mlflow', 'changeset', 'pnpm', 'patch', 'documentation', 'repository', 'issues']",
         "['swap doc preview and test steps to view the preview faster (#15504) devtools \\n\\n\\n[![open in github codespaces]( <URL> \\n\\n#### install mlflow from this pr\\n\\n```\\n# mlflow\\npip install git+ <URL> \\n# mlflow-skinny\\npip install git+ <URL> \\n```\\n\\nfor databricks, use the following command:\\n\\n```\\n%sh curl -lssf  <URL>  | sh -s 15509\\n```\\n\\n\\n\\nrelated issues/prs\\n uncomment \\'resolve\\' if this pr can close the linked items. \\nclose #15504 \\nwhat changes are proposed in this pull request?\\nswaps the steps in .circleci/config.yml to upload the docs artifacts before running example tests, enabling faster doc previews.\\n please fill in changes proposed in this pr. \\nhow is this pr tested?\\n\\n[ ] existing unit/integration tests\\n[ ] new unit/integration tests\\n[x] manual tests\\n\\n attach code, screenshot, video used for manual testing here. \\ndoes this pr require documentation update?\\n\\n[x] no. you can skip the rest of this section.\\n[ ] yes. i\\'ve updated:\\n[ ] examples\\n[ ] api references\\n[ ] instructions\\n\\nrelease notes\\nis this a user-facing change?\\n\\n[x] no. you can skip the rest of this section.\\n[ ] yes. give a description of this change to be included in the release notes for mlflow users.\\n\\n details in 1-2 sentences. you can just refer to another pr with a description if this pr is part of a larger change. \\nwhat component(s), interfaces, languages, and integrations does this pr affect?\\ncomponents\\n\\n[ ] area/artifacts: artifact stores and artifact logging\\n[x] area/build: build and test infrastructure for mlflow\\n[ ] area/deployments: mlflow deployments client apis, server, and third-party deployments integrations\\n[x] area/docs: mlflow documentation pages\\n[ ] area/examples: example code\\n[ ] area/model-registry: model registry service, apis, and the fluent client calls for model registry\\n[ ] area/models: mlmodel format, model serialization/deserialization, flavors\\n[ ] area/recipes: recipes, recipe apis, recipe configs, recipe templates\\n[ ] area/projects: mlproject format, project running backends\\n[ ] area/scoring: mlflow model server, model deployment tools, spark udfs\\n[ ] area/server-infra: mlflow tracking server backend\\n[ ] area/tracking: tracking service, tracking client apis, autologging\\n\\ninterface\\n\\n[ ] area/uiux: front-end, user experience, plotting, javascript, javascript dev server\\n[ ] area/docker: docker use across mlflow\\'s components, such as mlflow projects and mlflow models\\n[ ] area/sqlalchemy: use of sqlalchemy in the tracking service or model registry\\n[ ] area/windows: windows support\\n\\nlanguage\\n\\n[ ] language/r: r apis and clients\\n[ ] language/java: java apis and clients\\n[ ] language/new: proposals for new client languages\\n\\nintegrations\\n\\n[ ] integrations/azure: azure and azure ml integrations\\n[ ] integrations/sagemaker: sagemaker integrations\\n[ ] integrations/databricks: databricks integrations\\n\\n\\ninsert an empty named anchor here to allow jumping to this section with a fragment url\\n(e.g.  <URL> \\nnote that github prefixes anchor names in markdown with \"user-content-\".\\n\\n\\nhow should the pr be classified in the release notes? choose one:\\n\\n[x] rn/none - no description will be included. the pr will be mentioned only by the pr number in the \"small bugfixes and documentation updates\" section\\n[ ] rn/breaking-change - the pr will be mentioned in the \"breaking changes\" section\\n[ ] rn/feature - a new user-facing feature worth mentioning in the release notes\\n[ ] rn/bug-fix - a user-facing bug fix worth mentioning in the release notes\\n[ ] rn/documentation - a user-facing documentation change worth mentioning in the release notes\\n\\nshould this pr be included in the next patch release?\\nyes should be selected for bug fixes, documentation updates, and other small changes. no should be selected for new features and larger changes. if you\\'re unsure about the release classification of this pr, leave this unchecked to let the maintainers decide.\\n\\nwhat is a minor/patch release?\\n\\n- minor release: a release that increments the second part of the version number (e.g., 1.2.0 -> 1.3.0).\\n  bug fixes, doc updates and new features usually go into minor releases.\\n- patch release: a release that increments the third part of the version number (e.g., 1.2.0 -> 1.2.1).\\n  bug fixes and doc updates usually go into patch releases.\\n\\n\\n patch \\n\\n[ ] yes (this pr will be cherry-picked and included in the next patch release)\\n[x] no (this pr will be included in the next minor release)', 'clean up init file for some genai flavors devtools \\n\\n\\n[![open in github codespaces]( <URL> \\n\\n#### install mlflow from this pr\\n\\n```\\n# mlflow\\npip install git+ <URL> \\n# mlflow-skinny\\npip install git+ <URL> \\n```\\n\\nfor databricks, use the following command:\\n\\n```\\n%sh curl -lssf  <URL>  | sh -s 15481\\n```\\n\\n\\n\\nwhat changes are proposed in this pull request?\\nthe __init__ file for flavors are massive. this is ok, but becomes a challenge for the lightweight tracing sdk, because doing mlflow..autolog() imports everything defined in the mlflow//__init__.py, including various model logging dependencies.\\non the other hand, a good example is dspy flavor, which organize code well and keep __init__.py clean.\\nthis pr moves model logging code from __init__.py to model.py so we can later choose to import or not inside the __init__.py. this only updates three flavors that are problematic for tracing sdk: , langchain, and llama_index, but we can apply same to other flavors later.\\nnote: this pr targets to mlflow-3 branch rather than the feature branch, because this will cause huge merge conflict otherwise.\\nhow is this pr tested?\\n\\n[x] existing unit/integration tests\\n[ ] new unit/integration tests\\n[ ] manual tests\\n\\n attach code, screenshot, video used for manual testing here. \\ndoes this pr require documentation update?\\n\\n[x] no. you can skip the rest of this section.\\n[ ] yes. i\\'ve updated:\\n[ ] examples\\n[ ] api references\\n[ ] instructions\\n\\nrelease notes\\nis this a user-facing change?\\n\\n[ ] no. you can skip the rest of this section.\\n[ ] yes. give a description of this change to be included in the release notes for mlflow users.\\n\\n details in 1-2 sentences. you can just refer to another pr with a description if this pr is part of a larger change. \\nwhat component(s), interfaces, languages, and integrations does this pr affect?\\ncomponents\\n\\n[ ] area/artifacts: artifact stores and artifact logging\\n[ ] area/build: build and test infrastructure for mlflow\\n[ ] area/deployments: mlflow deployments client apis, server, and third-party deployments integrations\\n[ ] area/docs: mlflow documentation pages\\n[ ] area/examples: example code\\n[ ] area/model-registry: model registry service, apis, and the fluent client calls for model registry\\n[ ] area/models: mlmodel format, model serialization/deserialization, flavors\\n[ ] area/recipes: recipes, recipe apis, recipe configs, recipe templates\\n[ ] area/projects: mlproject format, project running backends\\n[ ] area/scoring: mlflow model server, model deployment tools, spark udfs\\n[ ] area/server-infra: mlflow tracking server backend\\n[x] area/tracking: tracking service, tracking client apis, autologging\\n\\ninterface\\n\\n[ ] area/uiux: front-end, user experience, plotting, javascript, javascript dev server\\n[ ] area/docker: docker use across mlflow\\'s components, such as mlflow projects and mlflow models\\n[ ] area/sqlalchemy: use of sqlalchemy in the tracking service or model registry\\n[ ] area/windows: windows support\\n\\nlanguage\\n\\n[ ] language/r: r apis and clients\\n[ ] language/java: java apis and clients\\n[ ] language/new: proposals for new client languages\\n\\nintegrations\\n\\n[ ] integrations/azure: azure and azure ml integrations\\n[ ] integrations/sagemaker: sagemaker integrations\\n[ ] integrations/databricks: databricks integrations\\n\\n\\ninsert an empty named anchor here to allow jumping to this section with a fragment url\\n(e.g.  <URL> \\nnote that github prefixes anchor names in markdown with \"user-content-\".\\n\\n\\nhow should the pr be classified in the release notes? choose one:\\n\\n[x] rn/none - no description will be included. the pr will be mentioned only by the pr number in the \"small bugfixes and documentation updates\" section\\n[ ] rn/breaking-change - the pr will be mentioned in the \"breaking changes\" section\\n[ ] rn/feature - a new user-facing feature worth mentioning in the release notes\\n[ ] rn/bug-fix - a user-facing bug fix worth mentioning in the release notes\\n[ ] rn/documentation - a user-facing documentation change worth mentioning in the release notes\\n\\nshould this pr be included in the next patch release?\\nyes should be selected for bug fixes, documentation updates, and other small changes. no should be selected for new features and larger changes. if you\\'re unsure about the release classification of this pr, leave this unchecked to let the maintainers decide.\\n\\nwhat is a minor/patch release?\\n\\n- minor release: a release that increments the second part of the version number (e.g., 1.2.0 -> 1.3.0).\\n  bug fixes, doc updates and new features usually go into minor releases.\\n- patch release: a release that increments the third part of the version number (e.g., 1.2.0 -> 1.2.1).\\n  bug fixes and doc updates usually go into patch releases.\\n\\n\\n patch \\n\\n[ ] yes (this pr will be cherry-picked and included in the next patch release)\\n[x] no (this pr will be included in the next minor release)', 'refactor: optimize react component rendering and improve robustnessgrey screen fix, jules experiment\\nthis commit introduces several changes aimed at improving ui rendering performance and overall robustness, potentially addressing \"grey screen\" or unresponsiveness issues.\\n\\n\\noptimized memoization for chat rows:\\n\\nreplaced generic deepequal with custom comparison functions for react.memo in chatrow.tsx and browsersessionrow.tsx. these custom functions perform more targeted comparisons of props, focusing only on fields relevant to rendering, which should reduce the overhead of memoization and prevent unnecessary re-renders.\\nthe internal chatrowcontentcomponent in chatrow.tsx was also wrapped with react.memo.\\n\\n\\n\\nincreased robustness:\\n\\nadded try-catch blocks around json.parse calls within browsersessionrow.tsx to prevent runtime errors from malformed json in message text.\\n\\n\\n\\ncode analysis confirmations:\\n\\nanalysis of chatview.tsx indicated that its useeffect dependency arrays were already in a reasonably optimized state.\\nreview of clineprovider.ts confirmed that its dispose method is comprehensive and correctly wired to the ondiddispose event of webview panels, ensuring cleanup of tab-specific provider instances.\\nreview of shadowcheckpointservice.ts confirmed that the renamenestedgitrepos method and its usage in stageall include appropriate try...catch and try...finally blocks for robust handling of file system operations.\\n\\n\\n\\nthese changes collectively aim to make the ui more efficient and the extension more stable.\\n\\nthank you for contributing to roo code!\\n\\nbefore submitting your pr, please ensure:\\n- it\\'s linked to an approved github issue.\\n- you\\'ve reviewed our [contributing guidelines](../contributing.md).\\n\\nrelated github issue\\n every pr must be linked to an approved issue. \\ncloses: #  replace with the issue number, e.g., closes: #123 \\ndescription\\n\\nbriefly summarize the changes in this pr and how they address the linked issue.\\nthe issue should cover the \"what\" and \"why\"; this section should focus on:\\n- the \"how\": key implementation details, design choices, or trade-offs made.\\n- anything specific reviewers should pay attention to in this pr.\\n\\ntest procedure\\n\\ndetail the steps to test your changes. this helps reviewers verify your work.\\n- how did you test this specific implementation? (e.g., unit tests, manual testing steps)\\n- how can reviewers reproduce your tests or verify the fix/feature?\\n- include relevant testing environment details if applicable.\\n\\ntype of change\\n mark all applicable boxes with an \\'x\\'. \\n\\n[ ]  bug fix: non-breaking change that fixes an issue.\\n[ ]  new feature: non-breaking change that adds functionality.\\n[ ]  breaking change: fix or feature that would cause existing functionality to not work as expected.\\n[ ]  refactor: code change that neither fixes a bug nor adds a feature.\\n[ ]  style: changes that do not affect the meaning of the code (white-space, formatting, etc.).\\n[ ]  documentation: updates to documentation files.\\n[ ]  build/ci: changes to the build process or ci configuration.\\n[ ]  chore: other changes that don\\'t modify src or test files.\\n\\npre-submission checklist\\n go through this checklist before marking your pr as ready for review. \\n\\n[ ] issue linked: this pr is linked to an approved github issue (see \"related github issue\" above).\\n[ ] scope: my changes are focused on the linked issue (one major feature/fix per pr).\\n[ ] self-review: i have performed a thorough self-review of my code.\\n[ ] code quality:\\n[ ] my code adheres to the project\\'s style guidelines.\\n[ ] there are no new linting errors or warnings (npm run lint).\\n[ ] all debug code (e.g., console.log) has been removed.\\n\\n\\n[ ] testing:\\n[ ] new and/or updated tests have been added to cover my changes.\\n[ ] all tests pass locally (npm test).\\n[ ] the application builds successfully with my changes.\\n\\n\\n[ ] branch hygiene: my branch is up-to-date (rebased) with the main branch.\\n[ ] documentation impact: i have considered if my changes require documentation updates (see \"documentation updates\" section below).\\n[ ] changeset: a changeset has been created using npm run changeset if this pr includes user-facing changes or dependency updates.\\n[ ] contribution guidelines: i have read and agree to the contributor guidelines.\\n\\nscreenshots / videos\\n\\nfor ui changes, please provide before-and-after screenshots or a short video of the *actual results*.\\nthis greatly helps in understanding the visual impact of your changes.\\n\\ndocumentation updates\\n\\ndoes this pr necessitate updates to user-facing documentation?\\n- [ ] no documentation updates are required.\\n- [ ] yes, documentation updates are required. (please describe what needs to be updated or link to a pr in the docs repository).\\n\\nadditional notes\\n add any other context, questions, or information for reviewers here. \\nget in touch\\n\\nplease provide your discord username for reviewers or maintainers to reach you if they have questions about your pr']"
        ],
        [
         "2",
         "1",
         "35",
         "1_cuda_optimisations_bugfixes_fix",
         "['cuda', 'optimisations', 'bugfixes', 'fix', 'optimisation', 'cache', 'tensorwide', 'gpu', 'implement', 'improve']",
         "['disable cache on ci on windows because downloading the cache takes a super long time', 'implement tx rate controller targeting given bps', 'perf: common sub-expression elimination, cast flatten rules']"
        ],
        [
         "3",
         "2",
         "24",
         "2_coderabbit_chatgpt_refactor_release",
         "['coderabbit', 'chatgpt', 'refactor', 'release', 'docker', 'nginx', 'applescript', 'readme', 'yml', 'api']",
         "['grida canvas - skia-safe rust backend - standalone performance testing <URL> \\n <URL> \\n <URL> \\n <URL> \\n\\n\\nrelated:\\n-  <URL> \\n-  <URL> \\n this is an auto-generated comment: release notes by coderabbit.ai \\nsummary by coderabbit\\n\\nnew features\\nintroduced a high-performance, real-time 2d rendering engine with comprehensive scene graph support, geometry caching, and advanced rendering optimizations.\\nadded extensive support for vector graphics, shapes, text, gradients, images, and effects such as blur and shadows.\\nimplemented asynchronous image and font loading with caching, and support for web fonts.\\nenabled figma and json file import for scene creation.\\nprovided interactive demo applications and benchmarking examples for rendering performance and resource management.\\n\\nadded a modular math utilities library for geometry, layout, color, and rasterization.\\n\\n\\ndocumentation\\n\\n\\nadded detailed readme files, architectural overviews, and optimization strategy documents for both engine and math libraries.\\n\\n\\ntests\\n\\n\\nincluded comprehensive unit and integration tests for geometry, rendering, resource loading, hit testing, and performance benchmarks.\\n\\n\\nchores\\n\\nadded configuration files for rust workspaces, docker, makefile automation, and package management.\\n\\n end of auto-generated comment: release notes by coderabbit.ai', 'inplace isq support and default to mmapisq update to support models that don\\'t fit in system ram, instead quantizing them as they are loaded.\\n this is an auto-generated comment: release notes by coderabbit.ai \\nsummary by coderabbit\\n\\nnew features\\nintroduced a global, thread-safe mechanism for setting and retrieving immediate in-situ quantization (isq) configuration.\\nadded support for applying isq to model layers immediately upon creation when configured.\\nadded predicate-based filtering for sharded tensor access to control accessible tensors more granularly.\\nadded measurement and reporting of \"time to first token\" (ttft) in interactive text and vision modes.\\n\\nextended model loaders with new immediate isq predicate methods for enhanced quantization targeting.\\n\\n\\nbug fixes\\n\\nimproved handling of buffer offsets in quantization and dequantization operations for correct data alignment.\\n\\ncorrected regex patterns for self-attention output projections in multiple model loaders.\\n\\n\\nrefactor\\n\\nsimplified and clarified isq flag logic and quantization setup during model loading.\\nenhanced tensor loading logic to explicitly handle cases with and without lora paths.\\nupdated layer constructors to conditionally apply immediate isq after creation, improving quantization consistency.\\nparallelized model layer loading across numerous architectures using rayon, significantly improving initialization performance.\\nintegrated progress bars with parallel iteration for better feedback during concurrent operations.\\nimproved error handling in model layer construction by replacing panics with proper error propagation in several models.\\nmoved logging utilities for one-time info/warn messages to a shared quantization utility module.\\n\\n end of auto-generated comment: release notes by coderabbit.ai', 'full fix and updatethis is for connecting ai agents/ desktop//etc to a locally hosted n8n instance on docker desktop. this update also further implements other features, such as connecting nodes and adding proper nodes, creating and deleting workflows, updating credentials, updating folders, and so much more. i use the api/v1/docs to parse the entire api setup for the locally hosted n8n.\\n this is an auto-generated comment: release notes by coderabbit.ai \\nsummary by coderabbit\\n\\nnew features\\nadded a comprehensive ai-powered outlook email assistant workflow automating email ingestion, ai classification, response generation, storage, analytics, and notifications.\\nintroduced a complete postgresql database schema supporting advanced email processing, analytics, conversation tracking, error logging, and user preferences.\\nenhanced the n8n node catalog with detailed metadata, examples, search, and recommendation capabilities for ai agents and workflow automation.\\nadded new, fully described node definitions for popular integrations including airtable, asana, bitbucket, box, copper, clickup,  ai, code, discord, dropbox, facebook, and github.\\nimplemented a robust node discovery system, production-ready node registry, and real-time search engine for all nodes.\\nprovided a detailed ai agent guidance module, usage patterns, and best practices for workflow creation.\\nintroduced scripts for automated node verification, fixing, optimization, and comprehensive testing.\\nadded performance optimizations: smart caching, connection pooling, real-time monitoring, and enhanced error handling.\\n\\nsupplied quick setup guides, workflow documentation, and project status reports for production deployment.\\n\\n\\nbug fixes\\n\\nautomated scripts to identify and fix typescript, formatting, and structural issues across all node files.\\n\\nresolved duplicate imports/exports, missing properties, and syntax errors in node definitions and registries.\\n\\n\\ndocumentation\\n\\nadded extensive user, agent, and setup guides, ai optimization summaries, and troubleshooting documentation.\\n\\nupdated workflow and api documentation examples for clarity and accuracy.\\n\\n\\nchores\\n\\nintroduced scripts for registry generation, node discovery, batch fixes, validation, and production readiness checks.\\n\\nimproved .gitignore to protect sensitive files and development artifacts.\\n\\n\\nstyle\\n\\n\\nstandardized node definition formats, naming conventions, and metadata for consistency and searchability.\\n\\n\\ntests\\n\\nadded automated test suites for node validation, server functionality, and mcp protocol compliance.\\ngenerated detailed test and validation reports confirming 100% success and production readiness.\\n\\n end of auto-generated comment: release notes by coderabbit.ai']"
        ],
        [
         "4",
         "3",
         "16",
         "3_linqfixes_linq_compiling_runtime",
         "['linqfixes', 'linq', 'compiling', 'runtime', 'concurrent', 'testing', 'performancechanged', 'cppcheck', 'tester', 'benchmarkthis']",
         "['add comprehensive vibetunnel protocol benchmark toolfeatures:\\n- complete http api client implementation for vibetunnel protocol\\n- session management benchmarks (create/get/list/delete operations)\\n- sse streaming performance testing with latency measurements\\n- concurrent user load testing with realistic simulation\\n- support for custom hostname/port configuration\\n- detailed performance statistics and reporting\\ncommands:\\n- session: test session lifecycle performance\\n- stream: benchmark sse streaming latency/throughput\\n- load: concurrent user load testing\\ntested against both go (port 4031) and rust (port 4044) servers. tool successfully creates sessions and measures performance metrics.\\n generated with  code', 'enable parallel c++ compilationdescription\\nallow compiling multiple c++ sources in parallel.\\ntype of change\\n\\nnew feature (non-breaking change which adds functionality)\\n\\nwhy\\nbuild times in rnw have jumped to between 35 minutes and over 60 minutes in continuous integration.\\nwhat\\n\\ndefault msbuild property multiproccl to true.\\n\\ninitial tests (cached nuget packages, clean build and target directories) showed the following results on a windows dev box:\\n- multiproccl = false: ~37 minutes\\n- multiproccl = true: ~6 minutes\\nabout 82% speed-up.\\nnote\\nshould this setting apply to continuous integration?\\\\\\nwhile it would drastically improve build times, it may compromise predictability and reproducibility of builds.\\n###### microsoft reviewers: open in codeflow', 'test-tp: reference assembly loading fixesi noticed that with the current reference loading of typeproviders, if i had 139 reference assemblies (in a solution memory), i ended up calling the assembly.load for 897 times. this is because so many assemblies have the same references like system.memory, system.xml, system.buffers, system.threading.tasks.extensions, ... and the code said \"load all reference assemblies\". simple fix: check already loaded reference assemblies before trying to call the slow assembly.load again.\\nthe sourceassembliestable_ is a concurrentdictionary to ensure thread-safety. however, instead of code using it in thread-safe way, it was used by double-lookup. so that is fixed to actually use it properly. (it\\'s role is to be used as a guard to sourceassemblies_ array, which is manually lazy-loaded from the queue.)\\nthese changes match the fsharp.typeprovider.sdk merged pr.']"
        ],
        [
         "5",
         "4",
         "14",
         "4_builtins_bun_tests_typescript",
         "['builtins', 'bun', 'tests', 'typescript', 'builtin', 'debug', 'cjs', 'test', 'posix', 'binary64']",
         "[\"report memory cost of sourcemaps to gcwhat does this pr do?\\n **please explain what your changes do**, example: \\n\\n\\nthis adds a new flag --bail to bun test. when set, it will stop running tests after the first failure. this is useful for ci environments where you want to fail fast.\\n\\n\\n\\n[ ] documentation or typescript types (it's okay to leave the rest blank in this case)\\n[ ] code changes\\n\\nhow did you verify your code works?\\n **for code changes, please include automated tests**. feel free to uncomment the line below \\n i wrote automated tests \\n if javascript/typescript modules or builtins changed:\\n\\n- [ ] i included a test for the new code, or existing tests cover it\\n- [ ] i ran my tests locally and they pass (`bun-debug test test-file-name.test`)\\n\\n\\n if zig files changed:\\n\\n- [ ] i checked the lifetime of memory allocated to verify it's (1) freed and (2) only freed when it should be\\n- [ ] i included a test for the new code, or an existing test covers it\\n- [ ] jsvalue used outside outside of the stack is either wrapped in a jsc.strong or is jsvalueprotect'ed\\n- [ ] i wrote typescript/javascript tests and they pass locally (`bun-debug test test-file-name.test`)\\n\\n if new methods, getters, or setters were added to a publicly exposed class:\\n\\n- [ ] i added typescript types for the new methods, getters, or setters\\n\\n if dependencies in tests changed:\\n\\n- [ ] i made sure that specific versions of dependencies are used instead of ranged or tagged versions\\n\\n if a new builtin esm/cjs module was added:\\n\\n- [ ] i updated aliases in `module_loader.zig` to include the new module\\n- [ ] i added a test that imports the module\\n- [ ] i added a test that require() the module\", \"avoid encoding as double in napi_create_double if possiblewhat does this pr do?\\narithmetic on numbers encoded as doubles in jsc seems to hit more slow paths compared to numbertag numbers.\\nfixes #9218\\nwe might want to do this in other places. with this change in a debug build, fps goes from ~1 to ~100 on m4 max\\n\\n **please explain what your changes do**, example: \\n\\n\\nthis adds a new flag --bail to bun test. when set, it will stop running tests after the first failure. this is useful for ci environments where you want to fail fast.\\n\\n\\nhow did you verify your code works?\\n **for code changes, please include automated tests**. feel free to uncomment the line below \\n i wrote automated tests \\n if javascript/typescript modules or builtins changed:\\n\\n- [ ] i included a test for the new code, or existing tests cover it\\n- [ ] i ran my tests locally and they pass (`bun-debug test test-file-name.test`)\\n\\n\\n if zig files changed:\\n\\n- [ ] i checked the lifetime of memory allocated to verify it's (1) freed and (2) only freed when it should be\\n- [ ] i included a test for the new code, or an existing test covers it\\n- [ ] jsvalue used outside of the stack is either wrapped in a jsc.strong or is jsvalueprotect'ed\\n- [ ] i wrote typescript/javascript tests and they pass locally (`bun-debug test test-file-name.test`)\\n\\n if new methods, getters, or setters were added to a publicly exposed class:\\n\\n- [ ] i added typescript types for the new methods, getters, or setters\\n\\n if dependencies in tests changed:\\n\\n- [ ] i made sure that specific versions of dependencies are used instead of ranged or tagged versions\\n\\n if a new builtin esm/cjs module was added:\\n\\n- [ ] i updated aliases in `module_loader.zig` to include the new module\\n- [ ] i added a test that imports the module\\n- [ ] i added a test that require() the module\", \"fix(node: <URL>  resume when reading and avoid unnecessary pause/resumes callswhat does this pr do?\\nfix:  <URL> \\n **please explain what your changes do**, example: \\n\\n\\nthis adds a new flag --bail to bun test. when set, it will stop running tests after the first failure. this is useful for ci environments where you want to fail fast.\\n\\n\\n\\n[ ] documentation or typescript types (it's okay to leave the rest blank in this case)\\n[x] code changes\\n\\nhow did you verify your code works?\\ntests\\n **for code changes, please include automated tests**. feel free to uncomment the line below \\n i wrote automated tests \\n if javascript/typescript modules or builtins changed:\\n\\n- [ ] i included a test for the new code, or existing tests cover it\\n- [ ] i ran my tests locally and they pass (`bun-debug test test-file-name.test`)\\n\\n\\n if zig files changed:\\n\\n- [ ] i checked the lifetime of memory allocated to verify it's (1) freed and (2) only freed when it should be\\n- [ ] i included a test for the new code, or an existing test covers it\\n- [ ] jsvalue used outside of the stack is either wrapped in a jsc.strong or is jsvalueprotect'ed\\n- [ ] i wrote typescript/javascript tests and they pass locally (`bun-debug test test-file-name.test`)\\n\\n if new methods, getters, or setters were added to a publicly exposed class:\\n\\n- [ ] i added typescript types for the new methods, getters, or setters\\n\\n if dependencies in tests changed:\\n\\n- [ ] i made sure that specific versions of dependencies are used instead of ranged or tagged versions\\n\\n if a new builtin esm/cjs module was added:\\n\\n- [ ] i updated aliases in `module_loader.zig` to include the new module\\n- [ ] i added a test that imports the module\\n- [ ] i added a test that require() the module\"]"
        ],
        [
         "6",
         "5",
         "13",
         "5_parser_refactor_buffer_chunksprefetch",
         "['parser', 'refactor', 'buffer', 'chunksprefetch', 'preload', 'docker', 'cache', 'remotion', 'resize', 'renderer']",
         "['@remotion/renderer: consider --memory flag from docker and warn on unrealistic valuesalso allow 50% of memory instead of just 40%', '@remotion/media-parser: ability to test remote files fast@remotion/media-parser: ability to test remote files fast', '@remotion/media-parser: refactor iso base media and matroska parser to be ready for seekingtodo: now tests / cis are freezing\\narray buffer resize is too big']"
        ],
        [
         "7",
         "6",
         "12",
         "6_torch_memory_saver_c_compiler_compiler_gemma3ntextscaledwordembedding",
         "['torch_memory_saver', 'c_compiler', 'compiler', 'gemma3ntextscaledwordembedding', 'multithreaded', 'commit', 'int8', 'serializeblock', 'gcc', 'rollout']",
         "['[mlas] dequantizelinear int8/uint8description\\n\\nadds multithreaded vectorized implementations of dequantizelinear for int8 and uint8 inputs:\\nintel sse 2\\narm neon\\nall other architectures fallback to a multithreaded scalar reference implementation (previous was not multithreaded).\\n\\nint8 dequantizelinear latency on intel core i9-10920x with 4 intra op threads (sse 2 implementation)\\n| number of elements | baseline latency (us) | multithreaded+simd latency (us) | speedup |\\n| ----------------------- | ---------------------- | ------------------------------------ | ---------- |\\n| 10 k                            | 1                               | 1                                                   | 1              |\\n| 20 k                            | 2                               | 2                                                   | 1              |\\n| 40 k                            | 5                               | 5                                                   | 1              |\\n| 80 k                            | 11                             | 4                                                   | 2.75         |\\n| 100 k                          | 14                             | 5                                                   | 2.80         |\\n| 150 k                          | 21                             | 7                                                   | 3.00         |\\n| 200 k                          | 28                             | 8                                                   | 3.50         |\\n| 400 k                          | 68                             | 15                                                 | 4.53         |\\n| 600 k                          | 107                           | 21                                                 | 5.10         |\\n| 800 k                          | 142                           | 28                                                 | 5.07         |\\n| 1 m                             | 187                           | 42                                                 | 4.45         |\\n| 2 m                             | 376                           | 102                                               | 3.69         |\\n| 4 m                             | 880                           | 236                                               | 3.73         |\\n| 6 m                             | 1547                         | 557                                               | 2.78         |\\n| 8 m                             | 2438                         | 1097                                             | 2.22         |\\n| 10 m                           | 3192                         | 1464                                             | 2.18         |\\n| 100 m                         | 38718                       | 17733                                           | 2.18         |\\nint8 dequantizelinear latency on snapdragon 8cx gen 3 @ 3.4ghz with 4 intra op threads (neon implementation)\\n| number of elements | baseline latency (us) | multithreaded+simd latency (us) | speedup |\\n| ----------------------- | ---------------------- | ------------------------------------ | ---------- |\\n| 10 k                            | 1                               | 1                                                   | 1              |\\n| 20 k                            | 1                               | 1                                                   | 1              |\\n| 40 k                            | 3                               | 3                                                   | 1              |\\n| 80 k                            | 7                               | 4                                                   | 1.75         |\\n| 100 k                          | 9                               | 3                                                   | 3.00         |\\n| 150 k                          | 14                             | 5                                                   | 2.80         |\\n| 200 k                          | 18                             | 6                                                   | 3.00         |\\n| 400 k                          | 38                             | 10                                                 | 3.80         |\\n| 600 k                          | 61                             | 15                                                 | 4.07         |\\n| 800 k                          | 76                             | 19                                                 | 4.00         |\\n| 1 m                             | 98                             | 24                                                 | 4.08         |\\n| 2 m                             | 204                           | 48                                                 | 4.25         |\\n| 4 m                             | 424                           | 112                                               | 3.79         |\\n| 6 m                             | 677                           | 384                                               | 1.76         |\\n| 8 m                             | 919                           | 621                                               | 1.48         |\\n| 10 m                           | 1132                         | 776                                               | 1.46         |\\n| 100 m                         | 11842                       | 10566                                           | 1.12         |\\nmotivation and context\\nimproves latency of quantized qdq models that with large dqs that dominate the inference latency.', \"[rollout] feat: support multi-stage awake for sglangco-authored with: mrata (immrata@gmail.com) and @zhaochenyang20 \\nchecklist before starting\\n\\n[x] search for similar pr(s).\\n\\nwhat does this pr do?\\nmotivation\\nin rl ecosystem which use colocate design like verl, we need to offload training model and load serving model & kv cache frequently.\\nbackground\\n\\ncurrently sglang is using torch_memory_saver to pause and resume.\\ntorch_memory_saver is a open source repo that provided easy to use api to hack cudamalloc and cudafree to make sure the virtual address could be consistent after pause and resume, which is critical to ensure cuda graph work.\\ncuda graph is critical to make sure sglang runs faster in decoding phases.\\n\\nhere is the current behavior of verl + sglang\\n\\n\\nduring training, we have training model and optimizer state in the gpu memory, and once training is done, we will offload optimizer state to cpu and keep the model weights in gpu, which is needed in update weight.\\nduring update weight, we awake the sglang engine, so those paused memory of model weights and kv cache will come back. then we update model from training model to serving model on the fly using the api: update_weights_in_tensor\\nafter model being updated, we delete the training model from gpu memory.\\n\\nabove design works pretty well so far, however, this would waste a big chunk of gpu memory during rollout, which could cause a few issues we've seen so far:\\n- small kv cache: we need to use relative lower number of mem fraction ratio (e.g: 0.6), hence our kv cache has less tokens. given kv cache has less tokens, we will hit runtimeerror: prefill out of memory. try to lower your batch size. when we try prefill large number of requests.\\n- out of memory: if we use mem fraction ratio 0.8 and run rl for 32b model on 8 h100, it will oom during update weight\\nchallenge\\n\\ntorch_memory_saver currently only supports singleton, hence sglang will pause and resume kv cache + weights together, they are treated as the same group of memory controlled by the singleton torch_memory_saver instance\\n\\nproposal\\n\\n\\nduring training, we do the same\\nduring update weight stage 1, we awake the model weights from sglang and then update weights\\nduring update weight stage 2, we delete the training model weights from gpu memory\\nawake the sglang's kv cache\\n\\n\\nbenefit\\nwith above feature, we can train larger model with same gpu, we can also make training/rollout more efficient given we can allocate larger kv cache\\nsolution: keep using singleton and provide tag based pause/resume\\n\\n[x] support tag based resume/pause:  <URL> \\n[x] support multiple stage awake in sglang:  <URL> \\n[ ] support multiple stage awake in verl:  <URL> \\n\\nhigh-level design\\n\\ndemonstrate the high-level design if this pr is complex.\\n\\nspecific changes\\n\\nlist the specific changes.\\n\\napi\\n\\ndemonstrate how the api changes if any.\\n\\nusage example\\n\\nprovide usage example(s) for easier usage.\\n\\n```python\\nadd code snippet or script demonstrating how to use this\\n```\\ntest\\n\\n\\nadditional info.\\n\\nissue number: fixes issue # or discussion # if any.\\ntraining: [note which backend this pr will affect: fsdp, megatron, both, or none]\\ninference: [note which backend this pr will affect: vllm, sglang, both, or none]\\n\\nchecklist before submitting\\n\\n[ ] read the contribute guide.\\n[ ] apply pre-commit checks.\\n[ ] add [breaking] to the pr title if it breaks any api.\\n[ ] update the documentation about your changes in the docs.\\n[ ] new ci unit test(s) are added to cover the code path.\\n[ ] rely on existing unit tests on ci that covers the code path.\", '[ibd] specialize block serializationthis change is part of [ibd] - tracking pr for speeding up initial block download\\n\\nthis pr is drafted until i remeasure everything after the recent merges and i need to find a way to simplify the 1 byte writes more nicely, i don\\'t like all the specializations.\\n\\nsummary\\nthis pr contain a few different optimization i found by ibd profiling, and via the newly added block seralization benchmarks. it also takes advantage of the recently merged std::span changes enabling propagating static extents.\\nthe commits merge similar (de)serialization methods, and separates them internally with  if constexpr - similarly to how it has been done here before. this enabled further sizecomputer optimizations as well.\\ncontext\\nother than these, since single byte writes are used very often (used for every (u)int8_t or std::byte or bool and for every varint\\'s first byte which is also needed for every (pre)vector), it makes sense to avoid the generalized serialization infrastructure that isn\\'t needed:\\n* autofile write doesn\\'t need to allocate 4k buffer for a single byte now;\\n* vectorwriter and datastream avoids memcpy/insert calls;\\n* csha256::write can avoid memcpy.\\ndeserializeblock is dominated by the hash calculations so the optimizations barely affect it.\\nmeasurements\\n\\nc compiler ............................ appleclang 16.0.0.16000026\\n\\n> before:\\n\\n|            ns/block |             block/s |    err% |     total | benchmark\\n|--------------------:|--------------------:|--------:|----------:|:----------\\n|          195,610.62 |            5,112.20 |    0.3% |     11.00 | `serializeblock`\\n|           12,061.83 |           82,906.19 |    0.1% |     11.01 | `sizecomputerblock`\\n\\n> after:\\n\\n|            ns/block |             block/s |    err% |     total | benchmark\\n|--------------------:|--------------------:|--------:|----------:|:----------\\n|          174,569.19 |            5,728.39 |    0.6% |     10.89 | `serializeblock`\\n|           10,241.16 |           97,645.21 |    0.0% |     11.00 | `sizecomputerblock`\\n\\n\\n\\nserializeblock - ~12.% faster\\nsizecomputerblock - ~17.7% faster\\n\\n\\n\\nc++ compiler .......................... gnu 13.3.0\\n\\n> before:\\n\\n|            ns/block |             block/s |    err% |       ins/block |       cyc/block |    ipc |      bra/block |   miss% |     total | benchmark\\n|--------------------:|--------------------:|--------:|----------------:|----------------:|-------:|---------------:|--------:|----------:|:----------\\n|          867,857.55 |            1,152.26 |    0.0% |    8,015,883.90 |    3,116,099.08 |  2.572 |   1,517,035.87 |    0.5% |     10.81 | `serializeblock`\\n|           30,928.27 |           32,332.88 |    0.0% |      221,683.03 |      111,055.84 |  1.996 |      53,037.03 |    0.8% |     11.03 | `sizecomputerblock`\\n\\n> after:\\n\\n|            ns/block |             block/s |    err% |       ins/block |       cyc/block |    ipc |      bra/block |   miss% |     total | benchmark\\n|--------------------:|--------------------:|--------:|----------------:|----------------:|-------:|---------------:|--------:|----------:|:----------\\n|          615,000.56 |            1,626.01 |    0.0% |    8,015,883.64 |    2,208,340.88 |  3.630 |   1,517,035.62 |    0.5% |     10.56 | `serializeblock`\\n|           25,676.76 |           38,945.72 |    0.0% |      159,390.03 |       92,202.10 |  1.729 |      42,131.03 |    0.9% |     11.00 | `sizecomputerblock`\\n\\n\\n\\nserializeblock - ~41.1% faster\\nsizecomputerblock - ~20.4% faster\\n\\n\\nwhile this wasn\\'t the main motivation for the change, ibd on ubuntu/gcc on ssd with i9 indicates a 2% speedup as well:\\n\\ndetails\\n\\n```bash\\ncommits=\"05314bde0b06b820225f10c6529b5afae128ff81 1cd94ec2511874ec68b92db34ad7ec7d9534fed1\"; \\\\\\nstop_height=880000; dbcache=10000; \\\\\\nc_compiler=gcc; cxx_compiler=g++; \\\\\\nhyperfine \\\\\\n--export-json \"/mnt/my_storage/ibd-${commits// /-}-${stop_height}-${dbcache}-${c_compiler}.json\" \\\\\\n--runs 3 \\\\\\n--parameter-list commit ${commits// /,} \\\\\\n--prepare \"killall bitcoind || true; rm -rf /mnt/my_storage/bitcoindata/*; git checkout {commit}; git clean -fxd; git reset --hard; cmake -b build -dcmake_build_type=release -denable_wallet=off -dcmake_c_compiler=$c_compiler -dcmake_cxx_compiler=$cxx_compiler && cmake --build build -j$(nproc) --target bitcoind && ./build/bin/bitcoind -datadir=/mnt/my_storage/bitcoindata -stopatheight=1 -printtoconsole=0 || true\" \\\\\\n--cleanup \"cp /mnt/my_storage/bitcoindata/debug.log /mnt/my_storage/logs/debug-{commit}-$(date +%s).log || true\" \\\\\\n\"compiler=$c_compiler commit={commit} ./build/bin/bitcoind -datadir=/mnt/my_storage/bitcoindata -stopatheight=$stop_height -dbcache=$dbcache -prune=550 -printtoconsole=0\"\\nbenchmark 1: compiler=gcc commit=05314bde0b06b820225f10c6529b5afae128ff81 ./build/bin/bitcoind -datadir=/mnt/my_storage/bitcoindata -stopatheight=880000 -dbcache=10000 -prune=550 -printtoconsole=0\\n  time (mean ± σ):     33647.918 s ± 508.655 s    [user: 71503.409 s, system: 4404.899 s]\\n  range (min … max):   33283.439 s … 34229.026 s    3 runs\\n\\nbenchmark 2: compiler=gcc commit=1cd94ec2511874ec68b92db34ad7ec7d9534fed1 ./build/bin/bitcoind -datadir=/mnt/my_storage/bitcoindata -stopatheight=880000 -dbcache=10000 -prune=550 -printtoconsole=0\\n  time (mean ± σ):     33062.491 s ± 183.335 s    [user: 71246.532 s, system: 4318.490 s]\\n  range (min … max):   32888.211 s … 33253.706 s    3 runs\\n\\nsummary\\n  compiler=gcc commit=1cd94ec2511874ec68b92db34ad7ec7d9534fed1 ./build/bin/bitcoind -datadir=/mnt/my_storage/bitcoindata -stopatheight=880000 -dbcache=10000 -prune=550 -printtoconsole=0 ran\\n    1.02 ± 0.02 times faster than compiler=gcc commit=05314bde0b06b820225f10c6529b5afae128ff81 ./build/bin/bitcoind -datadir=/mnt/my_storage/bitcoindata -stopatheight=880000 -dbcache=10000 -prune=550 -printtoconsole=0\\n```']"
        ],
        [
         "8",
         "7",
         "12",
         "7_refactor_workersupdateuserbalancestatscacheworker_caching_coderabbit",
         "['refactor', 'workersupdateuserbalancestatscacheworker', 'caching', 'coderabbit', 'dashboard', 'release', 'improved', 'api', 'workaround', 'fetching']",
         "[\"cached repetitive data lookups for creator analyticsbecause of how the code is structured, we create a separate creatoranalytics::web instance for every missing date range\\nthis then calls the products_for_creator_analytics method on a user, which returns a different relation each time, so query caching doesn't work\\ninstead, we can just calculate this once in the caching proxy and then pass it to the web instance\\ni'll refactor this properly in the future once the fix is confirmed good\\n\\n this is an auto-generated comment: release notes by coderabbit.ai \\nsummary by coderabbit\\n\\nperformance improvements\\nenhanced analytics performance by caching user’s first sale date and product data, resulting in faster loading times for analytics features.\\n\\n end of auto-generated comment: release notes by coderabbit.ai\", \"removed unnecessary api requests to fetch balancesrefs  <URL> \\n\\nsince the pr above, we've been including the balance/stats directly into the page instead of relying on loading them via secondary api requests\\nas a result, we're currently loading 4 different pages and not even using the result\\nsome of these requests are even just returning html because they don't have json equivalents\\nwe can do away with that and simplify the code in the process\\n\\n this is an auto-generated comment: release notes by coderabbit.ai \\nsummary by coderabbit\\n\\nrefactor\\nsimplified the statistics display by removing asynchronous fetching and progress indicators. the statistic value is now displayed directly from the provided value.\\nremoved clickable links and url-based navigation from the statistics components on the dashboard.\\nupdated font sizing and tooltip support for improved readability and usability.\\n\\n end of auto-generated comment: release notes by coderabbit.ai\", \"fix: recommended wallet listing when opening modal with namespace filterdescription\\n\\nfixes recommended wallets listing logics when opening the modal with the namespace filter: we've been showing evm wallets when opening the bitcoin modal. this was due to logic issues on how we are re-fetching the wallets when setting the namespace filter.\\noptimizes fetching wallets requests when not changing the namespace filter: we've been fetching wallets every time when we switch between namespace buttons on open / close modal, which is not good. optimises calls to fetch the wallets only when it's necessary\\n\\ntype of change\\n\\n[ ] chore (non-breaking change that addresses non-functional tasks, maintenance, or code quality improvements)\\n[x] bug fix (non-breaking change which fixes an issue)\\n[ ] new feature (non-breaking change which adds functionality)\\n[ ] breaking change (fix or feature that would cause existing functionality to not work as expected)\\n\\nassociated issues\\nfor linear issues: closes apkt-xxx\\nfor gh issues: closes #...\\nshowcase (optional)\\nif there is a ui change include the screenshots with before and after state.\\nif new feature is being introduced, include the link to demo recording.\\nchecklist\\n\\n[x] code in this pr is covered by automated tests (unit tests, e2e tests)\\n[x] my changes generate no new warnings\\n[x] i have reviewed my own code\\n[x] i have filled out all required sections\\n[x] i have tested my changes on the preview link\\n[ ] approver of this pr confirms that the changes are tested on the preview link\"]"
        ],
        [
         "9",
         "8",
         "12",
         "8_execute_gpt_4v_request_crewagentparser_jsonparser_pytest",
         "['execute_gpt_4v_request', 'crewagentparser', 'jsonparser', 'pytest', 'mockflow', 'parser', 'withfixedsizecache', 'parse_number', 'cache', 'tests']",
         "[' speed up function execute_gpt_4v_request by 100% in pr #1214 (-apikey-passthrough) this pull request contains optimizations for pr #1214\\nif you approve this dependent pr, these changes will be merged into the original pr branch -apikey-passthrough.\\n\\nthis pr will be automatically closed if the original pr is merged.\\n\\n\\n 100% (1.00x) speedup for execute_gpt_4v_request in inference/core/workflows/core_steps/models/foundation//v3.py\\n⏱ runtime :   107 milliseconds → 53.6 milliseconds (best of 5 runs)\\n explanation and details\\nhere is an optimized version of your program for runtime and memory. the majority of runtime is io/network-bound (api requests) and not cpu-bound code, so the best possible single-process cpu optimization is to avoid repeated work (e.g., repeated endpoint string formatting or client allocation) and simplify fast paths. if you can batch or async requests, that would reduce end-to-end latency, but that changes function signatures and semantics so is out of scope. here we focus on making your function as lean as possible within its expected use. \\nkey improvements:\\n- reuse  client () where possible: creating the client is surprisingly expensive per your profiling.\\n- optimize prompt and payload building: avoid unnecessary field-level assignments.\\n- use exception chaining efficiently.\\n- minimize calls to .startswith() by using a tuple form.\\n- precompute endpoint format string if possible.\\n- move non-error computations out of try/except.\\nsummary:\\n-  client creation is now cached, saving repeated cost.\\n- efficient prefix checking for  key.\\n- payloads & try/except blocks are trimmed for speed and clarity.\\n- function signatures and return values are preserved.\\n- comments are updated only where logic is improved or needs clarification.\\nif you control parallelism at a higher level, running requests in parallel (with asyncio or threading) would yield much higher throughput as both requests and  are io bound.\\n correctness verification report:\\n| test                        | status            |\\n| --------------------------- | ----------------- |\\n|  existing unit tests |  none found |\\n|  generated regression tests |  9 passed |\\n| ⏪ replay tests |  none found |\\n|  concolic coverage tests |  none found |\\n| tests coverage       |  |\\n\\n generated regression tests details\\n\\n```python\\nimport types\\nfrom typing import list, optional\\n\\n# imports\\nimport pytest  # used for our unit tests\\n# function to test\\nimport requests\\nfrom inference.core.env import api_base_url\\nfrom inference.core.workflows.core_steps.models.foundation..v3 import \\\\\\n    execute_gpt_4v_request\\nfrom  import \\nfrom ._types import not_given\\n\\n# unit tests\\n\\n\\n# --------- test helpers and monkeypatching ---------\\nclass dummyresponse:\\n    \"\"\"a dummy response object to simulate requests.response.\"\"\"\\n    def __init__(self, json_data=none, status_code=200, raise_exc=none, text=none):\\n        self._json_data = json_data or {}\\n        self.status_code = status_code\\n        self._raise_exc = raise_exc\\n        self.text = text or str(json_data)\\n    def json(self):\\n        return self._json_data\\n    def raise_for_status(self):\\n        if self._raise_exc:\\n            raise self._raise_exc\\n\\n# --------- basic test cases ---------\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\ndef test_proxied_request_missing_content(monkeypatch):\\n    \"\"\"test proxied request with missing \\'content\\' in response (should raise).\"\"\"\\n    def bad_post(url, json):\\n        return dummyresponse({\"choices\": [{\"message\": {}}]}, status_code=200)\\n    monkeypatch.setattr(requests, \"post\", bad_post)\\n    with pytest.raises(runtimeerror) as excinfo:\\n        execute_gpt_4v_request(\\n            roboflow_api_key=\"rfkey123\",\\n            _api_key=\"rf_key:account:abc\",\\n            prompt=[{\"role\": \"user\", \"content\": \"say hi\"}],\\n            gpt_model_version=\"gpt-4v\",\\n            max_tokens=10,\\n            temperature=0.5,\\n        )\\n\\ndef test_proxied_request_ <URL> \\n    \"\"\"test proxied request with http error (should raise).\"\"\"\\n    def bad_post(url, json):\\n        return dummyresponse({}, status_code=500, raise_exc=requests. <URL> \\n    monkeypatch.setattr(requests, \"post\", bad_post)\\n    with pytest.raises(runtimeerror) as excinfo:\\n        execute_gpt_4v_request(\\n            roboflow_api_key=\"rfkey123\",\\n            _api_key=\"rf_key:account:abc\",\\n            prompt=[{\"role\": \"user\", \"content\": \"say hi\"}],\\n            gpt_model_version=\"gpt-4v\",\\n            max_tokens=10,\\n            temperature=0.5,\\n        )\\n\\ndef test_direct_request_exception(monkeypatch):\\n    \"\"\"test direct request with  client raising exception (should raise).\"\"\"\\n    class failingclient:\\n        def __init__(self, api_key):\\n            pass\\n        @property\\n        def chat(self):\\n            class c:\\n                @property\\n                def completions(self):\\n                    class d:\\n                        def create(self, *a, **k):\\n                            raise exception(\" failure\")\\n                    return d()\\n            return c()\\n    monkeypatch.setattr(\".\", lambda api_key: failingclient(api_key))\\n    with pytest.raises(runtimeerror) as excinfo:\\n        execute_gpt_4v_request(\\n            roboflow_api_key=\"rfkey123\",\\n            _api_key=\"sk--002\",\\n            prompt=[{\"role\": \"user\", \"content\": \"say hi\"}],\\n            gpt_model_version=\"gpt-4v\",\\n            max_tokens=10,\\n            temperature=0.5,\\n        )\\n\\ndef test_proxied_request_index_error(monkeypatch):\\n    \"\"\"test proxied request with empty choices list (should raise).\"\"\"\\n    def bad_post(url, json):\\n        return dummyresponse({\"choices\": []}, status_code=200)\\n    monkeypatch.setattr(requests, \"post\", bad_post)\\n    with pytest.raises(runtimeerror) as excinfo:\\n        execute_gpt_4v_request(\\n            roboflow_api_key=\"rfkey123\",\\n            _api_key=\"rf_key:account:abc\",\\n            prompt=[{\"role\": \"user\", \"content\": \"say hi\"}],\\n            gpt_model_version=\"gpt-4v\",\\n            max_tokens=10,\\n            temperature=0.5,\\n        )\\n\\n# --------- large scale test cases ---------\\n\\n\\n\\n\\n\\n\\n\\n\\nimport types\\nfrom typing import list, optional\\n\\n# imports\\nimport pytest  # used for our unit tests\\n# function to test\\nimport requests\\nfrom inference.core.env import api_base_url\\nfrom inference.core.workflows.core_steps.models.foundation..v3 import \\\\\\n    execute_gpt_4v_request\\nfrom  import \\nfrom ._types import not_given\\n\\n# unit tests\\n\\n\\n# --- helpers for monkeypatching ---\\n\\nclass dummyresponse:\\n    def __init__(self, json_data, status_code=200):\\n        self._json = json_data\\n        self.status_code = status_code\\n        self.text = str(json_data)\\n    def json(self):\\n        return self._json\\n    def raise_for_status(self):\\n        if self.status_code >= 400:\\n            raise requests.exceptions. <URL>  {self.status_code}\")\\n\\nclass dummychoices:\\n    def __init__(self, content):\\n        self.message = types.simplenamespace(content=content)\\n\\nclass dummyresponse:\\n    def __init__(self, content):\\n        self.choices = [dummychoices(content)]\\n\\nclass dummychatcompletions:\\n    def __init__(self, content):\\n        self._content = content\\n    def create(self, model, messages, max_tokens, temperature):\\n        return dummyresponse(self._content)\\n\\n# --- test cases ---\\n\\n# basic test cases\\n\\n\\ndef test_proxied__basic(monkeypatch):\\n    \"\"\"test proxied  call with normal parameters.\"\"\"\\n    # patch requests.post to return a dummy response\\n    def dummy_post(url, json):\\n        return dummyresponse({\\n            \"choices\": [\\n                {\"message\": {\"content\": \"proxied hello\"}}\\n            ]\\n        })\\n    monkeypatch.setattr(requests, \"post\", dummy_post)\\n    # patch api_base_url to a dummy value for test\\n    monkeypatch.setattr(\"inference.core.env.api_base_url\", \" <URL> \\n    # call function with a proxied key\\n    codeflash_output = execute_gpt_4v_request(\\n        roboflow_api_key=\"rf_dummy\",\\n        _api_key=\"rf_key:account:abc123\",\\n        prompt=[{\"role\": \"user\", \"content\": \"say hello\"}],\\n        gpt_model_version=\"gpt-4-vision-preview\",\\n        max_tokens=10,\\n        temperature=0.5,\\n    ); result = codeflash_output\\n\\n\\ndef test_invalid__key(monkeypatch):\\n    \"\"\"test with an invalid  key (simulate exception from ).\"\"\"\\n    def dummy__init(self, api_key):\\n        raise exception(\"invalid api key\")\\n    monkeypatch.setattr(, \"__init__\", dummy__init)\\n    with pytest.raises(runtimeerror) as e:\\n        execute_gpt_4v_request(\\n            roboflow_api_key=\"rf_dummy\",\\n            _api_key=\"sk-bad\",\\n            prompt=[{\"role\": \"user\", \"content\": \"test\"}],\\n            gpt_model_version=\"gpt-4-vision-preview\",\\n            max_tokens=10,\\n            temperature=0.5,\\n        )\\n\\ndef test_proxied_ <URL> \\n    \"\"\"test proxied call with http error from requests.\"\"\"\\n    def dummy_post(url, json):\\n        return dummyresponse({}, status_code=500)\\n    monkeypatch.setattr(requests, \"post\", dummy_post)\\n    monkeypatch.setattr(\"inference.core.env.api_base_url\", \" <URL> \\n    with pytest.raises(runtimeerror) as e:\\n        execute_gpt_4v_request(\\n            roboflow_api_key=\"rf_dummy\",\\n            _api_key=\"rf_key:account:bad\",\\n            prompt=[{\"role\": \"user\", \"content\": \"test\"}],\\n            gpt_model_version=\"gpt-4-vision-preview\",\\n            max_tokens=10,\\n            temperature=0.5,\\n        )\\n\\ndef test_proxied_invalid_response_structure(monkeypatch):\\n    \"\"\"test proxied call with invalid response structure (missing keys).\"\"\"\\n    def dummy_post(url, json):\\n        return dummyresponse({\"bad\": \"data\"})\\n    monkeypatch.setattr(requests, \"post\", dummy_post)\\n    monkeypatch.setattr(\"inference.core.env.api_base_url\", \" <URL> \\n    with pytest.raises(runtimeerror) as e:\\n        execute_gpt_4v_request(\\n            roboflow_api_key=\"rf_dummy\",\\n            _api_key=\"rf_key:user:bad\",\\n            prompt=[{\"role\": \"user\", \"content\": \"test\"}],\\n            gpt_model_version=\"gpt-4-vision-preview\",\\n            max_tokens=10,\\n            temperature=0.5,\\n        )\\n\\n\\n\\n\\n\\n\\n\\ndef test_large_scale_proxied(monkeypatch):\\n    \"\"\"test proxied call with large prompt and max_tokens.\"\"\"\\n    large_prompt = [{\"role\": \"user\", \"content\": f\"message {i}\"} for i in range(900)]\\n    def dummy_post(url, json):\\n        return dummyresponse({\\n            \"choices\": [\\n                {\"message\": {\"content\": \"large proxied\"}}\\n            ]\\n        })\\n    monkeypatch.setattr(requests, \"post\", dummy_post)\\n    monkeypatch.setattr(\"inference.core.env.api_base_url\", \" <URL> \\n    codeflash_output = execute_gpt_4v_request(\\n        roboflow_api_key=\"rf_dummy\",\\n        _api_key=\"rf_key:account:abc123\",\\n        prompt=large_prompt,\\n        gpt_model_version=\"gpt-4-vision-preview\",\\n        max_tokens=999,\\n        temperature=0.5,\\n    ); result = codeflash_output\\n```\\n\\n\\nto edit these changes git checkout codeflash/optimize-pr1214-2025-05-14t16.32.54 and push.', ' speed up method jsonparser.parse_number by 17% 17% (0.17x) speedup for jsonparser.parse_number in src/json_repair/json_parser.py\\n⏱ runtime :   7.25 microseconds → 6.21 microseconds (best of 27 runs)\\n explanation and details\\nhere is an optimized version of your program, with a focus on reducing runtime and memory usage in the parse_number and get_char_at functions, as suggested by the profile data.\\nkey improvements.\\n- avoid unnecessary set creation inside hot loops.\\n- minimize attribute access in tight loops (local variable caching for self.get_char_at rather than repeated method call).\\n- reduce string concatenation inside loops by collecting characters into a list and joining once at the end.\\nall comments are preserved except those adjacent to changed lines, which are updated if relevant.\\nkey optimization notes:\\n- parse_number now uses a number_chars list to gather characters, avoiding \"string + char\" concatenation which is o(n²) in python.\\n- get_char_at is bound to a local variable to avoid repeated attribute/method lookup inside the loop.\\n- direct string \"0123456789-.ee/,\" is used for membership check instead of recreating a set each call, as the set is tiny and in-string checks are fast for small sets.\\nall changes preserve existing functionality and logging behavior. the rest of the code remains unmodified (other than whitespace for style).  \\nyou can further optimize if profiling shows parse_string as another hot spot,\\nbut the major bottleneck per your profile was only in parse_number and get_char_at.\\n correctness verification report:\\n| test                        | status            |\\n| --------------------------- | ----------------- |\\n|  existing unit tests |  none found |\\n|  generated regression tests |  97 passed |\\n| ⏪ replay tests |  none found |\\n|  concolic coverage tests |  4 passed |\\n| tests coverage       | 81.8% |\\n\\n generated regression tests details\\n\\n```python\\nfrom typing import any, classvar, literal\\n\\n# imports\\nimport pytest  # used for our unit tests\\nfrom src.json_repair.json_parser import jsonparser\\n\\n\\n# minimal stubs for contextvalues and jsoncontext to allow testing\\nclass contextvalues:\\n    array = \"array\"\\n    object_key = \"object_key\"\\n    object_value = \"object_value\"\\n\\n# unit tests\\n\\n@pytest.mark.parametrize(\\n    \"input_str,expected,desc\",\\n    [\\n        # basic integer\\n        (\"123\", 123, \"simple integer\"),\\n        (\"0\", 0, \"zero integer\"),\\n        (\"-42\", -42, \"negative integer\"),\\n        # basic float\\n        (\"3.14\", 3.14, \"simple float\"),\\n        (\"-0.001\", -0.001, \"negative float\"),\\n        (\"0.0\", 0.0, \"zero float\"),\\n        # basic scientific notation\\n        (\"1e3\", 1000.0, \"scientific notation positive exponent\"),\\n        (\"-2.5e-2\", -0.025, \"scientific notation negative exponent\"),\\n        (\"6.02e23\", 6.02e23, \"large scientific notation\"),\\n        # number with trailing non-numeric\\n        (\"42abc\", \"42\", \"number followed by alpha (should fallback to string)\"),\\n        (\"3.14pie\", \"3.14\", \"float followed by alpha (should fallback to string)\"),\\n        (\"-123.45xyz\", \"-123.45\", \"negative float followed by alpha\"),\\n        # number with comma (should return as string)\\n        (\"1,234\", \"1,234\", \"number with comma\"),\\n        (\"12,345.67\", \"12,345.67\", \"float with comma\"),\\n        # number ending with invalid char\\n        (\"789-\", 789, \"number ending with - (should ignore)\"),\\n        (\"5.3e/\", 5.3, \"number ending with / (should ignore)\"),\\n        (\"10e,\", 10, \"number ending with , (should ignore)\"),\\n        # edge: only sign\\n        (\"-\", \"\", \"just a minus sign\"),\\n        # edge: only decimal point\\n        (\".\", \"\", \"just a dot\"),\\n        # edge: only exponent\\n        (\"e\", \"\", \"just an exponent\"),\\n        # edge: empty string\\n        (\"\", \"\", \"empty input\"),\\n        # edge: multiple dots\\n        (\"1.2.3\", 1.2, \"multiple dots, should parse up to second dot\"),\\n        # edge: multiple exponents\\n        (\"1e2e3\", 100.0, \"multiple exponents, should parse up to second e\"),\\n        # edge: leading zeros\\n        (\"000123\", 123, \"leading zeros\"),\\n        (\"000.456\", 0.456, \"leading zeros in float\"),\\n        # edge: negative zero\\n        (\"-0\", 0, \"negative zero\"),\\n        # edge: large integer\\n        (\"999999999\", 999999999, \"large integer\"),\\n        # edge: large negative integer\\n        (\"-999999999\", -999999999, \"large negative integer\"),\\n        # edge: large float\\n        (\"3.141592653589793238\", 3.141592653589793, \"very long float (python float precision)\"),\\n        # edge: array context, should stop at comma\\n        (\"123,456\", 123, \"array context, stops at comma\"),\\n        # edge: array context, with negative number\\n        (\"-789,123\", -789, \"array context, negative number stops at comma\"),\\n        # edge: array context, float\\n        (\"1.23,4.56\", 1.23, \"array context, float stops at comma\"),\\n        # edge: number with leading plus (should fail to parse as number)\\n        (\"+123\", \"\", \"leading plus is not handled, should return empty string\"),\\n        # edge: number with embedded whitespace\\n        (\"12 34\", 12, \"whitespace breaks number parsing\"),\\n        # edge: number with tab\\n        (\"56\\\\t78\", 56, \"tab breaks number parsing\"),\\n        # edge: negative float with exponent\\n        (\"-1.23e-4\", -1.23e-4, \"negative float with exponent\"),\\n        # edge: float with positive exponent\\n        (\"2.5e+3\", 2500.0, \"float with explicit positive exponent\"),\\n        # edge: float with exponent and trailing chars\\n        (\"7.89e2abc\", \"7.89e2\", \"float with exponent and trailing alpha\"),\\n        # edge: number with multiple commas\\n        (\"1,234,567\", \"1,234,567\", \"number with multiple commas\"),\\n        # edge: number with trailing whitespace\\n        (\"123 \", 123, \"number with trailing space\"),\\n        # edge: number with leading whitespace\\n        (\" 456\", \"\", \"leading whitespace not handled, should return empty string\"),\\n        # edge: negative sign only\\n        (\"-\", \"\", \"just a negative sign\"),\\n        # edge: dot only\\n        (\".\", \"\", \"just a dot\"),\\n        # edge: exponent only\\n        (\"e\", \"\", \"just an e\"),\\n        # edge: negative exponent only\\n        (\"-e\", \"\", \"negative sign and e\"),\\n        # edge: float with no leading digit\\n        (\".5\", 0.5, \"float with no leading digit\"),\\n        # edge: float with no trailing digit\\n        (\"5.\", 5.0, \"float with no trailing digit\"),\\n        # edge: number with slash (should ignore slash)\\n        (\"123/456\", 123, \"slash breaks number parsing\"),\\n        # edge: number with multiple slashes\\n        (\"12/34/56\", 12, \"multiple slashes break parsing\"),\\n        # edge: number with embedded dash\\n        (\"12-34\", 12, \"dash in the middle breaks parsing\"),\\n        # edge: number with multiple exponents (invalid)\\n        (\"1e2e3\", 100.0, \"multiple exponents, only first is parsed\"),\\n        # edge: number with trailing dot and comma\\n        (\"5.,\", 5.0, \"number ending with dot and comma\"),\\n        # edge: number with trailing dot and slash\\n        (\"5./\", 5.0, \"number ending with dot and slash\"),\\n        # edge: number with trailing e and comma\\n        (\"5e,\", 5, \"number ending with e and comma\"),\\n        # edge: number with trailing e and slash\\n        (\"5e/\", 5, \"number ending with e and slash\"),\\n        # edge: number with trailing dash and comma\\n        (\"5-,\", 5, \"number ending with dash and comma\"),\\n        # edge: number with trailing dash and slash\\n        (\"5-/\", 5, \"number ending with dash and slash\"),\\n        # edge: number with comma at start (should return empty string)\\n        (\",123\", \"\", \"comma at start, should return empty string\"),\\n        # edge: number with dot at start (should return empty string)\\n        (\".123\", 0.123, \"dot at start, float with no leading digit\"),\\n        # edge: number with only comma\\n        (\",\", \"\", \"only comma\"),\\n        # edge: number with only dash\\n        (\"-\", \"\", \"only dash\"),\\n        # edge: number with only slash\\n        (\"/\", \"\", \"only slash\"),\\n    ]\\n)\\ndef test_parse_number_basic_and_edge(input_str, expected, desc):\\n    \"\"\"\\n    test basic and edge cases for parse_number.\\n    \"\"\"\\n    parser = jsonparser(input_str)\\n    codeflash_output = parser.parse_number(); result = codeflash_output\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nfrom typing import any, classvar, literal\\n\\n# imports\\nimport pytest\\nfrom src.json_repair.json_parser import jsonparser\\n\\n\\n# dummy contextvalues and jsoncontext for test purposes\\nclass contextvalues:\\n    object_key = \"object_key\"\\n    object_value = \"object_value\"\\n    array = \"array\"\\n\\n# unit tests\\n\\n# -------------------------\\n# 1. basic test cases\\n# -------------------------\\n\\n@pytest.mark.parametrize(\\n    \"input_str,expected\",\\n    [\\n        # integer\\n        (\"123\", 123),\\n        (\"0\", 0),\\n        (\"-42\", -42),\\n        # float\\n        (\"3.14\", 3.14),\\n        (\"-0.001\", -0.001),\\n        (\"2e3\", 2000.0),\\n        (\"-2e-2\", -0.02),\\n        # leading zeros (should parse as int)\\n        (\"007\", 7),\\n        # trailing whitespace (should ignore)\\n        (\"123 \", 123),\\n        # number with comma (should return as string)\\n        (\"1,234\", \"1,234\"),\\n        # number with trailing comma (should ignore comma)\\n        (\"123,\", 123),\\n        # number with trailing slash (should ignore slash)\\n        (\"123/\", 123),\\n        # number with trailing e (should ignore e)\\n        (\"123e\", 123),\\n        # number with trailing - (should ignore -)\\n        (\"123-\", 123),\\n        # negative float with exponent\\n        (\"-1.23e-10\", -1.23e-10),\\n        # float with positive exponent\\n        (\"1.23e+10\", 1.23e10),\\n    ]\\n)\\ndef test_parse_number_basic(input_str, expected):\\n    parser = jsonparser(input_str)\\n    codeflash_output = parser.parse_number(); result = codeflash_output\\n    if isinstance(expected, float):\\n        pass\\n    else:\\n        pass\\n\\n# -------------------------\\n# 2. edge test cases\\n# -------------------------\\n\\n@pytest.mark.parametrize(\\n    \"input_str,expected\",\\n    [\\n        # empty string\\n        (\"\", \"\"),\\n        # only minus sign\\n        (\"-\", \"\"),\\n        # only dot\\n        (\".\", \"\"),\\n        # only exponent\\n        (\"e\", \"\"),\\n        # only comma\\n        (\",\", \"\"),\\n        # only slash\\n        (\"/\", \"\"),\\n        # multiple dots (invalid float)\\n        (\"1.2.3\", \"1.2.3\"),\\n        # multiple exponents (invalid float)\\n        (\"1e2e3\", \"1e2e3\"),\\n        # number followed by alpha (should call parse_string, so returns as string)\\n        (\"123abc\", \"123abc\"),\\n        # number with comma in array context (should stop at comma)\\n        (\"123,456\", 123),\\n        # negative number with trailing comma\\n        (\"-42,\", -42),\\n        # negative float with trailing comma\\n        (\"-42.5,\", -42.5),\\n        # number with embedded slash (invalid, returns as string)\\n        (\"12/34\", \"12/34\"),\\n        # number with embedded comma (invalid, returns as string)\\n        (\"12,34\", \"12,34\"),\\n        # large negative exponent\\n        (\"1e-308\", 1e-308),\\n        # large positive exponent\\n        (\"1e308\", 1e308),\\n        # number with leading whitespace (should parse correctly)\\n        (\"   42\", 42),\\n        # number with trailing whitespace and comma\\n        (\"42 ,\", 42),\\n        # number with leading/trailing whitespace\\n        (\"  42  \", 42),\\n    ]\\n)\\ndef test_parse_number_edge(input_str, expected):\\n    parser = jsonparser(input_str.strip())\\n    codeflash_output = parser.parse_number(); result = codeflash_output\\n    if isinstance(expected, float):\\n        pass\\n    else:\\n        pass\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nfrom src.json_repair.json_parser import jsonparser\\n\\ndef test_jsonparser_parse_number():\\n    jsonparser.parse_number(jsonparser(\\'e\\', none, false, json_fd_chunk_length=0, stream_stable=true))\\n\\ndef test_jsonparser_parse_number_2():\\n    jsonparser.parse_number(jsonparser(\\'53\\', none, none, json_fd_chunk_length=0, stream_stable=false))\\n```\\n\\n\\nto edit these changes git checkout codeflash/optimize-jsonparser.parse_number-maqpo82d and push.', ' speed up method withfixedsizecache.add_model by 50% in pr #1373 (feat/pass-countinference-to-serverless-getweights) this pull request contains optimizations for pr #1373\\nif you approve this dependent pr, these changes will be merged into the original pr branch feat/pass-countinference-to-serverless-getweights.\\n\\nthis pr will be automatically closed if the original pr is merged.\\n\\n\\n 50% (0.50x) speedup for withfixedsizecache.add_model in inference/core/managers/decorators/fixed_size_cache.py\\n⏱ runtime :   1.08 seconds → 722 milliseconds (best of 12 runs)\\n explanation and details\\nhere\\'s an optimized rewrite of your program, addressing profiling hot spots and general efficiency improvements.\\noptimization summary:\\n\\navoid redundant method calls: \\nminimize repeated lookups and calculations.\\ncache computations/results when possible within function scope.\\nlazy imports: \\nmove gc and optional torch imports where needed (they are only used upon eviction).\\ndeque optimizations: \\nin withfixedsizecache.add_model, avoid repeated self._key_queue.remove(queue_id) by checking position or maintaining a set for fast checks (no need, since only called if known present, and block is rare). still, code can be reduced for clarity.\\nreduce logging in the hot add logic (unless debug mode; logging is a major time sink during profiling).\\nbatch removals: \\naccumulate models to remove and do a single gc.collect() call after, instead of per-iteration. \\ndata structure choices are left unchanged (deque is still best for explicit ordering here).\\ngeneral logic: use local variables for lookups on attributes used multiple times (minor, but helps).\\n\\n\\nkey runtime optimizations:\\n- only call gc.collect() after all removals in a batch, not after every single model eviction.\\n- reduced logging in hot code paths (this was responsible for noticeable time in profiling).\\n- use local variables when repeatedly accessing class attributes.\\n- use direct inlining for _resolve_queue_id for this use case.\\n- defensive handling if queue/model state falls out of sync—never throws unnecessarily.\\nperformance note:\\nif you profile again after these changes, most of the time will now be in actual model loading and removal. that is, this code will not be a noticeable bottleneck anymore in the workflow. if lru cache size is much larger, consider further data structure optimizations such as a dict for constant-time eviction and presence checking, but for n ~ 8 this is not needed.\\n correctness verification report:\\n| test                        | status            |\\n| --------------------------- | ----------------- |\\n| ⏪ replay tests |  none found |\\n|  existing unit tests |  none found |\\n|  concolic coverage tests |  none found |\\n|  generated regression tests |  476 passed |\\n| tests coverage       | 85.2% |\\n\\n generated regression tests and runtime\\n\\n```python\\nimport sys\\nfrom collections import deque\\n\\n# imports\\nimport pytest\\nfrom inference.core.managers.decorators.fixed_size_cache import \\\\\\n    withfixedsizecache\\n\\n# function to test and minimal stubs/mocks\\n\\nclass dummymodel:\\n    \"\"\"minimal dummy model for testing.\"\"\"\\n    def __init__(self, model_id, api_key):\\n        self.model_id = model_id\\n        self.api_key = api_key\\n        self.has_model_metadata = false\\n\\n    def clear_cache(self, delete_from_disk=true):\\n        pass\\n\\nclass dummymodelregistry:\\n    \"\"\"minimal dummy registry that returns dummymodel.\"\"\"\\n    def get_model(self, resolved_identifier, api_key, countinference=none, service_secret=none):\\n        return dummymodel\\nclass inferencemodelnotfound(exception): pass\\nclass invalidmodeliderror(exception): pass\\n\\n# enum stub\\nclass modelendpointtype:\\n    ort = type(\"ort\", (), {\"value\": \"ort\"})()\\n    value = \"ort\"\\n\\n# modelmanager and withfixedsizecache as in prompt, but minimal\\nclass modelmanager:\\n    def __init__(self, model_registry, models=none):\\n        self.model_registry = model_registry\\n        self._models = models if models is not none else {}\\n\\n    def add_model(self, model_id, api_key, model_id_alias=none, endpoint_type=modelendpointtype.ort, countinference=none, service_secret=none):\\n        resolved_identifier = model_id if model_id_alias is none else model_id_alias\\n        if resolved_identifier in self._models:\\n            return\\n        model_class = self.model_registry.get_model(resolved_identifier, api_key, countinference=countinference, service_secret=service_secret)\\n        model = model_class(model_id=model_id, api_key=api_key)\\n        self._models[resolved_identifier] = model\\n\\n    def remove(self, model_id, delete_from_disk=true):\\n        if model_id not in self._models:\\n            raise inferencemodelnotfound()\\n        self._models[model_id].clear_cache(delete_from_disk=delete_from_disk)\\n        del self._models[model_id]\\n\\n    def __contains__(self, model_id):\\n        return model_id in self._models\\n\\n    def __getitem__(self, key):\\n        if key not in self._models:\\n            raise inferencemodelnotfound()\\n        return self._models[key]\\n\\n    def __len__(self):\\n        return len(self._models)\\n\\n    def keys(self):\\n        return self._models.keys()\\n\\n# ========== unit tests below ==========\\n\\n@pytest.fixture\\ndef cache_manager():\\n    \"\"\"returns a withfixedsizecache with max_size=3 for testing.\"\"\"\\n    registry = dummymodelregistry()\\n    base_manager = modelmanager(registry)\\n    return withfixedsizecache(base_manager, max_size=3)\\n\\n@pytest.fixture\\ndef unique_model_id():\\n    \"\"\"returns a function to generate unique model_ids for tests.\"\"\"\\n    counter = [0]\\n    def _gen():\\n        counter[0] += 1\\n        return f\"dataset{counter[0]}/1\"\\n    return _gen\\n\\n# 1. basic test cases\\n\\ndef test_add_single_model(cache_manager, unique_model_id):\\n    \"\"\"test adding a single model works and is present.\"\"\"\\n    model_id = unique_model_id()\\n    cache_manager.add_model(model_id, api_key=\"key\")\\n\\ndef test_add_duplicate_model_noop(cache_manager, unique_model_id):\\n    \"\"\"adding the same model twice does not increase count.\"\"\"\\n    model_id = unique_model_id()\\n    cache_manager.add_model(model_id, api_key=\"key\")\\n    cache_manager.add_model(model_id, api_key=\"key\")\\n\\ndef test_add_model_with_alias(cache_manager, unique_model_id):\\n    \"\"\"adding with an alias stores under the alias, not model_id.\"\"\"\\n    model_id = unique_model_id()\\n    alias = \"alias1\"\\n    cache_manager.add_model(model_id, api_key=\"key\", model_id_alias=alias)\\n\\ndef test_add_multiple_models_up_to_capacity(cache_manager, unique_model_id):\\n    \"\"\"add up to max_size models, all should be present.\"\"\"\\n    ids = [unique_model_id() for _ in range(3)]\\n    for mid in ids:\\n        cache_manager.add_model(mid, api_key=\"key\")\\n    for mid in ids:\\n        pass\\n\\n# 2. edge test cases\\n\\ndef test_eviction_on_capacity(cache_manager, unique_model_id):\\n    \"\"\"adding more than max_size evicts least recently used.\"\"\"\\n    ids = [unique_model_id() for _ in range(4)]\\n    for mid in ids[:3]:\\n        cache_manager.add_model(mid, api_key=\"key\")\\n    # now add a 4th, should evict ids[0]\\n    cache_manager.add_model(ids[3], api_key=\"key\")\\n\\ndef test_eviction_marks_mru(cache_manager, unique_model_id):\\n    \"\"\"adding a model again marks it as most recently used (no eviction).\"\"\"\\n    ids = [unique_model_id() for _ in range(3)]\\n    for mid in ids:\\n        cache_manager.add_model(mid, api_key=\"key\")\\n    # access ids[0] to mark it as mru\\n    cache_manager.add_model(ids[0], api_key=\"key\")\\n    # add new model, should evict ids[1] now (was lru)\\n    new_id = unique_model_id()\\n    cache_manager.add_model(new_id, api_key=\"key\")\\n\\ndef test_add_model_with_alias_then_same_id(cache_manager, unique_model_id):\\n    \"\"\"adding with alias, then with same model_id, both can exist.\"\"\"\\n    model_id = unique_model_id()\\n    alias = \"alias2\"\\n    cache_manager.add_model(model_id, api_key=\"key\", model_id_alias=alias)\\n    cache_manager.add_model(model_id, api_key=\"key\")\\n\\ndef test_add_model_eviction_multiple_rounds(cache_manager, unique_model_id):\\n    \"\"\"eviction removes 3 at a time if possible when over threshold.\"\"\"\\n    # fill up to 3\\n    ids = [unique_model_id() for _ in range(3)]\\n    for mid in ids:\\n        cache_manager.add_model(mid, api_key=\"key\")\\n    # add 4th, should evict 1st\\n    cache_manager.add_model(\"dataset999/1\", api_key=\"key\")\\n    # add 5th, should evict 3 more (but only 3 in cache, so only possible to evict all)\\n    cache_manager.add_model(\"dataset1000/1\", api_key=\"key\")\\n\\ndef test_remove_model(cache_manager, unique_model_id):\\n    \"\"\"test removing a model actually removes it.\"\"\"\\n    model_id = unique_model_id()\\n    cache_manager.add_model(model_id, api_key=\"key\")\\n    cache_manager.remove(model_id)\\n\\ndef test_remove_nonexistent_model_raises(cache_manager):\\n    \"\"\"removing a model not present raises inferencemodelnotfound.\"\"\"\\n    with pytest.raises(inferencemodelnotfound):\\n        cache_manager.remove(\"not-present/1\")\\n\\n\\ndef test_add_model_with_alias_eviction(cache_manager, unique_model_id):\\n    \"\"\"eviction works when models are added by alias.\"\"\"\\n    ids = [unique_model_id() for _ in range(2)]\\n    alias = \"alias3\"\\n    cache_manager.add_model(ids[0], api_key=\"key\", model_id_alias=alias)\\n    cache_manager.add_model(ids[1], api_key=\"key\")\\n    cache_manager.add_model(\"dataset888/1\", api_key=\"key\")\\n    # now add another to force eviction\\n    cache_manager.add_model(\"dataset889/1\", api_key=\"key\")\\n    # at least one of the first 3 should be evicted\\n    count = sum(mid in cache_manager for mid in [alias, ids[1], \"dataset888/1\"])\\n\\ndef test_lru_eviction_order(cache_manager, unique_model_id):\\n    \"\"\"eviction order is lru, not fifo.\"\"\"\\n    ids = [unique_model_id() for _ in range(3)]\\n    for mid in ids:\\n        cache_manager.add_model(mid, api_key=\"key\")\\n    # access ids[1] to make it mru\\n    cache_manager.add_model(ids[1], api_key=\"key\")\\n    # add new model, should evict ids[0]\\n    new_id = unique_model_id()\\n    cache_manager.add_model(new_id, api_key=\"key\")\\n\\ndef test_add_model_memory_pressure(monkeypatch, cache_manager, unique_model_id):\\n    \"\"\"if memory_pressure_detected returns true, eviction is triggered.\"\"\"\\n    monkeypatch.setattr(cache_manager, \"memory_pressure_detected\", lambda: true)\\n    # fill up cache\\n    ids = [unique_model_id() for _ in range(3)]\\n    for mid in ids:\\n        cache_manager.add_model(mid, api_key=\"key\")\\n    # add another, should evict 3 at once\\n    cache_manager.add_model(\"dataset2000/1\", api_key=\"key\")\\n\\ndef test_add_model_exception_removes_from_queue(cache_manager, monkeypatch):\\n    \"\"\"if add_model raises, queue is cleaned up.\"\"\"\\n    # patch model_manager.add_model to raise\\n    def raise_exc(*a, **kw): raise runtimeerror(\"fail!\")\\n    monkeypatch.setattr(cache_manager.model_manager, \"add_model\", raise_exc)\\n    before_len = len(cache_manager._key_queue)\\n    with pytest.raises(runtimeerror):\\n        cache_manager.add_model(\"dataset/1\", api_key=\"key\")\\n\\n# 3. large scale test cases\\n\\ndef test_large_number_of_models_eviction():\\n    \"\"\"add 10 models to a cache of size 5, only last 5 remain.\"\"\"\\n    registry = dummymodelregistry()\\n    base_manager = modelmanager(registry)\\n    cache_manager = withfixedsizecache(base_manager, max_size=5)\\n    ids = [f\"ds{i}/1\" for i in range(10)]\\n    for mid in ids:\\n        cache_manager.add_model(mid, api_key=\"key\")\\n    # only last 5 should remain\\n    for mid in ids[:5]:\\n        pass\\n    for mid in ids[5:]:\\n        pass\\n\\ndef test_stress_add_and_access():\\n    \"\"\"add 20 models, repeatedly access some to keep them in cache.\"\"\"\\n    registry = dummymodelregistry()\\n    base_manager = modelmanager(registry)\\n    cache_manager = withfixedsizecache(base_manager, max_size=10)\\n    ids = [f\"ds{i}/1\" for i in range(20)]\\n    for mid in ids[:10]:\\n        cache_manager.add_model(mid, api_key=\"key\")\\n    # repeatedly access first 5 to keep them mru\\n    for _ in range(5):\\n        for mid in ids[:5]:\\n            cache_manager.add_model(mid, api_key=\"key\")\\n    # add next 10\\n    for mid in ids[10:]:\\n        cache_manager.add_model(mid, api_key=\"key\")\\n    # the first 5 should still be in cache, next 5 should have been evicted\\n    for mid in ids[:5]:\\n        pass\\n    for mid in ids[5:10]:\\n        pass\\n    for mid in ids[10:]:\\n        pass\\n\\ndef test_add_models_with_aliases_large_scale():\\n    \"\"\"add 50 models with unique aliases, only last 10 remain in cache.\"\"\"\\n    registry = dummymodelregistry()\\n    base_manager = modelmanager(registry)\\n    cache_manager = withfixedsizecache(base_manager, max_size=10)\\n    for i in range(50):\\n        model_id = f\"dataset{i}/1\"\\n        alias = f\"alias{i}\"\\n        cache_manager.add_model(model_id, api_key=\"key\", model_id_alias=alias)\\n    # only last 10 aliases should be present\\n    for i in range(40):\\n        pass\\n    for i in range(40, 50):\\n        pass\\n\\ndef test_eviction_never_exceeds_max_size():\\n    \"\"\"after many operations, cache never exceeds max_size.\"\"\"\\n    registry = dummymodelregistry()\\n    base_manager = modelmanager(registry)\\n    cache_manager = withfixedsizecache(base_manager, max_size=7)\\n    for i in range(30):\\n        cache_manager.add_model(f\"ds{i}/1\", api_key=\"key\")\\n\\ndef test_eviction_when_queue_empty_does_not_crash():\\n    \"\"\"eviction with empty queue does not raise.\"\"\"\\n    registry = dummymodelregistry()\\n    base_manager = modelmanager(registry)\\n    cache_manager = withfixedsizecache(base_manager, max_size=1)\\n    # remove all models to empty queue\\n    cache_manager._key_queue.clear()\\n    try:\\n        cache_manager.add_model(\"ds1/1\", api_key=\"key\")\\n    except exception:\\n        pytest.fail(\"add_model should not raise when queue is empty\")\\n# codeflash_output is used to check that the output of the original code is the same as that of the optimized code.\\n\\nfrom collections import deque\\n\\n# imports\\nimport pytest\\nfrom inference.core.managers.decorators.fixed_size_cache import \\\\\\n    withfixedsizecache\\n\\n# --- minimal stubs and mocks for dependencies ---\\n\\n# exception classes\\nclass roboflowapinotauthorizederror(exception):\\n    pass\\n\\nclass inferencemodelnotfound(exception):\\n    pass\\n\\n# modelendpointtype enum stub\\nclass modelendpointtype:\\n    ort = \"ort\"\\n\\n# model stub\\nclass dummymodel:\\n    def __init__(self, model_id, api_key):\\n        self.model_id = model_id\\n        self.api_key = api_key\\n        self.cleared = false\\n\\n    def clear_cache(self, delete_from_disk=true):\\n        self.cleared = true\\n\\n# modelregistry stub\\nclass dummymodelregistry:\\n    def get_model(self, resolved_identifier, api_key, countinference=none, service_secret=none):\\n        # always returns dummymodel constructor\\n        return dummymodel\\n\\n# --- the modelmanager, modelmanagerdecorator, and withfixedsizecache implementations ---\\n\\nclass modelmanager:\\n    def __init__(self, model_registry, models=none):\\n        self.model_registry = model_registry\\n        self._models = {} if models is none else models\\n\\n    def add_model(\\n        self,\\n        model_id,\\n        api_key,\\n        model_id_alias=none,\\n        endpoint_type=modelendpointtype.ort,\\n        countinference=none,\\n        service_secret=none,\\n    ):\\n        resolved_identifier = model_id if model_id_alias is none else model_id_alias\\n        if resolved_identifier in self._models:\\n            return\\n        model_class = self.model_registry.get_model(\\n            resolved_identifier, api_key, countinference=countinference, service_secret=service_secret\\n        )\\n        model = model_class(model_id=model_id, api_key=api_key)\\n        self._models[resolved_identifier] = model\\n\\n    def remove(self, model_id, delete_from_disk=true):\\n        if model_id not in self._models:\\n            raise inferencemodelnotfound(f\"model {model_id} not found\")\\n        self._models[model_id].clear_cache(delete_from_disk=delete_from_disk)\\n        del self._models[model_id]\\n\\n    def __contains__(self, model_id):\\n        return model_id in self._models\\n\\n    def __getitem__(self, key):\\n        if key not in self._models:\\n            raise inferencemodelnotfound(f\"model {key} not found\")\\n        return self._models[key]\\n\\n    def __len__(self):\\n        return len(self._models)\\n\\n    def keys(self):\\n        return self._models.keys()\\n\\n# global flag for api key check\\nmodels_cache_auth_enabled = false\\n\\n# --- unit tests ---\\n\\n@pytest.fixture\\ndef model_manager():\\n    # returns a fresh modelmanager with dummymodelregistry\\n    return modelmanager(dummymodelregistry())\\n\\n@pytest.fixture\\ndef cache_manager(model_manager):\\n    # returns a withfixedsizecache wrapping the above\\n    return withfixedsizecache(model_manager, max_size=4)\\n\\n# 1. basic test cases\\n\\ndef test_add_single_model_basic(cache_manager):\\n    \"\"\"test adding a single model to an empty cache.\"\"\"\\n    cache_manager.add_model(\"modela/1\", \"key\")\\n\\ndef test_add_duplicate_model_noop(cache_manager):\\n    \"\"\"test that adding the same model twice does not increase cache size.\"\"\"\\n    cache_manager.add_model(\"modela/1\", \"key\")\\n    cache_manager.add_model(\"modela/1\", \"key\")\\n\\ndef test_add_model_with_alias(cache_manager):\\n    \"\"\"test adding a model with an alias as queue id.\"\"\"\\n    cache_manager.add_model(\"modela/1\", \"key\", model_id_alias=\"aliasa\")\\n\\ndef test_add_model_with_different_aliases(cache_manager):\\n    \"\"\"test that different aliases are treated as different cache entries.\"\"\"\\n    cache_manager.add_model(\"modela/1\", \"key\", model_id_alias=\"aliasa\")\\n    cache_manager.add_model(\"modela/1\", \"key\", model_id_alias=\"aliasb\")\\n\\ndef test_add_multiple_models_basic(cache_manager):\\n    \"\"\"test adding multiple distinct models.\"\"\"\\n    cache_manager.add_model(\"modela/1\", \"key\")\\n    cache_manager.add_model(\"modelb/1\", \"key\")\\n    cache_manager.add_model(\"modelc/1\", \"key\")\\n\\n# 2. edge test cases\\n\\ndef test_add_model_eviction_lru(cache_manager):\\n    \"\"\"test that adding models over max_size evicts least recently used.\"\"\"\\n    # fill up cache\\n    cache_manager.add_model(\"a/1\", \"key\")\\n    cache_manager.add_model(\"b/1\", \"key\")\\n    cache_manager.add_model(\"c/1\", \"key\")\\n    cache_manager.add_model(\"d/1\", \"key\")\\n    # add one more, triggers eviction (removes a/1, b/1, c/1 in order)\\n    cache_manager.add_model(\"e/1\", \"key\")\\n    # add another, triggers more evictions\\n    cache_manager.add_model(\"f/1\", \"key\")\\n\\ndef test_add_model_lru_refresh(cache_manager):\\n    \"\"\"test that re-adding an existing model refreshes its lru position.\"\"\"\\n    cache_manager.add_model(\"a/1\", \"key\")\\n    cache_manager.add_model(\"b/1\", \"key\")\\n    cache_manager.add_model(\"c/1\", \"key\")\\n    cache_manager.add_model(\"d/1\", \"key\")\\n    # refresh a/1\\n    cache_manager.add_model(\"a/1\", \"key\")\\n    # add e/1, should evict b/1, c/1, d/1 (a/1 was refreshed)\\n    cache_manager.add_model(\"e/1\", \"key\")\\n\\n\\ndef test_add_model_with_invalid_model_id(cache_manager):\\n    \"\"\"test that a model_id_alias with same name as another model_id is treated as distinct.\"\"\"\\n    cache_manager.add_model(\"modela/1\", \"key\")\\n    cache_manager.add_model(\"modelb/1\", \"key\", model_id_alias=\"modela/1\")\\n\\ndef test_add_model_evicts_all_when_cache_full(cache_manager):\\n    \"\"\"test that if more than max_size+3 models are added, all old models are evicted.\"\"\"\\n    # fill cache\\n    cache_manager.add_model(\"a/1\", \"key\")\\n    cache_manager.add_model(\"b/1\", \"key\")\\n    cache_manager.add_model(\"c/1\", \"key\")\\n    cache_manager.add_model(\"d/1\", \"key\")\\n    # add 4 more, causing two eviction rounds\\n    cache_manager.add_model(\"e/1\", \"key\")\\n    cache_manager.add_model(\"f/1\", \"key\")\\n    cache_manager.add_model(\"g/1\", \"key\")\\n    cache_manager.add_model(\"h/1\", \"key\")\\n    # only last 4 models should remain\\n    for mid in [\"e/1\", \"f/1\", \"g/1\", \"h/1\"]:\\n        pass\\n    for mid in [\"a/1\", \"b/1\", \"c/1\", \"d/1\"]:\\n        pass\\n\\ndef test_add_model_handles_exception_and_removes_from_queue(cache_manager):\\n    \"\"\"test that if modelmanager.add_model raises, the queue is cleaned up.\"\"\"\\n    # patch model_manager.add_model to raise\\n    orig_add_model = cache_manager.model_manager.add_model\\n    def raise_exc(*a, **kw):\\n        raise valueerror(\"fail!\")\\n    cache_manager.model_manager.add_model = raise_exc\\n    with pytest.raises(valueerror):\\n        cache_manager.add_model(\"z/1\", \"key\")\\n    # restore\\n    cache_manager.model_manager.add_model = orig_add_model\\n\\ndef test_add_model_with_alias_and_duplicate(cache_manager):\\n    \"\"\"test that adding same model with and without alias treats them as separate.\"\"\"\\n    cache_manager.add_model(\"a/1\", \"key\")\\n    cache_manager.add_model(\"a/1\", \"key\", model_id_alias=\"aliasa\")\\n\\n# 3. large scale test cases\\n\\ndef test_add_many_models_and_evictions():\\n    \"\"\"test adding up to 20 models with cache size 10, check lru eviction.\"\"\"\\n    mm = modelmanager(dummymodelregistry())\\n    cache = withfixedsizecache(mm, max_size=10)\\n    # add 20 models\\n    for i in range(20):\\n        cache.add_model(f\"model{i}/1\", \"key\")\\n    # only last 10 should remain\\n    for i in range(10, 20):\\n        pass\\n    for i in range(10):\\n        pass\\n\\ndef test_add_models_with_aliases_large_scale():\\n    \"\"\"test adding models with unique aliases does not cause collisions.\"\"\"\\n    mm = modelmanager(dummymodelregistry())\\n    cache = withfixedsizecache(mm, max_size=50)\\n    # add 50 models with unique aliases\\n    for i in range(50):\\n        cache.add_model(f\"modelx/1\", \"key\", model_id_alias=f\"alias_{i}\")\\n    # all aliases should be present\\n    for i in range(50):\\n        pass\\n\\ndef test_lru_eviction_pattern_stress():\\n    \"\"\"test lru eviction pattern with repeated access and additions.\"\"\"\\n    mm = modelmanager(dummymodelregistry())\\n    cache = withfixedsizecache(mm, max_size=5)\\n    # add 5 models\\n    for i in range(5):\\n        cache.add_model(f\"m{i}/1\", \"key\")\\n    # access models to change lru order\\n    cache.add_model(\"m2/1\", \"key\")\\n    cache.add_model(\"m4/1\", \"key\")\\n    # add new model, should evict oldest (m0/1, m1/1, m3/1 in order)\\n    cache.add_model(\"m5/1\", \"key\")\\n    # only most recently used and new should remain\\n    for mid in [\"m2/1\", \"m4/1\", \"m5/1\"]:\\n        pass\\n\\ndef test_add_models_performance_under_load():\\n    \"\"\"test that adding 100 models with cache size 50 only keeps last 50.\"\"\"\\n    mm = modelmanager(dummymodelregistry())\\n    cache = withfixedsizecache(mm, max_size=50)\\n    for i in range(100):\\n        cache.add_model(f\"large_{i}/1\", \"key\")\\n    for i in range(50, 100):\\n        pass\\n    for i in range(50):\\n        pass\\n\\ndef test_add_models_with_same_alias_large_scale():\\n    \"\"\"test that adding many models with same alias overwrites previous.\"\"\"\\n    mm = modelmanager(dummymodelregistry())\\n    cache = withfixedsizecache(mm, max_size=10)\\n    for i in range(20):\\n        cache.add_model(f\"modelq_{i}/1\", \"key\", model_id_alias=\"shared_alias\")\\n# codeflash_output is used to check that the output of the original code is the same as that of the optimized code.\\n```\\n\\n\\nto edit these changes git checkout codeflash/optimize-pr1373-2025-06-24t21.57.17 and push.']"
        ],
        [
         "10",
         "9",
         "11",
         "9_sdk_flutter_cache_manager_apphost_git",
         "['sdk', 'flutter_cache_manager', 'apphost', 'git', 'ui', 'workflow', 'appcontrol', 'releases', 'improve', 'flutter']",
         "[\"finishing up work on appcontrol manager v.1.9.1.0added control flow guard support to the appcontrol manager. a great security feature that prevents certain exploits.\\n\\n\\nset intel's control-flow enforcement technology to explicitly enabled in the project.\\n\\n\\nthe native ahead-of-time compilation now favors execution speed and performance over package size. (will be monitoring this one as the app gets bigger when more features are implemented in the future. for now, it only increases the package size by 1mb)\\n\\n\\nenabled a new code analyzer to enforce more code best practices: dotnet_diagnostic.ca1724. (type names should not match namespaces)\\n\\n\\nupdated the appcontrol manager document.\\n\\n\\nformatted all xaml codes for better readability.\\n\\n\\nadjusted the app control simulation code based on the custom serialization/deserialization logic introduced in the previous update.\\n\\n\\nupdated microsoft.identitymodel.abstractions nuget dependency.\\n\\n\\naligned some columns in the new listviews.\", 'improve workflow system with bug fixes and optimizationsthis pr improves the workflow system with several bug fixes and optimizations:\\noverall, fixed mutliple warnings when running tests.\\nchanges made\\n\\nfix variable getter method redefinition: prevent redefining getter methods that already exist in each_step.rb\\noptimize sqlite state repository operations: improve database operations and state management\\nenhance step executor reporting: better error handling and reporting functionality\\nrefactor test suites: simplify and improve test maintainability across multiple test files\\nupdate interpolator test: use simpler expectation syntax for better readability\\n\\nfiles modified\\n\\nlib/roast/workflow/each_step.rb\\nlib/roast/workflow/sqlite_state_repository.rb\\nlib/roast/workflow/step_executor_with_reporting.rb\\ntest/roast/workflow/interpolator_test.rb\\ntest/roast/workflow/iteration_steps_test.rb\\ntest/roast/workflow/step_executor_with_reporting_test.rb\\ntest/roast/workflow/step_loader_test.rb\\n\\ntesting\\nall existing tests pass, and the refactored test suites are more maintainable and readable.\\nimpact\\nthese changes improve the robustness and maintainability of the workflow system while fixing potential bugs with method redefinition.', \"feat: add --download flag for faster flutter sdk installation feature: download flag for faster flutter sdk installation\\nadds a new --download flag to the fvm install command that downloads pre-built flutter sdk archives instead of git cloning, providing faster installation for official releases.\\n what's new\\ncommand usage\\n```bash\\ndownload pre-built flutter 3.24.0 archive\\nfvm install 3.24.0 --download\\nshort form\\nfvm install 3.24.0 -d\\nfalls back to git for unsupported versions\\nfvm install custom_version --download  # uses git cloning\\nfvm install myfork/stable --download    # uses git cloning\\n```\\nkey features\\n\\n faster installation: pre-built archives vs git clone + build\\n reduced bandwidth: smaller archive files vs full git repositories\\n smart fallback: automatically uses git when download isn't available\\n official releases only: supports stable, beta, dev channels\\n backward compatible: existing workflows unchanged\\n cross-platform: works on linux, macos, windows\\n\\n implementation details\\nfiles added\\n\\nlib/src/services/download_service.dart - handles archive downloads and extraction\\ntest/services/download_service_test.dart - comprehensive test suite\\n\\nfiles modified\\n\\nlib/src/commands/install_command.dart - added --download flag\\nlib/src/workflows/ensure_cache.workflow.dart - integrated download workflow\\nlib/src/utils/context.dart - registered downloadservice\\npubspec.yaml - added archive package dependency\\ntest/commands/install_command_test.dart - added flag parsing tests\\n\\narchitecture\\n\\ndownloadservice: handles tar.xz archive downloads and extraction\\nensurecacheworkflow: enhanced to support download mode with git fallback\\ninstallcommand: added --download flag integration\\nsmart detection: only official releases can be downloaded\\n\\n testing\\ntest coverage\\n\\n unit tests for downloadservice functionality\\n integration tests with real flutter releases api\\n command-line flag parsing tests\\n error scenarios and fallback behavior\\n archive structure validation\\n\\nquality checks\\n\\n dart analyze - no issues\\n dart fix --apply - applied fixes\\n all tests passing (11/11)\\n successful compilation\\n updated with latest main branch\\n\\n benefits\\n\\nperformance: significantly faster installation for official releases\\nreliability: less dependent on git infrastructure\\nbandwidth: reduced download size\\nuser experience: faster developer onboarding\\nrobustness: automatic fallback ensures compatibility\\n\\n backward compatibility\\n\\ndefault behavior unchanged - existing scripts continue to work\\ngit cloning remains the default installation method\\ndownload flag is opt-in only\\nall existing command options preserved\\n\\n code quality\\n\\nfollows dry, kiss, yagni principles\\nreuses existing services (flutterreleaseclient, cacheservice)\\nclean error handling with meaningful messages\\ncomprehensive test coverage\\nzero static analysis issues\\n\\n recent updates\\nlatest merge (a2d89c6): \\n-  merged with latest main branch\\n-  resolved conflicts in test files\\n-  maintained both download flag tests and new project config tests\\n-  all tests passing after merge\\n-  zero code quality issues\\n review notes\\n\\nimplementation focuses on official flutter releases only\\nsmart fallback ensures no breaking changes\\nuses existing fvm infrastructure and patterns\\nminimal code footprint with maximum benefit\\nbranch is up-to-date with main\\n\\ncloses #[issue-number] (if applicable)\"]"
        ],
        [
         "11",
         "10",
         "11",
         "10_benchmark_redirect_oldinputs_benchmark_redirect_parseandclearflashmessages_benchmark_redirect_processflashmessages_fiber",
         "['benchmark_redirect_oldinputs', 'benchmark_redirect_parseandclearflashmessages', 'benchmark_redirect_processflashmessages', 'fiber', 'benchmark_redirect_messages', 'benchmark_redirect_oldinput', 'js', 'maintainers', 'benchmarks', 'benchmark_redirect_route']",
         "[\" refactor: the value of map is unused in uniqueroutestackdescription\\nthe value of map m in uniqueroutestack is unused. we can set the value type of m to struct{} instead of int to save some memory and improve code clarity and readability.\\ntype of change\\nplease delete options that are not relevant.\\n\\n[ ] new feature (non-breaking change which adds functionality)\\n[ ] enhancement (improvement to existing features and functionality)\\n[ ] documentation update (changes to documentation)\\n[ ] performance improvement (non-breaking change which improves efficiency)\\n[x] code consistency (non-breaking change which improves code reliability and robustness)\\n\\nchecklist\\nbefore you submit your pull request, please make sure you meet these requirements:\\n\\n[x] followed the inspiration of the express.js framework for new functionalities, making them similar in usage.\\n[x] conducted a self-review of the code and provided comments for complex or critical parts.\\n[ ] updated the documentation in the /docs/ directory for fiber's documentation.\\n[ ] added or updated unit tests to validate the effectiveness of the changes or new features.\\n[x] ensured that new and existing unit tests pass locally with the changes.\\n[ ] verified that any new dependencies are essential and have been agreed upon by the maintainers/community.\\n[ ] aimed for optimal performance with minimal allocations in the new code.\\n[ ] provided benchmarks for the new code to analyze and improve upon.\", \" refactor: migrate randstring to rand v2description\\nthis pr migrates randstring from math/rand to math/rand/v2 to improve randomness quality and concurrency safety. in addition, it reduces execution time by approximately 97%, as shown in the benchmark test below.\\ngo\\nfunc benchmark_randomstring(b *testing.b) {\\n    for i := 0; i < b.n; i++ {\\n        _ = randstring(100)\\n    }\\n}\\ngoos: linux\\ngoarch: amd64\\npkg: github.com/gofiber/fiber/v3/client\\ncpu: amd epyc 7763 64-core processor                \\n                   old.txt                   new.txt               \\n                    sec/op       sec/op     vs base                \\n_randomstring-4   9862.0n ± 0%   252.1n ± 3%  -97.44% (p=0.000 n=10)\\nchanges introduced\\nlist the new features or adjustments introduced in this pull request. provide details on benchmarks, documentation updates, changelog entries, and if applicable, the migration guide.\\n\\n[x] benchmarks: describe any performance benchmarks and improvements related to the changes.\\n[ ] documentation update: detail the updates made to the documentation and links to the changed files.\\n[ ] changelog/what's new: include a summary of the additions for the upcoming release notes.\\n[ ] migration guide: if necessary, provide a guide or steps for users to migrate their existing code to accommodate these changes.\\n[ ] api alignment with express: explain how the changes align with the express api.\\n[ ] api longevity: discuss the steps taken to ensure that the new or updated apis are consistent and not prone to breaking changes.\\n[ ] examples: provide examples demonstrating the new features or changes in action.\\n\\ntype of change\\n\\n[x] performance improvement (non-breaking change which improves efficiency)\\n[x] code consistency (non-breaking change which improves code reliability and robustness)\\n\\nchecklist\\nbefore you submit your pull request, please make sure you meet these requirements:\\n\\n[x] followed the inspiration of the express.js framework for new functionalities, making them similar in usage.\\n[x] conducted a self-review of the code and provided comments for complex or critical parts.\\n[ ] updated the documentation in the /docs/ directory for fiber's documentation.\\n[ ] added or updated unit tests to validate the effectiveness of the changes or new features.\\n[x] ensured that new and existing unit tests pass locally with the changes.\\n[ ] verified that any new dependencies are essential and have been agreed upon by the maintainers/community.\\n[x] aimed for optimal performance with minimal allocations in the new code.\\n[x] provided benchmarks for the new code to analyze and improve upon.\", \":bug: bug: fix redirection flash messages violate cookie structuredescription\\nthis issue fixes redirection with flash messages breaks http header format by encoding msgpack serialized binary data with hex. it has little overhead, but still we get benefit of using messagepack. here are the benchmark results:\\n```go\\nnew:\\nbenchmark_redirect_route-16                              6792206           173.7 ns/op        16 b/op          1 allocs/op\\nbenchmark_redirect_route_withqueries-16                  3043641           391.3 ns/op        40 b/op          2 allocs/op\\nbenchmark_redirect_route_withflashmessages-16            2439327           481.1 ns/op       293 b/op          3 allocs/op\\nbenchmark_redirect_parseandclearflashmessages-16         2350215           510.5 ns/op       192 b/op          7 allocs/op\\nbenchmark_redirect_processflashmessages-16               3483320           344.6 ns/op       288 b/op          2 allocs/op\\nbenchmark_redirect_messages-16                          13537572            87.54 ns/op      128 b/op          2 allocs/op\\nbenchmark_redirect_oldinputs-16                         14712171            77.44 ns/op       96 b/op          2 allocs/op\\nbenchmark_redirect_message-16                           83187558            14.43 ns/op        0 b/op          0 allocs/op\\nbenchmark_redirect_oldinput-16                          153820603            7.814 ns/op           0 b/op          0 allocs/op\\nold:\\nbenchmark_redirect_route-16                              6861298           175.9 ns/op        16 b/op          1 allocs/op\\nbenchmark_redirect_route_withqueries-16                  2901906           408.6 ns/op        40 b/op          2 allocs/op\\nbenchmark_redirect_route_withflashmessages-16            3456422           328.7 ns/op       117 b/op          2 allocs/op\\nbenchmark_redirect_parseandclearflashmessages-16         4298755           278.1 ns/op        32 b/op          6 allocs/op\\nbenchmark_redirect_processflashmessages-16               6022623           198.2 ns/op       112 b/op          1 allocs/op\\nbenchmark_redirect_messages-16                          13548512            87.84 ns/op      128 b/op          2 allocs/op\\nbenchmark_redirect_oldinputs-16                         14798696            78.96 ns/op       96 b/op          2 allocs/op\\nbenchmark_redirect_message-16                           82884747            14.44 ns/op        0 b/op          0 allocs/op\\nbenchmark_redirect_oldinput-16                          148364563            8.104 ns/op           0 b/op          0 allocs/op\\n```\\nfixes  <URL> \\ntype of change\\n\\n[x] enhancement (improvement to existing features and functionality)\\n\\nchecklist\\nbefore you submit your pull request, please make sure you meet these requirements:\\n\\n[ ] followed the inspiration of the express.js framework for new functionalities, making them similar in usage.\\n[ ] conducted a self-review of the code and provided comments for complex or critical parts.\\n[ ] updated the documentation in the /docs/ directory for fiber's documentation.\\n[ ] added or updated unit tests to validate the effectiveness of the changes or new features.\\n[ ] ensured that new and existing unit tests pass locally with the changes.\\n[ ] verified that any new dependencies are essential and have been agreed upon by the maintainers/community.\\n[ ] aimed for optimal performance with minimal allocations in the new code.\\n[ ] provided benchmarks for the new code to analyze and improve upon.\\n\\ncommit formatting\\nplease use emojis in commit messages for an easy way to identify the purpose or intention of a commit. check out the emoji cheatsheet here: contributing.md\"]"
        ],
        [
         "12",
         "11",
         "8",
         "11_perf_prefetch_splitshard_query",
         "['perf', 'prefetch', 'splitshard', 'query', 'prioritized', 'glaredb', 'distribute', 'split', 'indexitem', 'benchmark_equalfieldtype']",
         "[\"index followers to better support our query patternswe frequently query followers filtered by followed_id and ordered by\\nconfirmed_at. (e.g. on the /followers page)\\nthis could take 80+ seconds for sellers with a large amount of\\nfollowing.\\ni am hoping that this composite index on (followed_id, confirmed_at) can\\nhelp more efficiently perform range scans without a filesort, improving\\nquery performances.\\ni left these out of the composite index:\\n\\ndeleted_at: vast majority of the records should have deleted_at is\\n  null\\nid: i think this is only going to help if we switch to -based\\n  pagination (unlikely to be prioritized)\\n\\ni had tested this with a local table of 1m records. will further\\nbench this once it's rolled out and make adjustments if needed.\", 'start with minimal prefetch distance to minimize prefetch overhead for exact or limited index scansproblem\\nsee  <URL> \\nin case of queries index scan with limit clause, multiple backends can concurrently send larger number of duplicated prefetch requests which are not stored in lfc and so actually do useless job.\\ncurrent implementation of index prefetch starts with maximal prefetch distance (10 by default now) when there are no key bounds, so in queries with limit clause like select * from t order by pk limit 1 compute can send a lot of useless prefetch requests to page server.\\nsummary of changes\\nalways start with minimal prefetch distance even if there are not key boundaries.\\nrelated postgres prs:\\n <URL> \\n <URL> \\n <URL> \\n <URL> ', \"perf: short-circuit selection evaluationshort-circuit evaluation of conjunction predicates in filters.\\noptimizer rule for reordering of conjunction filter predicates to take advantage of short-circuiting.\\n\\nbefore:\\nglaredb> select searchphrase, min(url), count(*) as c from hits where url like '%google%' and searc\\n     ... hphrase <> '' group by searchphrase order by c desc limit 10;\\n\\n searchphrase                    min                                                     c     \\n utf8                            utf8                                                    int64 \\n\\n прокур горбуши                   <URL>                         60 \\n римском качественны for cry      <URL>                           24 \\n стоит похуден                    <URL>      23 \\n испанч боб новости дейская       <URL>      21 \\n прокур готовки видеоэндоменя     <URL>                         14 \\n прокур гипоаллеры                <URL>                         11 \\n камедицинск авт…                 <URL>       9 \\n универ 11.6/1366x768/40…         <URL>       8 \\n купить трудован…                 <URL>       7 \\n вспомню о названы монстэр        <URL>       7 \\n\\nexecution duration (s): 3.31950\\nafter:\\nglaredb> select searchphrase, min(url), count(*) as c from hits where url like '%google%' and searc\\n     ... hphrase <> '' group by searchphrase order by c desc limit 10;\\n\\n searchphrase                    min                                                     c     \\n utf8                            utf8                                                    int64 \\n\\n прокур горбуши                   <URL>                         60 \\n римском качественны for cry      <URL>                           24 \\n стоит похуден                    <URL>      23 \\n испанч боб новости дейская       <URL>      21 \\n прокур готовки видеоэндоменя     <URL>                         14 \\n прокур гипоаллеры                <URL>                         11 \\n камедицинск авт…                 <URL>       9 \\n универ 11.6/1366x768/40…         <URL>       8 \\n вспомню о названы монстэр        <URL>       7 \\n купить трудован…                 <URL>       7 \\n\\nexecution duration (s): 2.55737\"]"
        ],
        [
         "13",
         "12",
         "6",
         "12_fixes_coderabbit_commit_automation",
         "['fixes', 'coderabbit', 'commit', 'automation', 'chore', 'workflow', 'dashboard', 'debounce', 'widgets', 'ide']",
         "['chore: remove analytics execution from the critical pathdescription\\n\\npushed out the sendexecuteanalyticsevent from the critical path of returning action\\'s execution result.\\nimproved the critical path of sendexecuteanalyticsevent by running the application mono concurrent to other events.\\nadded more telemetry code around the execution flow.\\n\\nfixes #issue number\\nor\\nfixes issue url\\n\\n[!warning]\\nif no issue exists, please create an issue first, and check with the maintainers if the issue is valid.\\n\\nautomation\\n/ok-to-test tags=\"@tag.all\"\\n:mag: cypress test results\\n this is an auto-generated comment: cypress test results  \\n\\n[!tip]\\n   all cypress tests have passed!   \\nworkflow run:  <URL> \\ncommit: ddf93dd06cd4facabdde5898d1cc40ce7dc4765f\\ncypress dashboard.\\ntags: @tag.all\\nspec:\\ntue, 18 mar 2025 10:28:52 utc\\n\\n end of auto-generated comment: cypress test results  \\ncommunication\\nshould the devrel and marketing teams inform users about this change?\\n- [ ] yes\\n- [ ] no\\n this is an auto-generated comment: release notes by coderabbit.ai \\nsummary by coderabbit\\n\\nnew features\\nintroduced additional action tracking identifiers to support enhanced analytics and authentication validation.\\n\\n\\nrefactor\\noptimized asynchronous operations for data retrieval to improve responsiveness.\\nenhanced the flow and error handling of action execution, ensuring smoother and more reliable performance.\\n\\n\\n\\n end of auto-generated comment: release notes by coderabbit.ai', 'chore: adding debounce to onvaluechange for input widgetsdescription\\nadding debounce to onvaluechange for input widgets to fix multiple execute api calls happening in reactive queries flow.\\nfixes #40813\\nautomation\\n/ok-to-test tags=\"@tag.all\"\\n:mag: cypress test results\\n this is an auto-generated comment: cypress test results  \\n\\n[!tip]\\n   all cypress tests have passed!   \\nworkflow run:  <URL> \\ncommit: 6943ba5d0df915256cf29831df53e9ff9880d617\\ncypress dashboard.\\ntags: @tag.all\\nspec:\\nfri, 06 jun 2025 09:40:52 utc\\n\\n end of auto-generated comment: cypress test results  \\ncommunication\\nshould the devrel and marketing teams inform users about this change?\\n- [ ] yes\\n- [ ] no\\n this is an auto-generated comment: release notes by coderabbit.ai \\nsummary by coderabbit\\n\\nnew features\\ninput widgets now update their displayed values instantly while saving changes in the background with a short delay, improving typing responsiveness.\\ninput changes are grouped and saved after a brief pause, reducing unnecessary updates and enhancing performance.\\nbug fixes\\ninput fields now stay in sync with external updates and clear any pending background updates when needed, preventing outdated or duplicate changes.\\nchores\\nimproved cleanup of background processes when input widgets are removed, ensuring smoother operation.\\ntests\\nadded typing delays in input simulation during cypress tests to better mimic real user input timing.\\n\\n end of auto-generated comment: release notes by coderabbit.ai', 'chore: ce changes related to decoupling webworkerdescription\\nwe are improving the lcp by reducing the time to reach the first evaluation, aiming for a 1.8 to 2.2 second reduction. to achieve this, we’ve implemented the following changes:\\ncode splitting of widgets: during page load, only the widgets required for the initial evaluation are loaded and registered. the remaining widgets are registered after the first evaluation message is sent. this parallelizes widget loading with evaluation computation, reducing the critical path.\\nweb worker offloading: macro tasks such as clearcache and javascript library installation have been moved to the web worker setup. these are now executed in a separate thread, allowing the firstunevaluatedtree to be computed in parallel with js library installation.\\nparallel js library loading: all javascript libraries are now loaded in parallel within the web worker, instead of sequentially, improving efficiency.\\ndeferred rendering of appviewer: we now render the appviewer component only after registering the remaining widgets. this ensures that heavy rendering tasks—such as expensive selector computations and loading additional chunks related to the appviewer—can execute in parallel with the first evaluation, further enhancing performance.\\nautomation\\n/ok-to-test tags=\"@tag.all\"\\n:mag: cypress test results\\n this is an auto-generated comment: cypress test results  \\n\\n[!caution]\\n   some tests have failed.\\nworkflow run:  <URL> \\ncommit: 2dc9dbcd6b60cb63ec954713dbf7335d788df9a4\\ncypress dashboard.\\ntags: @tag.all\\nspec: \\nthe following are new failures, please fix them before merging the pr: \\ncypress/e2e/regression/clientside/otheruifeatures/analytics_spec.js\\nlist of identified flaky tests.\\nthu, 26 jun 2025 07:57:26 utc\\n\\n end of auto-generated comment: cypress test results  \\ncommunication\\nshould the devrel and marketing teams inform users about this change?\\n- [ ] yes\\n- [ ] no\\n this is an auto-generated comment: release notes by coderabbit.ai \\nsummary by coderabbit\\n\\nnew features\\nadded support for deferred loading of javascript libraries and improved control over page rendering and first page load behavior.\\nintroduced granular widget registration, allowing partial widget initialization for faster initial rendering.\\nadded new redux actions and selectors to manage and track evaluation and rendering state.\\n\\nadded explicit cache clearing for widget factory memoization functions.\\n\\n\\nimprovements\\n\\nrefactored widget loading to be asynchronous and on-demand, reducing initial load time and improving modularity.\\nenhanced sagas and reducers to better handle first-time evaluations and widget registration.\\noptimized js library loading to occur in parallel for improved performance.\\nmodularized theme application and improved conditional rendering in the app viewer.\\nreorganized widget registration to initialize widgets individually rather than in bulk.\\nimproved memoization decorator to allow explicit cache clearing globally.\\nupdated evaluation sagas to support partial widget initialization and deferred js library loading.\\n\\nupdated widget loading utilities and tests to support asynchronous dynamic loading.\\n\\n\\nbug fixes\\n\\n\\nimproved conditional logic to prevent errors when rendering components with missing functions.\\n\\n\\ntests\\n\\nexpanded and refactored test suites to cover asynchronous widget loading, partial initialization, and evaluation saga behaviors.\\n\\nadded tests verifying widget factory cache behavior and first evaluation integration.\\n\\n\\nchores\\n\\nupdated imports and code structure for clarity and maintainability.\\nreorganized type imports and moved interface declarations to dedicated modules.\\n\\n end of auto-generated comment: release notes by coderabbit.ai']"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 14
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>58</td>\n",
       "      <td>-1_fixes_dotnet_implementation_benchmark</td>\n",
       "      <td>[fixes, dotnet, implementation, benchmark, inv...</td>\n",
       "      <td>[reduce http headers validation overheadwhen a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>0_github_lint_fixes_mlflow</td>\n",
       "      <td>[github, lint, fixes, mlflow, changeset, pnpm,...</td>\n",
       "      <td>[swap doc preview and test steps to view the p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>1_cuda_optimisations_bugfixes_fix</td>\n",
       "      <td>[cuda, optimisations, bugfixes, fix, optimisat...</td>\n",
       "      <td>[disable cache on ci on windows because downlo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>2_coderabbit_chatgpt_refactor_release</td>\n",
       "      <td>[coderabbit, chatgpt, refactor, release, docke...</td>\n",
       "      <td>[grida canvas - skia-safe rust backend - stand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>3_linqfixes_linq_compiling_runtime</td>\n",
       "      <td>[linqfixes, linq, compiling, runtime, concurre...</td>\n",
       "      <td>[add comprehensive vibetunnel protocol benchma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>4_builtins_bun_tests_typescript</td>\n",
       "      <td>[builtins, bun, tests, typescript, builtin, de...</td>\n",
       "      <td>[report memory cost of sourcemaps to gcwhat do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>5_parser_refactor_buffer_chunksprefetch</td>\n",
       "      <td>[parser, refactor, buffer, chunksprefetch, pre...</td>\n",
       "      <td>[@remotion/renderer: consider --memory flag fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>6_torch_memory_saver_c_compiler_compiler_gemma...</td>\n",
       "      <td>[torch_memory_saver, c_compiler, compiler, gem...</td>\n",
       "      <td>[[mlas] dequantizelinear int8/uint8description...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>7_refactor_workersupdateuserbalancestatscachew...</td>\n",
       "      <td>[refactor, workersupdateuserbalancestatscachew...</td>\n",
       "      <td>[cached repetitive data lookups for creator an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>8_execute_gpt_4v_request_crewagentparser_jsonp...</td>\n",
       "      <td>[execute_gpt_4v_request, crewagentparser, json...</td>\n",
       "      <td>[ speed up function execute_gpt_4v_request by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>9_sdk_flutter_cache_manager_apphost_git</td>\n",
       "      <td>[sdk, flutter_cache_manager, apphost, git, ui,...</td>\n",
       "      <td>[finishing up work on appcontrol manager v.1.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>10_benchmark_redirect_oldinputs_benchmark_redi...</td>\n",
       "      <td>[benchmark_redirect_oldinputs, benchmark_redir...</td>\n",
       "      <td>[ refactor: the value of map is unused in uniq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>11_perf_prefetch_splitshard_query</td>\n",
       "      <td>[perf, prefetch, splitshard, query, prioritize...</td>\n",
       "      <td>[index followers to better support our query p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>12_fixes_coderabbit_commit_automation</td>\n",
       "      <td>[fixes, coderabbit, commit, automation, chore,...</td>\n",
       "      <td>[chore: remove analytics execution from the cr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic  Count                                               Name  \\\n",
       "0      -1     58           -1_fixes_dotnet_implementation_benchmark   \n",
       "1       0     41                         0_github_lint_fixes_mlflow   \n",
       "2       1     35                  1_cuda_optimisations_bugfixes_fix   \n",
       "3       2     24              2_coderabbit_chatgpt_refactor_release   \n",
       "4       3     16                 3_linqfixes_linq_compiling_runtime   \n",
       "5       4     14                    4_builtins_bun_tests_typescript   \n",
       "6       5     13            5_parser_refactor_buffer_chunksprefetch   \n",
       "7       6     12  6_torch_memory_saver_c_compiler_compiler_gemma...   \n",
       "8       7     12  7_refactor_workersupdateuserbalancestatscachew...   \n",
       "9       8     12  8_execute_gpt_4v_request_crewagentparser_jsonp...   \n",
       "10      9     11            9_sdk_flutter_cache_manager_apphost_git   \n",
       "11     10     11  10_benchmark_redirect_oldinputs_benchmark_redi...   \n",
       "12     11      8                  11_perf_prefetch_splitshard_query   \n",
       "13     12      6              12_fixes_coderabbit_commit_automation   \n",
       "\n",
       "                                       Representation  \\\n",
       "0   [fixes, dotnet, implementation, benchmark, inv...   \n",
       "1   [github, lint, fixes, mlflow, changeset, pnpm,...   \n",
       "2   [cuda, optimisations, bugfixes, fix, optimisat...   \n",
       "3   [coderabbit, chatgpt, refactor, release, docke...   \n",
       "4   [linqfixes, linq, compiling, runtime, concurre...   \n",
       "5   [builtins, bun, tests, typescript, builtin, de...   \n",
       "6   [parser, refactor, buffer, chunksprefetch, pre...   \n",
       "7   [torch_memory_saver, c_compiler, compiler, gem...   \n",
       "8   [refactor, workersupdateuserbalancestatscachew...   \n",
       "9   [execute_gpt_4v_request, crewagentparser, json...   \n",
       "10  [sdk, flutter_cache_manager, apphost, git, ui,...   \n",
       "11  [benchmark_redirect_oldinputs, benchmark_redir...   \n",
       "12  [perf, prefetch, splitshard, query, prioritize...   \n",
       "13  [fixes, coderabbit, commit, automation, chore,...   \n",
       "\n",
       "                                  Representative_Docs  \n",
       "0   [reduce http headers validation overheadwhen a...  \n",
       "1   [swap doc preview and test steps to view the p...  \n",
       "2   [disable cache on ci on windows because downlo...  \n",
       "3   [grida canvas - skia-safe rust backend - stand...  \n",
       "4   [add comprehensive vibetunnel protocol benchma...  \n",
       "5   [report memory cost of sourcemaps to gcwhat do...  \n",
       "6   [@remotion/renderer: consider --memory flag fr...  \n",
       "7   [[mlas] dequantizelinear int8/uint8description...  \n",
       "8   [cached repetitive data lookups for creator an...  \n",
       "9   [ speed up function execute_gpt_4v_request by ...  \n",
       "10  [finishing up work on appcontrol manager v.1.9...  \n",
       "11  [ refactor: the value of map is unused in uniq...  \n",
       "12  [index followers to better support our query p...  \n",
       "13  [chore: remove analytics execution from the cr...  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e6f5af3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-07 13:05:04,835 - BERTopic - WARNING: Using a custom list of topic assignments may lead to errors if topic reduction techniques are used afterwards. Make sure that manually assigning topics is the last step in the pipeline.Note that topic embeddings will also be created through weightedc-TF-IDF embeddings instead of centroid embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Topic",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Representation",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Representative_Docs",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "c7c51b87-76ac-47b9-af8a-611460440c3f",
       "rows": [
        [
         "0",
         "0",
         "54",
         "0_github_refactor_lint_fixes",
         "['github', 'refactor', 'lint', 'fixes', 'mlflow', 'pnpm', 'documentation', 'repository', 'issues', 'release']",
         "['swap doc preview and test steps to view the preview faster (#15504) devtools \\n\\n\\n[![open in github codespaces]( <URL> \\n\\n#### install mlflow from this pr\\n\\n```\\n# mlflow\\npip install git+ <URL> \\n# mlflow-skinny\\npip install git+ <URL> \\n```\\n\\nfor databricks, use the following command:\\n\\n```\\n%sh curl -lssf  <URL>  | sh -s 15509\\n```\\n\\n\\n\\nrelated issues/prs\\n uncomment \\'resolve\\' if this pr can close the linked items. \\nclose #15504 \\nwhat changes are proposed in this pull request?\\nswaps the steps in .circleci/config.yml to upload the docs artifacts before running example tests, enabling faster doc previews.\\n please fill in changes proposed in this pr. \\nhow is this pr tested?\\n\\n[ ] existing unit/integration tests\\n[ ] new unit/integration tests\\n[x] manual tests\\n\\n attach code, screenshot, video used for manual testing here. \\ndoes this pr require documentation update?\\n\\n[x] no. you can skip the rest of this section.\\n[ ] yes. i\\'ve updated:\\n[ ] examples\\n[ ] api references\\n[ ] instructions\\n\\nrelease notes\\nis this a user-facing change?\\n\\n[x] no. you can skip the rest of this section.\\n[ ] yes. give a description of this change to be included in the release notes for mlflow users.\\n\\n details in 1-2 sentences. you can just refer to another pr with a description if this pr is part of a larger change. \\nwhat component(s), interfaces, languages, and integrations does this pr affect?\\ncomponents\\n\\n[ ] area/artifacts: artifact stores and artifact logging\\n[x] area/build: build and test infrastructure for mlflow\\n[ ] area/deployments: mlflow deployments client apis, server, and third-party deployments integrations\\n[x] area/docs: mlflow documentation pages\\n[ ] area/examples: example code\\n[ ] area/model-registry: model registry service, apis, and the fluent client calls for model registry\\n[ ] area/models: mlmodel format, model serialization/deserialization, flavors\\n[ ] area/recipes: recipes, recipe apis, recipe configs, recipe templates\\n[ ] area/projects: mlproject format, project running backends\\n[ ] area/scoring: mlflow model server, model deployment tools, spark udfs\\n[ ] area/server-infra: mlflow tracking server backend\\n[ ] area/tracking: tracking service, tracking client apis, autologging\\n\\ninterface\\n\\n[ ] area/uiux: front-end, user experience, plotting, javascript, javascript dev server\\n[ ] area/docker: docker use across mlflow\\'s components, such as mlflow projects and mlflow models\\n[ ] area/sqlalchemy: use of sqlalchemy in the tracking service or model registry\\n[ ] area/windows: windows support\\n\\nlanguage\\n\\n[ ] language/r: r apis and clients\\n[ ] language/java: java apis and clients\\n[ ] language/new: proposals for new client languages\\n\\nintegrations\\n\\n[ ] integrations/azure: azure and azure ml integrations\\n[ ] integrations/sagemaker: sagemaker integrations\\n[ ] integrations/databricks: databricks integrations\\n\\n\\ninsert an empty named anchor here to allow jumping to this section with a fragment url\\n(e.g.  <URL> \\nnote that github prefixes anchor names in markdown with \"user-content-\".\\n\\n\\nhow should the pr be classified in the release notes? choose one:\\n\\n[x] rn/none - no description will be included. the pr will be mentioned only by the pr number in the \"small bugfixes and documentation updates\" section\\n[ ] rn/breaking-change - the pr will be mentioned in the \"breaking changes\" section\\n[ ] rn/feature - a new user-facing feature worth mentioning in the release notes\\n[ ] rn/bug-fix - a user-facing bug fix worth mentioning in the release notes\\n[ ] rn/documentation - a user-facing documentation change worth mentioning in the release notes\\n\\nshould this pr be included in the next patch release?\\nyes should be selected for bug fixes, documentation updates, and other small changes. no should be selected for new features and larger changes. if you\\'re unsure about the release classification of this pr, leave this unchecked to let the maintainers decide.\\n\\nwhat is a minor/patch release?\\n\\n- minor release: a release that increments the second part of the version number (e.g., 1.2.0 -> 1.3.0).\\n  bug fixes, doc updates and new features usually go into minor releases.\\n- patch release: a release that increments the third part of the version number (e.g., 1.2.0 -> 1.2.1).\\n  bug fixes and doc updates usually go into patch releases.\\n\\n\\n patch \\n\\n[ ] yes (this pr will be cherry-picked and included in the next patch release)\\n[x] no (this pr will be included in the next minor release)', 'clean up init file for some genai flavors devtools \\n\\n\\n[![open in github codespaces]( <URL> \\n\\n#### install mlflow from this pr\\n\\n```\\n# mlflow\\npip install git+ <URL> \\n# mlflow-skinny\\npip install git+ <URL> \\n```\\n\\nfor databricks, use the following command:\\n\\n```\\n%sh curl -lssf  <URL>  | sh -s 15481\\n```\\n\\n\\n\\nwhat changes are proposed in this pull request?\\nthe __init__ file for flavors are massive. this is ok, but becomes a challenge for the lightweight tracing sdk, because doing mlflow..autolog() imports everything defined in the mlflow//__init__.py, including various model logging dependencies.\\non the other hand, a good example is dspy flavor, which organize code well and keep __init__.py clean.\\nthis pr moves model logging code from __init__.py to model.py so we can later choose to import or not inside the __init__.py. this only updates three flavors that are problematic for tracing sdk: , langchain, and llama_index, but we can apply same to other flavors later.\\nnote: this pr targets to mlflow-3 branch rather than the feature branch, because this will cause huge merge conflict otherwise.\\nhow is this pr tested?\\n\\n[x] existing unit/integration tests\\n[ ] new unit/integration tests\\n[ ] manual tests\\n\\n attach code, screenshot, video used for manual testing here. \\ndoes this pr require documentation update?\\n\\n[x] no. you can skip the rest of this section.\\n[ ] yes. i\\'ve updated:\\n[ ] examples\\n[ ] api references\\n[ ] instructions\\n\\nrelease notes\\nis this a user-facing change?\\n\\n[ ] no. you can skip the rest of this section.\\n[ ] yes. give a description of this change to be included in the release notes for mlflow users.\\n\\n details in 1-2 sentences. you can just refer to another pr with a description if this pr is part of a larger change. \\nwhat component(s), interfaces, languages, and integrations does this pr affect?\\ncomponents\\n\\n[ ] area/artifacts: artifact stores and artifact logging\\n[ ] area/build: build and test infrastructure for mlflow\\n[ ] area/deployments: mlflow deployments client apis, server, and third-party deployments integrations\\n[ ] area/docs: mlflow documentation pages\\n[ ] area/examples: example code\\n[ ] area/model-registry: model registry service, apis, and the fluent client calls for model registry\\n[ ] area/models: mlmodel format, model serialization/deserialization, flavors\\n[ ] area/recipes: recipes, recipe apis, recipe configs, recipe templates\\n[ ] area/projects: mlproject format, project running backends\\n[ ] area/scoring: mlflow model server, model deployment tools, spark udfs\\n[ ] area/server-infra: mlflow tracking server backend\\n[x] area/tracking: tracking service, tracking client apis, autologging\\n\\ninterface\\n\\n[ ] area/uiux: front-end, user experience, plotting, javascript, javascript dev server\\n[ ] area/docker: docker use across mlflow\\'s components, such as mlflow projects and mlflow models\\n[ ] area/sqlalchemy: use of sqlalchemy in the tracking service or model registry\\n[ ] area/windows: windows support\\n\\nlanguage\\n\\n[ ] language/r: r apis and clients\\n[ ] language/java: java apis and clients\\n[ ] language/new: proposals for new client languages\\n\\nintegrations\\n\\n[ ] integrations/azure: azure and azure ml integrations\\n[ ] integrations/sagemaker: sagemaker integrations\\n[ ] integrations/databricks: databricks integrations\\n\\n\\ninsert an empty named anchor here to allow jumping to this section with a fragment url\\n(e.g.  <URL> \\nnote that github prefixes anchor names in markdown with \"user-content-\".\\n\\n\\nhow should the pr be classified in the release notes? choose one:\\n\\n[x] rn/none - no description will be included. the pr will be mentioned only by the pr number in the \"small bugfixes and documentation updates\" section\\n[ ] rn/breaking-change - the pr will be mentioned in the \"breaking changes\" section\\n[ ] rn/feature - a new user-facing feature worth mentioning in the release notes\\n[ ] rn/bug-fix - a user-facing bug fix worth mentioning in the release notes\\n[ ] rn/documentation - a user-facing documentation change worth mentioning in the release notes\\n\\nshould this pr be included in the next patch release?\\nyes should be selected for bug fixes, documentation updates, and other small changes. no should be selected for new features and larger changes. if you\\'re unsure about the release classification of this pr, leave this unchecked to let the maintainers decide.\\n\\nwhat is a minor/patch release?\\n\\n- minor release: a release that increments the second part of the version number (e.g., 1.2.0 -> 1.3.0).\\n  bug fixes, doc updates and new features usually go into minor releases.\\n- patch release: a release that increments the third part of the version number (e.g., 1.2.0 -> 1.2.1).\\n  bug fixes and doc updates usually go into patch releases.\\n\\n\\n patch \\n\\n[ ] yes (this pr will be cherry-picked and included in the next patch release)\\n[x] no (this pr will be included in the next minor release)', 'refactor: optimize react component rendering and improve robustnessgrey screen fix, jules experiment\\nthis commit introduces several changes aimed at improving ui rendering performance and overall robustness, potentially addressing \"grey screen\" or unresponsiveness issues.\\n\\n\\noptimized memoization for chat rows:\\n\\nreplaced generic deepequal with custom comparison functions for react.memo in chatrow.tsx and browsersessionrow.tsx. these custom functions perform more targeted comparisons of props, focusing only on fields relevant to rendering, which should reduce the overhead of memoization and prevent unnecessary re-renders.\\nthe internal chatrowcontentcomponent in chatrow.tsx was also wrapped with react.memo.\\n\\n\\n\\nincreased robustness:\\n\\nadded try-catch blocks around json.parse calls within browsersessionrow.tsx to prevent runtime errors from malformed json in message text.\\n\\n\\n\\ncode analysis confirmations:\\n\\nanalysis of chatview.tsx indicated that its useeffect dependency arrays were already in a reasonably optimized state.\\nreview of clineprovider.ts confirmed that its dispose method is comprehensive and correctly wired to the ondiddispose event of webview panels, ensuring cleanup of tab-specific provider instances.\\nreview of shadowcheckpointservice.ts confirmed that the renamenestedgitrepos method and its usage in stageall include appropriate try...catch and try...finally blocks for robust handling of file system operations.\\n\\n\\n\\nthese changes collectively aim to make the ui more efficient and the extension more stable.\\n\\nthank you for contributing to roo code!\\n\\nbefore submitting your pr, please ensure:\\n- it\\'s linked to an approved github issue.\\n- you\\'ve reviewed our [contributing guidelines](../contributing.md).\\n\\nrelated github issue\\n every pr must be linked to an approved issue. \\ncloses: #  replace with the issue number, e.g., closes: #123 \\ndescription\\n\\nbriefly summarize the changes in this pr and how they address the linked issue.\\nthe issue should cover the \"what\" and \"why\"; this section should focus on:\\n- the \"how\": key implementation details, design choices, or trade-offs made.\\n- anything specific reviewers should pay attention to in this pr.\\n\\ntest procedure\\n\\ndetail the steps to test your changes. this helps reviewers verify your work.\\n- how did you test this specific implementation? (e.g., unit tests, manual testing steps)\\n- how can reviewers reproduce your tests or verify the fix/feature?\\n- include relevant testing environment details if applicable.\\n\\ntype of change\\n mark all applicable boxes with an \\'x\\'. \\n\\n[ ]  bug fix: non-breaking change that fixes an issue.\\n[ ]  new feature: non-breaking change that adds functionality.\\n[ ]  breaking change: fix or feature that would cause existing functionality to not work as expected.\\n[ ]  refactor: code change that neither fixes a bug nor adds a feature.\\n[ ]  style: changes that do not affect the meaning of the code (white-space, formatting, etc.).\\n[ ]  documentation: updates to documentation files.\\n[ ]  build/ci: changes to the build process or ci configuration.\\n[ ]  chore: other changes that don\\'t modify src or test files.\\n\\npre-submission checklist\\n go through this checklist before marking your pr as ready for review. \\n\\n[ ] issue linked: this pr is linked to an approved github issue (see \"related github issue\" above).\\n[ ] scope: my changes are focused on the linked issue (one major feature/fix per pr).\\n[ ] self-review: i have performed a thorough self-review of my code.\\n[ ] code quality:\\n[ ] my code adheres to the project\\'s style guidelines.\\n[ ] there are no new linting errors or warnings (npm run lint).\\n[ ] all debug code (e.g., console.log) has been removed.\\n\\n\\n[ ] testing:\\n[ ] new and/or updated tests have been added to cover my changes.\\n[ ] all tests pass locally (npm test).\\n[ ] the application builds successfully with my changes.\\n\\n\\n[ ] branch hygiene: my branch is up-to-date (rebased) with the main branch.\\n[ ] documentation impact: i have considered if my changes require documentation updates (see \"documentation updates\" section below).\\n[ ] changeset: a changeset has been created using npm run changeset if this pr includes user-facing changes or dependency updates.\\n[ ] contribution guidelines: i have read and agree to the contributor guidelines.\\n\\nscreenshots / videos\\n\\nfor ui changes, please provide before-and-after screenshots or a short video of the *actual results*.\\nthis greatly helps in understanding the visual impact of your changes.\\n\\ndocumentation updates\\n\\ndoes this pr necessitate updates to user-facing documentation?\\n- [ ] no documentation updates are required.\\n- [ ] yes, documentation updates are required. (please describe what needs to be updated or link to a pr in the docs repository).\\n\\nadditional notes\\n add any other context, questions, or information for reviewers here. \\nget in touch\\n\\nplease provide your discord username for reviewers or maintainers to reach you if they have questions about your pr']"
        ],
        [
         "1",
         "1",
         "35",
         "1_cuda_optimisations_bugfixes_fix",
         "['cuda', 'optimisations', 'bugfixes', 'fix', 'optimisation', 'cache', 'tensorwide', 'gpu', 'implement', 'improve']",
         "['disable cache on ci on windows because downloading the cache takes a super long time', 'implement tx rate controller targeting given bps', 'perf: common sub-expression elimination, cast flatten rules']"
        ],
        [
         "2",
         "2",
         "27",
         "2_coderabbit_fixes_release_refactor",
         "['coderabbit', 'fixes', 'release', 'refactor', 'rendering', 'docker', 'download', 'nginx', 'readme', 'caching']",
         "['grida canvas - skia-safe rust backend - standalone performance testing <URL> \\n <URL> \\n <URL> \\n <URL> \\n\\n\\nrelated:\\n-  <URL> \\n-  <URL> \\n this is an auto-generated comment: release notes by coderabbit.ai \\nsummary by coderabbit\\n\\nnew features\\nintroduced a high-performance, real-time 2d rendering engine with comprehensive scene graph support, geometry caching, and advanced rendering optimizations.\\nadded extensive support for vector graphics, shapes, text, gradients, images, and effects such as blur and shadows.\\nimplemented asynchronous image and font loading with caching, and support for web fonts.\\nenabled figma and json file import for scene creation.\\nprovided interactive demo applications and benchmarking examples for rendering performance and resource management.\\n\\nadded a modular math utilities library for geometry, layout, color, and rasterization.\\n\\n\\ndocumentation\\n\\n\\nadded detailed readme files, architectural overviews, and optimization strategy documents for both engine and math libraries.\\n\\n\\ntests\\n\\n\\nincluded comprehensive unit and integration tests for geometry, rendering, resource loading, hit testing, and performance benchmarks.\\n\\n\\nchores\\n\\nadded configuration files for rust workspaces, docker, makefile automation, and package management.\\n\\n end of auto-generated comment: release notes by coderabbit.ai', 'inplace isq support and default to mmapisq update to support models that don\\'t fit in system ram, instead quantizing them as they are loaded.\\n this is an auto-generated comment: release notes by coderabbit.ai \\nsummary by coderabbit\\n\\nnew features\\nintroduced a global, thread-safe mechanism for setting and retrieving immediate in-situ quantization (isq) configuration.\\nadded support for applying isq to model layers immediately upon creation when configured.\\nadded predicate-based filtering for sharded tensor access to control accessible tensors more granularly.\\nadded measurement and reporting of \"time to first token\" (ttft) in interactive text and vision modes.\\n\\nextended model loaders with new immediate isq predicate methods for enhanced quantization targeting.\\n\\n\\nbug fixes\\n\\nimproved handling of buffer offsets in quantization and dequantization operations for correct data alignment.\\n\\ncorrected regex patterns for self-attention output projections in multiple model loaders.\\n\\n\\nrefactor\\n\\nsimplified and clarified isq flag logic and quantization setup during model loading.\\nenhanced tensor loading logic to explicitly handle cases with and without lora paths.\\nupdated layer constructors to conditionally apply immediate isq after creation, improving quantization consistency.\\nparallelized model layer loading across numerous architectures using rayon, significantly improving initialization performance.\\nintegrated progress bars with parallel iteration for better feedback during concurrent operations.\\nimproved error handling in model layer construction by replacing panics with proper error propagation in several models.\\nmoved logging utilities for one-time info/warn messages to a shared quantization utility module.\\n\\n end of auto-generated comment: release notes by coderabbit.ai', 'full fix and updatethis is for connecting ai agents/ desktop//etc to a locally hosted n8n instance on docker desktop. this update also further implements other features, such as connecting nodes and adding proper nodes, creating and deleting workflows, updating credentials, updating folders, and so much more. i use the api/v1/docs to parse the entire api setup for the locally hosted n8n.\\n this is an auto-generated comment: release notes by coderabbit.ai \\nsummary by coderabbit\\n\\nnew features\\nadded a comprehensive ai-powered outlook email assistant workflow automating email ingestion, ai classification, response generation, storage, analytics, and notifications.\\nintroduced a complete postgresql database schema supporting advanced email processing, analytics, conversation tracking, error logging, and user preferences.\\nenhanced the n8n node catalog with detailed metadata, examples, search, and recommendation capabilities for ai agents and workflow automation.\\nadded new, fully described node definitions for popular integrations including airtable, asana, bitbucket, box, copper, clickup,  ai, code, discord, dropbox, facebook, and github.\\nimplemented a robust node discovery system, production-ready node registry, and real-time search engine for all nodes.\\nprovided a detailed ai agent guidance module, usage patterns, and best practices for workflow creation.\\nintroduced scripts for automated node verification, fixing, optimization, and comprehensive testing.\\nadded performance optimizations: smart caching, connection pooling, real-time monitoring, and enhanced error handling.\\n\\nsupplied quick setup guides, workflow documentation, and project status reports for production deployment.\\n\\n\\nbug fixes\\n\\nautomated scripts to identify and fix typescript, formatting, and structural issues across all node files.\\n\\nresolved duplicate imports/exports, missing properties, and syntax errors in node definitions and registries.\\n\\n\\ndocumentation\\n\\nadded extensive user, agent, and setup guides, ai optimization summaries, and troubleshooting documentation.\\n\\nupdated workflow and api documentation examples for clarity and accuracy.\\n\\n\\nchores\\n\\nintroduced scripts for registry generation, node discovery, batch fixes, validation, and production readiness checks.\\n\\nimproved .gitignore to protect sensitive files and development artifacts.\\n\\n\\nstyle\\n\\n\\nstandardized node definition formats, naming conventions, and metadata for consistency and searchability.\\n\\n\\ntests\\n\\nadded automated test suites for node validation, server functionality, and mcp protocol compliance.\\ngenerated detailed test and validation reports confirming 100% success and production readiness.\\n\\n end of auto-generated comment: release notes by coderabbit.ai']"
        ],
        [
         "3",
         "3",
         "35",
         "3_benchmark_dotnet_msbuild_netperf",
         "['benchmark', 'dotnet', 'msbuild', 'netperf', 'performance', 'headersbench', 'nettrace', 'concurrent', 'multiproccl', 'buffer']",
         "['add comprehensive vibetunnel protocol benchmark toolfeatures:\\n- complete http api client implementation for vibetunnel protocol\\n- session management benchmarks (create/get/list/delete operations)\\n- sse streaming performance testing with latency measurements\\n- concurrent user load testing with realistic simulation\\n- support for custom hostname/port configuration\\n- detailed performance statistics and reporting\\ncommands:\\n- session: test session lifecycle performance\\n- stream: benchmark sse streaming latency/throughput\\n- load: concurrent user load testing\\ntested against both go (port 4031) and rust (port 4044) servers. tool successfully creates sessions and measures performance metrics.\\n generated with  code', 'enable parallel c++ compilationdescription\\nallow compiling multiple c++ sources in parallel.\\ntype of change\\n\\nnew feature (non-breaking change which adds functionality)\\n\\nwhy\\nbuild times in rnw have jumped to between 35 minutes and over 60 minutes in continuous integration.\\nwhat\\n\\ndefault msbuild property multiproccl to true.\\n\\ninitial tests (cached nuget packages, clean build and target directories) showed the following results on a windows dev box:\\n- multiproccl = false: ~37 minutes\\n- multiproccl = true: ~6 minutes\\nabout 82% speed-up.\\nnote\\nshould this setting apply to continuous integration?\\\\\\nwhile it would drastically improve build times, it may compromise predictability and reproducibility of builds.\\n###### microsoft reviewers: open in codeflow', 'test-tp: reference assembly loading fixesi noticed that with the current reference loading of typeproviders, if i had 139 reference assemblies (in a solution memory), i ended up calling the assembly.load for 897 times. this is because so many assemblies have the same references like system.memory, system.xml, system.buffers, system.threading.tasks.extensions, ... and the code said \"load all reference assemblies\". simple fix: check already loaded reference assemblies before trying to call the slow assembly.load again.\\nthe sourceassembliestable_ is a concurrentdictionary to ensure thread-safety. however, instead of code using it in thread-safe way, it was used by double-lookup. so that is fixed to actually use it properly. (it\\'s role is to be used as a guard to sourceassemblies_ array, which is manually lazy-loaded from the queue.)\\nthese changes match the fsharp.typeprovider.sdk merged pr.']"
        ],
        [
         "4",
         "4",
         "18",
         "4_builtins_bun_tests_typescript",
         "['builtins', 'bun', 'tests', 'typescript', 'builtin', 'debug', 'cjs', 'test', 'posix', 'binary64']",
         "[\"report memory cost of sourcemaps to gcwhat does this pr do?\\n **please explain what your changes do**, example: \\n\\n\\nthis adds a new flag --bail to bun test. when set, it will stop running tests after the first failure. this is useful for ci environments where you want to fail fast.\\n\\n\\n\\n[ ] documentation or typescript types (it's okay to leave the rest blank in this case)\\n[ ] code changes\\n\\nhow did you verify your code works?\\n **for code changes, please include automated tests**. feel free to uncomment the line below \\n i wrote automated tests \\n if javascript/typescript modules or builtins changed:\\n\\n- [ ] i included a test for the new code, or existing tests cover it\\n- [ ] i ran my tests locally and they pass (`bun-debug test test-file-name.test`)\\n\\n\\n if zig files changed:\\n\\n- [ ] i checked the lifetime of memory allocated to verify it's (1) freed and (2) only freed when it should be\\n- [ ] i included a test for the new code, or an existing test covers it\\n- [ ] jsvalue used outside outside of the stack is either wrapped in a jsc.strong or is jsvalueprotect'ed\\n- [ ] i wrote typescript/javascript tests and they pass locally (`bun-debug test test-file-name.test`)\\n\\n if new methods, getters, or setters were added to a publicly exposed class:\\n\\n- [ ] i added typescript types for the new methods, getters, or setters\\n\\n if dependencies in tests changed:\\n\\n- [ ] i made sure that specific versions of dependencies are used instead of ranged or tagged versions\\n\\n if a new builtin esm/cjs module was added:\\n\\n- [ ] i updated aliases in `module_loader.zig` to include the new module\\n- [ ] i added a test that imports the module\\n- [ ] i added a test that require() the module\", \"avoid encoding as double in napi_create_double if possiblewhat does this pr do?\\narithmetic on numbers encoded as doubles in jsc seems to hit more slow paths compared to numbertag numbers.\\nfixes #9218\\nwe might want to do this in other places. with this change in a debug build, fps goes from ~1 to ~100 on m4 max\\n\\n **please explain what your changes do**, example: \\n\\n\\nthis adds a new flag --bail to bun test. when set, it will stop running tests after the first failure. this is useful for ci environments where you want to fail fast.\\n\\n\\nhow did you verify your code works?\\n **for code changes, please include automated tests**. feel free to uncomment the line below \\n i wrote automated tests \\n if javascript/typescript modules or builtins changed:\\n\\n- [ ] i included a test for the new code, or existing tests cover it\\n- [ ] i ran my tests locally and they pass (`bun-debug test test-file-name.test`)\\n\\n\\n if zig files changed:\\n\\n- [ ] i checked the lifetime of memory allocated to verify it's (1) freed and (2) only freed when it should be\\n- [ ] i included a test for the new code, or an existing test covers it\\n- [ ] jsvalue used outside of the stack is either wrapped in a jsc.strong or is jsvalueprotect'ed\\n- [ ] i wrote typescript/javascript tests and they pass locally (`bun-debug test test-file-name.test`)\\n\\n if new methods, getters, or setters were added to a publicly exposed class:\\n\\n- [ ] i added typescript types for the new methods, getters, or setters\\n\\n if dependencies in tests changed:\\n\\n- [ ] i made sure that specific versions of dependencies are used instead of ranged or tagged versions\\n\\n if a new builtin esm/cjs module was added:\\n\\n- [ ] i updated aliases in `module_loader.zig` to include the new module\\n- [ ] i added a test that imports the module\\n- [ ] i added a test that require() the module\", \"fix(node: <URL>  resume when reading and avoid unnecessary pause/resumes callswhat does this pr do?\\nfix:  <URL> \\n **please explain what your changes do**, example: \\n\\n\\nthis adds a new flag --bail to bun test. when set, it will stop running tests after the first failure. this is useful for ci environments where you want to fail fast.\\n\\n\\n\\n[ ] documentation or typescript types (it's okay to leave the rest blank in this case)\\n[x] code changes\\n\\nhow did you verify your code works?\\ntests\\n **for code changes, please include automated tests**. feel free to uncomment the line below \\n i wrote automated tests \\n if javascript/typescript modules or builtins changed:\\n\\n- [ ] i included a test for the new code, or existing tests cover it\\n- [ ] i ran my tests locally and they pass (`bun-debug test test-file-name.test`)\\n\\n\\n if zig files changed:\\n\\n- [ ] i checked the lifetime of memory allocated to verify it's (1) freed and (2) only freed when it should be\\n- [ ] i included a test for the new code, or an existing test covers it\\n- [ ] jsvalue used outside of the stack is either wrapped in a jsc.strong or is jsvalueprotect'ed\\n- [ ] i wrote typescript/javascript tests and they pass locally (`bun-debug test test-file-name.test`)\\n\\n if new methods, getters, or setters were added to a publicly exposed class:\\n\\n- [ ] i added typescript types for the new methods, getters, or setters\\n\\n if dependencies in tests changed:\\n\\n- [ ] i made sure that specific versions of dependencies are used instead of ranged or tagged versions\\n\\n if a new builtin esm/cjs module was added:\\n\\n- [ ] i updated aliases in `module_loader.zig` to include the new module\\n- [ ] i added a test that imports the module\\n- [ ] i added a test that require() the module\"]"
        ],
        [
         "5",
         "5",
         "16",
         "5_parser_parse_chunksprefetch_preload",
         "['parser', 'parse', 'chunksprefetch', 'preload', 'resize', 'useoffthreadvideotexture', 'docker', 'usevideotexture', 'remotion', 'renderer']",
         "['@remotion/renderer: consider --memory flag from docker and warn on unrealistic valuesalso allow 50% of memory instead of just 40%', '@remotion/media-parser: ability to test remote files fast@remotion/media-parser: ability to test remote files fast', '@remotion/media-parser: refactor iso base media and matroska parser to be ready for seekingtodo: now tests / cis are freezing\\narray buffer resize is too big']"
        ],
        [
         "6",
         "6",
         "15",
         "6_cxx_compiler_c_compiler_cmake_compiler",
         "['cxx_compiler', 'c_compiler', 'cmake', 'compiler', 'torch_memory_saver', 'multithreaded', 'gcc', 'commit', 'int8', 'cache']",
         "['[mlas] dequantizelinear int8/uint8description\\n\\nadds multithreaded vectorized implementations of dequantizelinear for int8 and uint8 inputs:\\nintel sse 2\\narm neon\\nall other architectures fallback to a multithreaded scalar reference implementation (previous was not multithreaded).\\n\\nint8 dequantizelinear latency on intel core i9-10920x with 4 intra op threads (sse 2 implementation)\\n| number of elements | baseline latency (us) | multithreaded+simd latency (us) | speedup |\\n| ----------------------- | ---------------------- | ------------------------------------ | ---------- |\\n| 10 k                            | 1                               | 1                                                   | 1              |\\n| 20 k                            | 2                               | 2                                                   | 1              |\\n| 40 k                            | 5                               | 5                                                   | 1              |\\n| 80 k                            | 11                             | 4                                                   | 2.75         |\\n| 100 k                          | 14                             | 5                                                   | 2.80         |\\n| 150 k                          | 21                             | 7                                                   | 3.00         |\\n| 200 k                          | 28                             | 8                                                   | 3.50         |\\n| 400 k                          | 68                             | 15                                                 | 4.53         |\\n| 600 k                          | 107                           | 21                                                 | 5.10         |\\n| 800 k                          | 142                           | 28                                                 | 5.07         |\\n| 1 m                             | 187                           | 42                                                 | 4.45         |\\n| 2 m                             | 376                           | 102                                               | 3.69         |\\n| 4 m                             | 880                           | 236                                               | 3.73         |\\n| 6 m                             | 1547                         | 557                                               | 2.78         |\\n| 8 m                             | 2438                         | 1097                                             | 2.22         |\\n| 10 m                           | 3192                         | 1464                                             | 2.18         |\\n| 100 m                         | 38718                       | 17733                                           | 2.18         |\\nint8 dequantizelinear latency on snapdragon 8cx gen 3 @ 3.4ghz with 4 intra op threads (neon implementation)\\n| number of elements | baseline latency (us) | multithreaded+simd latency (us) | speedup |\\n| ----------------------- | ---------------------- | ------------------------------------ | ---------- |\\n| 10 k                            | 1                               | 1                                                   | 1              |\\n| 20 k                            | 1                               | 1                                                   | 1              |\\n| 40 k                            | 3                               | 3                                                   | 1              |\\n| 80 k                            | 7                               | 4                                                   | 1.75         |\\n| 100 k                          | 9                               | 3                                                   | 3.00         |\\n| 150 k                          | 14                             | 5                                                   | 2.80         |\\n| 200 k                          | 18                             | 6                                                   | 3.00         |\\n| 400 k                          | 38                             | 10                                                 | 3.80         |\\n| 600 k                          | 61                             | 15                                                 | 4.07         |\\n| 800 k                          | 76                             | 19                                                 | 4.00         |\\n| 1 m                             | 98                             | 24                                                 | 4.08         |\\n| 2 m                             | 204                           | 48                                                 | 4.25         |\\n| 4 m                             | 424                           | 112                                               | 3.79         |\\n| 6 m                             | 677                           | 384                                               | 1.76         |\\n| 8 m                             | 919                           | 621                                               | 1.48         |\\n| 10 m                           | 1132                         | 776                                               | 1.46         |\\n| 100 m                         | 11842                       | 10566                                           | 1.12         |\\nmotivation and context\\nimproves latency of quantized qdq models that with large dqs that dominate the inference latency.', \"[rollout] feat: support multi-stage awake for sglangco-authored with: mrata (immrata@gmail.com) and @zhaochenyang20 \\nchecklist before starting\\n\\n[x] search for similar pr(s).\\n\\nwhat does this pr do?\\nmotivation\\nin rl ecosystem which use colocate design like verl, we need to offload training model and load serving model & kv cache frequently.\\nbackground\\n\\ncurrently sglang is using torch_memory_saver to pause and resume.\\ntorch_memory_saver is a open source repo that provided easy to use api to hack cudamalloc and cudafree to make sure the virtual address could be consistent after pause and resume, which is critical to ensure cuda graph work.\\ncuda graph is critical to make sure sglang runs faster in decoding phases.\\n\\nhere is the current behavior of verl + sglang\\n\\n\\nduring training, we have training model and optimizer state in the gpu memory, and once training is done, we will offload optimizer state to cpu and keep the model weights in gpu, which is needed in update weight.\\nduring update weight, we awake the sglang engine, so those paused memory of model weights and kv cache will come back. then we update model from training model to serving model on the fly using the api: update_weights_in_tensor\\nafter model being updated, we delete the training model from gpu memory.\\n\\nabove design works pretty well so far, however, this would waste a big chunk of gpu memory during rollout, which could cause a few issues we've seen so far:\\n- small kv cache: we need to use relative lower number of mem fraction ratio (e.g: 0.6), hence our kv cache has less tokens. given kv cache has less tokens, we will hit runtimeerror: prefill out of memory. try to lower your batch size. when we try prefill large number of requests.\\n- out of memory: if we use mem fraction ratio 0.8 and run rl for 32b model on 8 h100, it will oom during update weight\\nchallenge\\n\\ntorch_memory_saver currently only supports singleton, hence sglang will pause and resume kv cache + weights together, they are treated as the same group of memory controlled by the singleton torch_memory_saver instance\\n\\nproposal\\n\\n\\nduring training, we do the same\\nduring update weight stage 1, we awake the model weights from sglang and then update weights\\nduring update weight stage 2, we delete the training model weights from gpu memory\\nawake the sglang's kv cache\\n\\n\\nbenefit\\nwith above feature, we can train larger model with same gpu, we can also make training/rollout more efficient given we can allocate larger kv cache\\nsolution: keep using singleton and provide tag based pause/resume\\n\\n[x] support tag based resume/pause:  <URL> \\n[x] support multiple stage awake in sglang:  <URL> \\n[ ] support multiple stage awake in verl:  <URL> \\n\\nhigh-level design\\n\\ndemonstrate the high-level design if this pr is complex.\\n\\nspecific changes\\n\\nlist the specific changes.\\n\\napi\\n\\ndemonstrate how the api changes if any.\\n\\nusage example\\n\\nprovide usage example(s) for easier usage.\\n\\n```python\\nadd code snippet or script demonstrating how to use this\\n```\\ntest\\n\\n\\nadditional info.\\n\\nissue number: fixes issue # or discussion # if any.\\ntraining: [note which backend this pr will affect: fsdp, megatron, both, or none]\\ninference: [note which backend this pr will affect: vllm, sglang, both, or none]\\n\\nchecklist before submitting\\n\\n[ ] read the contribute guide.\\n[ ] apply pre-commit checks.\\n[ ] add [breaking] to the pr title if it breaks any api.\\n[ ] update the documentation about your changes in the docs.\\n[ ] new ci unit test(s) are added to cover the code path.\\n[ ] rely on existing unit tests on ci that covers the code path.\", '[ibd] specialize block serializationthis change is part of [ibd] - tracking pr for speeding up initial block download\\n\\nthis pr is drafted until i remeasure everything after the recent merges and i need to find a way to simplify the 1 byte writes more nicely, i don\\'t like all the specializations.\\n\\nsummary\\nthis pr contain a few different optimization i found by ibd profiling, and via the newly added block seralization benchmarks. it also takes advantage of the recently merged std::span changes enabling propagating static extents.\\nthe commits merge similar (de)serialization methods, and separates them internally with  if constexpr - similarly to how it has been done here before. this enabled further sizecomputer optimizations as well.\\ncontext\\nother than these, since single byte writes are used very often (used for every (u)int8_t or std::byte or bool and for every varint\\'s first byte which is also needed for every (pre)vector), it makes sense to avoid the generalized serialization infrastructure that isn\\'t needed:\\n* autofile write doesn\\'t need to allocate 4k buffer for a single byte now;\\n* vectorwriter and datastream avoids memcpy/insert calls;\\n* csha256::write can avoid memcpy.\\ndeserializeblock is dominated by the hash calculations so the optimizations barely affect it.\\nmeasurements\\n\\nc compiler ............................ appleclang 16.0.0.16000026\\n\\n> before:\\n\\n|            ns/block |             block/s |    err% |     total | benchmark\\n|--------------------:|--------------------:|--------:|----------:|:----------\\n|          195,610.62 |            5,112.20 |    0.3% |     11.00 | `serializeblock`\\n|           12,061.83 |           82,906.19 |    0.1% |     11.01 | `sizecomputerblock`\\n\\n> after:\\n\\n|            ns/block |             block/s |    err% |     total | benchmark\\n|--------------------:|--------------------:|--------:|----------:|:----------\\n|          174,569.19 |            5,728.39 |    0.6% |     10.89 | `serializeblock`\\n|           10,241.16 |           97,645.21 |    0.0% |     11.00 | `sizecomputerblock`\\n\\n\\n\\nserializeblock - ~12.% faster\\nsizecomputerblock - ~17.7% faster\\n\\n\\n\\nc++ compiler .......................... gnu 13.3.0\\n\\n> before:\\n\\n|            ns/block |             block/s |    err% |       ins/block |       cyc/block |    ipc |      bra/block |   miss% |     total | benchmark\\n|--------------------:|--------------------:|--------:|----------------:|----------------:|-------:|---------------:|--------:|----------:|:----------\\n|          867,857.55 |            1,152.26 |    0.0% |    8,015,883.90 |    3,116,099.08 |  2.572 |   1,517,035.87 |    0.5% |     10.81 | `serializeblock`\\n|           30,928.27 |           32,332.88 |    0.0% |      221,683.03 |      111,055.84 |  1.996 |      53,037.03 |    0.8% |     11.03 | `sizecomputerblock`\\n\\n> after:\\n\\n|            ns/block |             block/s |    err% |       ins/block |       cyc/block |    ipc |      bra/block |   miss% |     total | benchmark\\n|--------------------:|--------------------:|--------:|----------------:|----------------:|-------:|---------------:|--------:|----------:|:----------\\n|          615,000.56 |            1,626.01 |    0.0% |    8,015,883.64 |    2,208,340.88 |  3.630 |   1,517,035.62 |    0.5% |     10.56 | `serializeblock`\\n|           25,676.76 |           38,945.72 |    0.0% |      159,390.03 |       92,202.10 |  1.729 |      42,131.03 |    0.9% |     11.00 | `sizecomputerblock`\\n\\n\\n\\nserializeblock - ~41.1% faster\\nsizecomputerblock - ~20.4% faster\\n\\n\\nwhile this wasn\\'t the main motivation for the change, ibd on ubuntu/gcc on ssd with i9 indicates a 2% speedup as well:\\n\\ndetails\\n\\n```bash\\ncommits=\"05314bde0b06b820225f10c6529b5afae128ff81 1cd94ec2511874ec68b92db34ad7ec7d9534fed1\"; \\\\\\nstop_height=880000; dbcache=10000; \\\\\\nc_compiler=gcc; cxx_compiler=g++; \\\\\\nhyperfine \\\\\\n--export-json \"/mnt/my_storage/ibd-${commits// /-}-${stop_height}-${dbcache}-${c_compiler}.json\" \\\\\\n--runs 3 \\\\\\n--parameter-list commit ${commits// /,} \\\\\\n--prepare \"killall bitcoind || true; rm -rf /mnt/my_storage/bitcoindata/*; git checkout {commit}; git clean -fxd; git reset --hard; cmake -b build -dcmake_build_type=release -denable_wallet=off -dcmake_c_compiler=$c_compiler -dcmake_cxx_compiler=$cxx_compiler && cmake --build build -j$(nproc) --target bitcoind && ./build/bin/bitcoind -datadir=/mnt/my_storage/bitcoindata -stopatheight=1 -printtoconsole=0 || true\" \\\\\\n--cleanup \"cp /mnt/my_storage/bitcoindata/debug.log /mnt/my_storage/logs/debug-{commit}-$(date +%s).log || true\" \\\\\\n\"compiler=$c_compiler commit={commit} ./build/bin/bitcoind -datadir=/mnt/my_storage/bitcoindata -stopatheight=$stop_height -dbcache=$dbcache -prune=550 -printtoconsole=0\"\\nbenchmark 1: compiler=gcc commit=05314bde0b06b820225f10c6529b5afae128ff81 ./build/bin/bitcoind -datadir=/mnt/my_storage/bitcoindata -stopatheight=880000 -dbcache=10000 -prune=550 -printtoconsole=0\\n  time (mean ± σ):     33647.918 s ± 508.655 s    [user: 71503.409 s, system: 4404.899 s]\\n  range (min … max):   33283.439 s … 34229.026 s    3 runs\\n\\nbenchmark 2: compiler=gcc commit=1cd94ec2511874ec68b92db34ad7ec7d9534fed1 ./build/bin/bitcoind -datadir=/mnt/my_storage/bitcoindata -stopatheight=880000 -dbcache=10000 -prune=550 -printtoconsole=0\\n  time (mean ± σ):     33062.491 s ± 183.335 s    [user: 71246.532 s, system: 4318.490 s]\\n  range (min … max):   32888.211 s … 33253.706 s    3 runs\\n\\nsummary\\n  compiler=gcc commit=1cd94ec2511874ec68b92db34ad7ec7d9534fed1 ./build/bin/bitcoind -datadir=/mnt/my_storage/bitcoindata -stopatheight=880000 -dbcache=10000 -prune=550 -printtoconsole=0 ran\\n    1.02 ± 0.02 times faster than compiler=gcc commit=05314bde0b06b820225f10c6529b5afae128ff81 ./build/bin/bitcoind -datadir=/mnt/my_storage/bitcoindata -stopatheight=880000 -dbcache=10000 -prune=550 -printtoconsole=0\\n```']"
        ],
        [
         "7",
         "7",
         "13",
         "7_refactor_caching_invoices_coderabbit",
         "['refactor', 'caching', 'invoices', 'coderabbit', 'improved', 'dashboard', 'optimizes', 'invoice', 'issues', 'release']",
         "[\"cached repetitive data lookups for creator analyticsbecause of how the code is structured, we create a separate creatoranalytics::web instance for every missing date range\\nthis then calls the products_for_creator_analytics method on a user, which returns a different relation each time, so query caching doesn't work\\ninstead, we can just calculate this once in the caching proxy and then pass it to the web instance\\ni'll refactor this properly in the future once the fix is confirmed good\\n\\n this is an auto-generated comment: release notes by coderabbit.ai \\nsummary by coderabbit\\n\\nperformance improvements\\nenhanced analytics performance by caching user’s first sale date and product data, resulting in faster loading times for analytics features.\\n\\n end of auto-generated comment: release notes by coderabbit.ai\", \"removed unnecessary api requests to fetch balancesrefs  <URL> \\n\\nsince the pr above, we've been including the balance/stats directly into the page instead of relying on loading them via secondary api requests\\nas a result, we're currently loading 4 different pages and not even using the result\\nsome of these requests are even just returning html because they don't have json equivalents\\nwe can do away with that and simplify the code in the process\\n\\n this is an auto-generated comment: release notes by coderabbit.ai \\nsummary by coderabbit\\n\\nrefactor\\nsimplified the statistics display by removing asynchronous fetching and progress indicators. the statistic value is now displayed directly from the provided value.\\nremoved clickable links and url-based navigation from the statistics components on the dashboard.\\nupdated font sizing and tooltip support for improved readability and usability.\\n\\n end of auto-generated comment: release notes by coderabbit.ai\", \"fix: recommended wallet listing when opening modal with namespace filterdescription\\n\\nfixes recommended wallets listing logics when opening the modal with the namespace filter: we've been showing evm wallets when opening the bitcoin modal. this was due to logic issues on how we are re-fetching the wallets when setting the namespace filter.\\noptimizes fetching wallets requests when not changing the namespace filter: we've been fetching wallets every time when we switch between namespace buttons on open / close modal, which is not good. optimises calls to fetch the wallets only when it's necessary\\n\\ntype of change\\n\\n[ ] chore (non-breaking change that addresses non-functional tasks, maintenance, or code quality improvements)\\n[x] bug fix (non-breaking change which fixes an issue)\\n[ ] new feature (non-breaking change which adds functionality)\\n[ ] breaking change (fix or feature that would cause existing functionality to not work as expected)\\n\\nassociated issues\\nfor linear issues: closes apkt-xxx\\nfor gh issues: closes #...\\nshowcase (optional)\\nif there is a ui change include the screenshots with before and after state.\\nif new feature is being introduced, include the link to demo recording.\\nchecklist\\n\\n[x] code in this pr is covered by automated tests (unit tests, e2e tests)\\n[x] my changes generate no new warnings\\n[x] i have reviewed my own code\\n[x] i have filled out all required sections\\n[x] i have tested my changes on the preview link\\n[ ] approver of this pr confirms that the changes are tested on the preview link\"]"
        ],
        [
         "8",
         "8",
         "12",
         "8_execute_gpt_4v_request_crewagentparser_jsonparser_pytest",
         "['execute_gpt_4v_request', 'crewagentparser', 'jsonparser', 'pytest', 'mockflow', 'parser', 'withfixedsizecache', 'parse_number', 'cache', 'tests']",
         "[' speed up function execute_gpt_4v_request by 100% in pr #1214 (-apikey-passthrough) this pull request contains optimizations for pr #1214\\nif you approve this dependent pr, these changes will be merged into the original pr branch -apikey-passthrough.\\n\\nthis pr will be automatically closed if the original pr is merged.\\n\\n\\n 100% (1.00x) speedup for execute_gpt_4v_request in inference/core/workflows/core_steps/models/foundation//v3.py\\n⏱ runtime :   107 milliseconds → 53.6 milliseconds (best of 5 runs)\\n explanation and details\\nhere is an optimized version of your program for runtime and memory. the majority of runtime is io/network-bound (api requests) and not cpu-bound code, so the best possible single-process cpu optimization is to avoid repeated work (e.g., repeated endpoint string formatting or client allocation) and simplify fast paths. if you can batch or async requests, that would reduce end-to-end latency, but that changes function signatures and semantics so is out of scope. here we focus on making your function as lean as possible within its expected use. \\nkey improvements:\\n- reuse  client () where possible: creating the client is surprisingly expensive per your profiling.\\n- optimize prompt and payload building: avoid unnecessary field-level assignments.\\n- use exception chaining efficiently.\\n- minimize calls to .startswith() by using a tuple form.\\n- precompute endpoint format string if possible.\\n- move non-error computations out of try/except.\\nsummary:\\n-  client creation is now cached, saving repeated cost.\\n- efficient prefix checking for  key.\\n- payloads & try/except blocks are trimmed for speed and clarity.\\n- function signatures and return values are preserved.\\n- comments are updated only where logic is improved or needs clarification.\\nif you control parallelism at a higher level, running requests in parallel (with asyncio or threading) would yield much higher throughput as both requests and  are io bound.\\n correctness verification report:\\n| test                        | status            |\\n| --------------------------- | ----------------- |\\n|  existing unit tests |  none found |\\n|  generated regression tests |  9 passed |\\n| ⏪ replay tests |  none found |\\n|  concolic coverage tests |  none found |\\n| tests coverage       |  |\\n\\n generated regression tests details\\n\\n```python\\nimport types\\nfrom typing import list, optional\\n\\n# imports\\nimport pytest  # used for our unit tests\\n# function to test\\nimport requests\\nfrom inference.core.env import api_base_url\\nfrom inference.core.workflows.core_steps.models.foundation..v3 import \\\\\\n    execute_gpt_4v_request\\nfrom  import \\nfrom ._types import not_given\\n\\n# unit tests\\n\\n\\n# --------- test helpers and monkeypatching ---------\\nclass dummyresponse:\\n    \"\"\"a dummy response object to simulate requests.response.\"\"\"\\n    def __init__(self, json_data=none, status_code=200, raise_exc=none, text=none):\\n        self._json_data = json_data or {}\\n        self.status_code = status_code\\n        self._raise_exc = raise_exc\\n        self.text = text or str(json_data)\\n    def json(self):\\n        return self._json_data\\n    def raise_for_status(self):\\n        if self._raise_exc:\\n            raise self._raise_exc\\n\\n# --------- basic test cases ---------\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\ndef test_proxied_request_missing_content(monkeypatch):\\n    \"\"\"test proxied request with missing \\'content\\' in response (should raise).\"\"\"\\n    def bad_post(url, json):\\n        return dummyresponse({\"choices\": [{\"message\": {}}]}, status_code=200)\\n    monkeypatch.setattr(requests, \"post\", bad_post)\\n    with pytest.raises(runtimeerror) as excinfo:\\n        execute_gpt_4v_request(\\n            roboflow_api_key=\"rfkey123\",\\n            _api_key=\"rf_key:account:abc\",\\n            prompt=[{\"role\": \"user\", \"content\": \"say hi\"}],\\n            gpt_model_version=\"gpt-4v\",\\n            max_tokens=10,\\n            temperature=0.5,\\n        )\\n\\ndef test_proxied_request_ <URL> \\n    \"\"\"test proxied request with http error (should raise).\"\"\"\\n    def bad_post(url, json):\\n        return dummyresponse({}, status_code=500, raise_exc=requests. <URL> \\n    monkeypatch.setattr(requests, \"post\", bad_post)\\n    with pytest.raises(runtimeerror) as excinfo:\\n        execute_gpt_4v_request(\\n            roboflow_api_key=\"rfkey123\",\\n            _api_key=\"rf_key:account:abc\",\\n            prompt=[{\"role\": \"user\", \"content\": \"say hi\"}],\\n            gpt_model_version=\"gpt-4v\",\\n            max_tokens=10,\\n            temperature=0.5,\\n        )\\n\\ndef test_direct_request_exception(monkeypatch):\\n    \"\"\"test direct request with  client raising exception (should raise).\"\"\"\\n    class failingclient:\\n        def __init__(self, api_key):\\n            pass\\n        @property\\n        def chat(self):\\n            class c:\\n                @property\\n                def completions(self):\\n                    class d:\\n                        def create(self, *a, **k):\\n                            raise exception(\" failure\")\\n                    return d()\\n            return c()\\n    monkeypatch.setattr(\".\", lambda api_key: failingclient(api_key))\\n    with pytest.raises(runtimeerror) as excinfo:\\n        execute_gpt_4v_request(\\n            roboflow_api_key=\"rfkey123\",\\n            _api_key=\"sk--002\",\\n            prompt=[{\"role\": \"user\", \"content\": \"say hi\"}],\\n            gpt_model_version=\"gpt-4v\",\\n            max_tokens=10,\\n            temperature=0.5,\\n        )\\n\\ndef test_proxied_request_index_error(monkeypatch):\\n    \"\"\"test proxied request with empty choices list (should raise).\"\"\"\\n    def bad_post(url, json):\\n        return dummyresponse({\"choices\": []}, status_code=200)\\n    monkeypatch.setattr(requests, \"post\", bad_post)\\n    with pytest.raises(runtimeerror) as excinfo:\\n        execute_gpt_4v_request(\\n            roboflow_api_key=\"rfkey123\",\\n            _api_key=\"rf_key:account:abc\",\\n            prompt=[{\"role\": \"user\", \"content\": \"say hi\"}],\\n            gpt_model_version=\"gpt-4v\",\\n            max_tokens=10,\\n            temperature=0.5,\\n        )\\n\\n# --------- large scale test cases ---------\\n\\n\\n\\n\\n\\n\\n\\n\\nimport types\\nfrom typing import list, optional\\n\\n# imports\\nimport pytest  # used for our unit tests\\n# function to test\\nimport requests\\nfrom inference.core.env import api_base_url\\nfrom inference.core.workflows.core_steps.models.foundation..v3 import \\\\\\n    execute_gpt_4v_request\\nfrom  import \\nfrom ._types import not_given\\n\\n# unit tests\\n\\n\\n# --- helpers for monkeypatching ---\\n\\nclass dummyresponse:\\n    def __init__(self, json_data, status_code=200):\\n        self._json = json_data\\n        self.status_code = status_code\\n        self.text = str(json_data)\\n    def json(self):\\n        return self._json\\n    def raise_for_status(self):\\n        if self.status_code >= 400:\\n            raise requests.exceptions. <URL>  {self.status_code}\")\\n\\nclass dummychoices:\\n    def __init__(self, content):\\n        self.message = types.simplenamespace(content=content)\\n\\nclass dummyresponse:\\n    def __init__(self, content):\\n        self.choices = [dummychoices(content)]\\n\\nclass dummychatcompletions:\\n    def __init__(self, content):\\n        self._content = content\\n    def create(self, model, messages, max_tokens, temperature):\\n        return dummyresponse(self._content)\\n\\n# --- test cases ---\\n\\n# basic test cases\\n\\n\\ndef test_proxied__basic(monkeypatch):\\n    \"\"\"test proxied  call with normal parameters.\"\"\"\\n    # patch requests.post to return a dummy response\\n    def dummy_post(url, json):\\n        return dummyresponse({\\n            \"choices\": [\\n                {\"message\": {\"content\": \"proxied hello\"}}\\n            ]\\n        })\\n    monkeypatch.setattr(requests, \"post\", dummy_post)\\n    # patch api_base_url to a dummy value for test\\n    monkeypatch.setattr(\"inference.core.env.api_base_url\", \" <URL> \\n    # call function with a proxied key\\n    codeflash_output = execute_gpt_4v_request(\\n        roboflow_api_key=\"rf_dummy\",\\n        _api_key=\"rf_key:account:abc123\",\\n        prompt=[{\"role\": \"user\", \"content\": \"say hello\"}],\\n        gpt_model_version=\"gpt-4-vision-preview\",\\n        max_tokens=10,\\n        temperature=0.5,\\n    ); result = codeflash_output\\n\\n\\ndef test_invalid__key(monkeypatch):\\n    \"\"\"test with an invalid  key (simulate exception from ).\"\"\"\\n    def dummy__init(self, api_key):\\n        raise exception(\"invalid api key\")\\n    monkeypatch.setattr(, \"__init__\", dummy__init)\\n    with pytest.raises(runtimeerror) as e:\\n        execute_gpt_4v_request(\\n            roboflow_api_key=\"rf_dummy\",\\n            _api_key=\"sk-bad\",\\n            prompt=[{\"role\": \"user\", \"content\": \"test\"}],\\n            gpt_model_version=\"gpt-4-vision-preview\",\\n            max_tokens=10,\\n            temperature=0.5,\\n        )\\n\\ndef test_proxied_ <URL> \\n    \"\"\"test proxied call with http error from requests.\"\"\"\\n    def dummy_post(url, json):\\n        return dummyresponse({}, status_code=500)\\n    monkeypatch.setattr(requests, \"post\", dummy_post)\\n    monkeypatch.setattr(\"inference.core.env.api_base_url\", \" <URL> \\n    with pytest.raises(runtimeerror) as e:\\n        execute_gpt_4v_request(\\n            roboflow_api_key=\"rf_dummy\",\\n            _api_key=\"rf_key:account:bad\",\\n            prompt=[{\"role\": \"user\", \"content\": \"test\"}],\\n            gpt_model_version=\"gpt-4-vision-preview\",\\n            max_tokens=10,\\n            temperature=0.5,\\n        )\\n\\ndef test_proxied_invalid_response_structure(monkeypatch):\\n    \"\"\"test proxied call with invalid response structure (missing keys).\"\"\"\\n    def dummy_post(url, json):\\n        return dummyresponse({\"bad\": \"data\"})\\n    monkeypatch.setattr(requests, \"post\", dummy_post)\\n    monkeypatch.setattr(\"inference.core.env.api_base_url\", \" <URL> \\n    with pytest.raises(runtimeerror) as e:\\n        execute_gpt_4v_request(\\n            roboflow_api_key=\"rf_dummy\",\\n            _api_key=\"rf_key:user:bad\",\\n            prompt=[{\"role\": \"user\", \"content\": \"test\"}],\\n            gpt_model_version=\"gpt-4-vision-preview\",\\n            max_tokens=10,\\n            temperature=0.5,\\n        )\\n\\n\\n\\n\\n\\n\\n\\ndef test_large_scale_proxied(monkeypatch):\\n    \"\"\"test proxied call with large prompt and max_tokens.\"\"\"\\n    large_prompt = [{\"role\": \"user\", \"content\": f\"message {i}\"} for i in range(900)]\\n    def dummy_post(url, json):\\n        return dummyresponse({\\n            \"choices\": [\\n                {\"message\": {\"content\": \"large proxied\"}}\\n            ]\\n        })\\n    monkeypatch.setattr(requests, \"post\", dummy_post)\\n    monkeypatch.setattr(\"inference.core.env.api_base_url\", \" <URL> \\n    codeflash_output = execute_gpt_4v_request(\\n        roboflow_api_key=\"rf_dummy\",\\n        _api_key=\"rf_key:account:abc123\",\\n        prompt=large_prompt,\\n        gpt_model_version=\"gpt-4-vision-preview\",\\n        max_tokens=999,\\n        temperature=0.5,\\n    ); result = codeflash_output\\n```\\n\\n\\nto edit these changes git checkout codeflash/optimize-pr1214-2025-05-14t16.32.54 and push.', ' speed up method jsonparser.parse_number by 17% 17% (0.17x) speedup for jsonparser.parse_number in src/json_repair/json_parser.py\\n⏱ runtime :   7.25 microseconds → 6.21 microseconds (best of 27 runs)\\n explanation and details\\nhere is an optimized version of your program, with a focus on reducing runtime and memory usage in the parse_number and get_char_at functions, as suggested by the profile data.\\nkey improvements.\\n- avoid unnecessary set creation inside hot loops.\\n- minimize attribute access in tight loops (local variable caching for self.get_char_at rather than repeated method call).\\n- reduce string concatenation inside loops by collecting characters into a list and joining once at the end.\\nall comments are preserved except those adjacent to changed lines, which are updated if relevant.\\nkey optimization notes:\\n- parse_number now uses a number_chars list to gather characters, avoiding \"string + char\" concatenation which is o(n²) in python.\\n- get_char_at is bound to a local variable to avoid repeated attribute/method lookup inside the loop.\\n- direct string \"0123456789-.ee/,\" is used for membership check instead of recreating a set each call, as the set is tiny and in-string checks are fast for small sets.\\nall changes preserve existing functionality and logging behavior. the rest of the code remains unmodified (other than whitespace for style).  \\nyou can further optimize if profiling shows parse_string as another hot spot,\\nbut the major bottleneck per your profile was only in parse_number and get_char_at.\\n correctness verification report:\\n| test                        | status            |\\n| --------------------------- | ----------------- |\\n|  existing unit tests |  none found |\\n|  generated regression tests |  97 passed |\\n| ⏪ replay tests |  none found |\\n|  concolic coverage tests |  4 passed |\\n| tests coverage       | 81.8% |\\n\\n generated regression tests details\\n\\n```python\\nfrom typing import any, classvar, literal\\n\\n# imports\\nimport pytest  # used for our unit tests\\nfrom src.json_repair.json_parser import jsonparser\\n\\n\\n# minimal stubs for contextvalues and jsoncontext to allow testing\\nclass contextvalues:\\n    array = \"array\"\\n    object_key = \"object_key\"\\n    object_value = \"object_value\"\\n\\n# unit tests\\n\\n@pytest.mark.parametrize(\\n    \"input_str,expected,desc\",\\n    [\\n        # basic integer\\n        (\"123\", 123, \"simple integer\"),\\n        (\"0\", 0, \"zero integer\"),\\n        (\"-42\", -42, \"negative integer\"),\\n        # basic float\\n        (\"3.14\", 3.14, \"simple float\"),\\n        (\"-0.001\", -0.001, \"negative float\"),\\n        (\"0.0\", 0.0, \"zero float\"),\\n        # basic scientific notation\\n        (\"1e3\", 1000.0, \"scientific notation positive exponent\"),\\n        (\"-2.5e-2\", -0.025, \"scientific notation negative exponent\"),\\n        (\"6.02e23\", 6.02e23, \"large scientific notation\"),\\n        # number with trailing non-numeric\\n        (\"42abc\", \"42\", \"number followed by alpha (should fallback to string)\"),\\n        (\"3.14pie\", \"3.14\", \"float followed by alpha (should fallback to string)\"),\\n        (\"-123.45xyz\", \"-123.45\", \"negative float followed by alpha\"),\\n        # number with comma (should return as string)\\n        (\"1,234\", \"1,234\", \"number with comma\"),\\n        (\"12,345.67\", \"12,345.67\", \"float with comma\"),\\n        # number ending with invalid char\\n        (\"789-\", 789, \"number ending with - (should ignore)\"),\\n        (\"5.3e/\", 5.3, \"number ending with / (should ignore)\"),\\n        (\"10e,\", 10, \"number ending with , (should ignore)\"),\\n        # edge: only sign\\n        (\"-\", \"\", \"just a minus sign\"),\\n        # edge: only decimal point\\n        (\".\", \"\", \"just a dot\"),\\n        # edge: only exponent\\n        (\"e\", \"\", \"just an exponent\"),\\n        # edge: empty string\\n        (\"\", \"\", \"empty input\"),\\n        # edge: multiple dots\\n        (\"1.2.3\", 1.2, \"multiple dots, should parse up to second dot\"),\\n        # edge: multiple exponents\\n        (\"1e2e3\", 100.0, \"multiple exponents, should parse up to second e\"),\\n        # edge: leading zeros\\n        (\"000123\", 123, \"leading zeros\"),\\n        (\"000.456\", 0.456, \"leading zeros in float\"),\\n        # edge: negative zero\\n        (\"-0\", 0, \"negative zero\"),\\n        # edge: large integer\\n        (\"999999999\", 999999999, \"large integer\"),\\n        # edge: large negative integer\\n        (\"-999999999\", -999999999, \"large negative integer\"),\\n        # edge: large float\\n        (\"3.141592653589793238\", 3.141592653589793, \"very long float (python float precision)\"),\\n        # edge: array context, should stop at comma\\n        (\"123,456\", 123, \"array context, stops at comma\"),\\n        # edge: array context, with negative number\\n        (\"-789,123\", -789, \"array context, negative number stops at comma\"),\\n        # edge: array context, float\\n        (\"1.23,4.56\", 1.23, \"array context, float stops at comma\"),\\n        # edge: number with leading plus (should fail to parse as number)\\n        (\"+123\", \"\", \"leading plus is not handled, should return empty string\"),\\n        # edge: number with embedded whitespace\\n        (\"12 34\", 12, \"whitespace breaks number parsing\"),\\n        # edge: number with tab\\n        (\"56\\\\t78\", 56, \"tab breaks number parsing\"),\\n        # edge: negative float with exponent\\n        (\"-1.23e-4\", -1.23e-4, \"negative float with exponent\"),\\n        # edge: float with positive exponent\\n        (\"2.5e+3\", 2500.0, \"float with explicit positive exponent\"),\\n        # edge: float with exponent and trailing chars\\n        (\"7.89e2abc\", \"7.89e2\", \"float with exponent and trailing alpha\"),\\n        # edge: number with multiple commas\\n        (\"1,234,567\", \"1,234,567\", \"number with multiple commas\"),\\n        # edge: number with trailing whitespace\\n        (\"123 \", 123, \"number with trailing space\"),\\n        # edge: number with leading whitespace\\n        (\" 456\", \"\", \"leading whitespace not handled, should return empty string\"),\\n        # edge: negative sign only\\n        (\"-\", \"\", \"just a negative sign\"),\\n        # edge: dot only\\n        (\".\", \"\", \"just a dot\"),\\n        # edge: exponent only\\n        (\"e\", \"\", \"just an e\"),\\n        # edge: negative exponent only\\n        (\"-e\", \"\", \"negative sign and e\"),\\n        # edge: float with no leading digit\\n        (\".5\", 0.5, \"float with no leading digit\"),\\n        # edge: float with no trailing digit\\n        (\"5.\", 5.0, \"float with no trailing digit\"),\\n        # edge: number with slash (should ignore slash)\\n        (\"123/456\", 123, \"slash breaks number parsing\"),\\n        # edge: number with multiple slashes\\n        (\"12/34/56\", 12, \"multiple slashes break parsing\"),\\n        # edge: number with embedded dash\\n        (\"12-34\", 12, \"dash in the middle breaks parsing\"),\\n        # edge: number with multiple exponents (invalid)\\n        (\"1e2e3\", 100.0, \"multiple exponents, only first is parsed\"),\\n        # edge: number with trailing dot and comma\\n        (\"5.,\", 5.0, \"number ending with dot and comma\"),\\n        # edge: number with trailing dot and slash\\n        (\"5./\", 5.0, \"number ending with dot and slash\"),\\n        # edge: number with trailing e and comma\\n        (\"5e,\", 5, \"number ending with e and comma\"),\\n        # edge: number with trailing e and slash\\n        (\"5e/\", 5, \"number ending with e and slash\"),\\n        # edge: number with trailing dash and comma\\n        (\"5-,\", 5, \"number ending with dash and comma\"),\\n        # edge: number with trailing dash and slash\\n        (\"5-/\", 5, \"number ending with dash and slash\"),\\n        # edge: number with comma at start (should return empty string)\\n        (\",123\", \"\", \"comma at start, should return empty string\"),\\n        # edge: number with dot at start (should return empty string)\\n        (\".123\", 0.123, \"dot at start, float with no leading digit\"),\\n        # edge: number with only comma\\n        (\",\", \"\", \"only comma\"),\\n        # edge: number with only dash\\n        (\"-\", \"\", \"only dash\"),\\n        # edge: number with only slash\\n        (\"/\", \"\", \"only slash\"),\\n    ]\\n)\\ndef test_parse_number_basic_and_edge(input_str, expected, desc):\\n    \"\"\"\\n    test basic and edge cases for parse_number.\\n    \"\"\"\\n    parser = jsonparser(input_str)\\n    codeflash_output = parser.parse_number(); result = codeflash_output\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nfrom typing import any, classvar, literal\\n\\n# imports\\nimport pytest\\nfrom src.json_repair.json_parser import jsonparser\\n\\n\\n# dummy contextvalues and jsoncontext for test purposes\\nclass contextvalues:\\n    object_key = \"object_key\"\\n    object_value = \"object_value\"\\n    array = \"array\"\\n\\n# unit tests\\n\\n# -------------------------\\n# 1. basic test cases\\n# -------------------------\\n\\n@pytest.mark.parametrize(\\n    \"input_str,expected\",\\n    [\\n        # integer\\n        (\"123\", 123),\\n        (\"0\", 0),\\n        (\"-42\", -42),\\n        # float\\n        (\"3.14\", 3.14),\\n        (\"-0.001\", -0.001),\\n        (\"2e3\", 2000.0),\\n        (\"-2e-2\", -0.02),\\n        # leading zeros (should parse as int)\\n        (\"007\", 7),\\n        # trailing whitespace (should ignore)\\n        (\"123 \", 123),\\n        # number with comma (should return as string)\\n        (\"1,234\", \"1,234\"),\\n        # number with trailing comma (should ignore comma)\\n        (\"123,\", 123),\\n        # number with trailing slash (should ignore slash)\\n        (\"123/\", 123),\\n        # number with trailing e (should ignore e)\\n        (\"123e\", 123),\\n        # number with trailing - (should ignore -)\\n        (\"123-\", 123),\\n        # negative float with exponent\\n        (\"-1.23e-10\", -1.23e-10),\\n        # float with positive exponent\\n        (\"1.23e+10\", 1.23e10),\\n    ]\\n)\\ndef test_parse_number_basic(input_str, expected):\\n    parser = jsonparser(input_str)\\n    codeflash_output = parser.parse_number(); result = codeflash_output\\n    if isinstance(expected, float):\\n        pass\\n    else:\\n        pass\\n\\n# -------------------------\\n# 2. edge test cases\\n# -------------------------\\n\\n@pytest.mark.parametrize(\\n    \"input_str,expected\",\\n    [\\n        # empty string\\n        (\"\", \"\"),\\n        # only minus sign\\n        (\"-\", \"\"),\\n        # only dot\\n        (\".\", \"\"),\\n        # only exponent\\n        (\"e\", \"\"),\\n        # only comma\\n        (\",\", \"\"),\\n        # only slash\\n        (\"/\", \"\"),\\n        # multiple dots (invalid float)\\n        (\"1.2.3\", \"1.2.3\"),\\n        # multiple exponents (invalid float)\\n        (\"1e2e3\", \"1e2e3\"),\\n        # number followed by alpha (should call parse_string, so returns as string)\\n        (\"123abc\", \"123abc\"),\\n        # number with comma in array context (should stop at comma)\\n        (\"123,456\", 123),\\n        # negative number with trailing comma\\n        (\"-42,\", -42),\\n        # negative float with trailing comma\\n        (\"-42.5,\", -42.5),\\n        # number with embedded slash (invalid, returns as string)\\n        (\"12/34\", \"12/34\"),\\n        # number with embedded comma (invalid, returns as string)\\n        (\"12,34\", \"12,34\"),\\n        # large negative exponent\\n        (\"1e-308\", 1e-308),\\n        # large positive exponent\\n        (\"1e308\", 1e308),\\n        # number with leading whitespace (should parse correctly)\\n        (\"   42\", 42),\\n        # number with trailing whitespace and comma\\n        (\"42 ,\", 42),\\n        # number with leading/trailing whitespace\\n        (\"  42  \", 42),\\n    ]\\n)\\ndef test_parse_number_edge(input_str, expected):\\n    parser = jsonparser(input_str.strip())\\n    codeflash_output = parser.parse_number(); result = codeflash_output\\n    if isinstance(expected, float):\\n        pass\\n    else:\\n        pass\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nfrom src.json_repair.json_parser import jsonparser\\n\\ndef test_jsonparser_parse_number():\\n    jsonparser.parse_number(jsonparser(\\'e\\', none, false, json_fd_chunk_length=0, stream_stable=true))\\n\\ndef test_jsonparser_parse_number_2():\\n    jsonparser.parse_number(jsonparser(\\'53\\', none, none, json_fd_chunk_length=0, stream_stable=false))\\n```\\n\\n\\nto edit these changes git checkout codeflash/optimize-jsonparser.parse_number-maqpo82d and push.', ' speed up method withfixedsizecache.add_model by 50% in pr #1373 (feat/pass-countinference-to-serverless-getweights) this pull request contains optimizations for pr #1373\\nif you approve this dependent pr, these changes will be merged into the original pr branch feat/pass-countinference-to-serverless-getweights.\\n\\nthis pr will be automatically closed if the original pr is merged.\\n\\n\\n 50% (0.50x) speedup for withfixedsizecache.add_model in inference/core/managers/decorators/fixed_size_cache.py\\n⏱ runtime :   1.08 seconds → 722 milliseconds (best of 12 runs)\\n explanation and details\\nhere\\'s an optimized rewrite of your program, addressing profiling hot spots and general efficiency improvements.\\noptimization summary:\\n\\navoid redundant method calls: \\nminimize repeated lookups and calculations.\\ncache computations/results when possible within function scope.\\nlazy imports: \\nmove gc and optional torch imports where needed (they are only used upon eviction).\\ndeque optimizations: \\nin withfixedsizecache.add_model, avoid repeated self._key_queue.remove(queue_id) by checking position or maintaining a set for fast checks (no need, since only called if known present, and block is rare). still, code can be reduced for clarity.\\nreduce logging in the hot add logic (unless debug mode; logging is a major time sink during profiling).\\nbatch removals: \\naccumulate models to remove and do a single gc.collect() call after, instead of per-iteration. \\ndata structure choices are left unchanged (deque is still best for explicit ordering here).\\ngeneral logic: use local variables for lookups on attributes used multiple times (minor, but helps).\\n\\n\\nkey runtime optimizations:\\n- only call gc.collect() after all removals in a batch, not after every single model eviction.\\n- reduced logging in hot code paths (this was responsible for noticeable time in profiling).\\n- use local variables when repeatedly accessing class attributes.\\n- use direct inlining for _resolve_queue_id for this use case.\\n- defensive handling if queue/model state falls out of sync—never throws unnecessarily.\\nperformance note:\\nif you profile again after these changes, most of the time will now be in actual model loading and removal. that is, this code will not be a noticeable bottleneck anymore in the workflow. if lru cache size is much larger, consider further data structure optimizations such as a dict for constant-time eviction and presence checking, but for n ~ 8 this is not needed.\\n correctness verification report:\\n| test                        | status            |\\n| --------------------------- | ----------------- |\\n| ⏪ replay tests |  none found |\\n|  existing unit tests |  none found |\\n|  concolic coverage tests |  none found |\\n|  generated regression tests |  476 passed |\\n| tests coverage       | 85.2% |\\n\\n generated regression tests and runtime\\n\\n```python\\nimport sys\\nfrom collections import deque\\n\\n# imports\\nimport pytest\\nfrom inference.core.managers.decorators.fixed_size_cache import \\\\\\n    withfixedsizecache\\n\\n# function to test and minimal stubs/mocks\\n\\nclass dummymodel:\\n    \"\"\"minimal dummy model for testing.\"\"\"\\n    def __init__(self, model_id, api_key):\\n        self.model_id = model_id\\n        self.api_key = api_key\\n        self.has_model_metadata = false\\n\\n    def clear_cache(self, delete_from_disk=true):\\n        pass\\n\\nclass dummymodelregistry:\\n    \"\"\"minimal dummy registry that returns dummymodel.\"\"\"\\n    def get_model(self, resolved_identifier, api_key, countinference=none, service_secret=none):\\n        return dummymodel\\nclass inferencemodelnotfound(exception): pass\\nclass invalidmodeliderror(exception): pass\\n\\n# enum stub\\nclass modelendpointtype:\\n    ort = type(\"ort\", (), {\"value\": \"ort\"})()\\n    value = \"ort\"\\n\\n# modelmanager and withfixedsizecache as in prompt, but minimal\\nclass modelmanager:\\n    def __init__(self, model_registry, models=none):\\n        self.model_registry = model_registry\\n        self._models = models if models is not none else {}\\n\\n    def add_model(self, model_id, api_key, model_id_alias=none, endpoint_type=modelendpointtype.ort, countinference=none, service_secret=none):\\n        resolved_identifier = model_id if model_id_alias is none else model_id_alias\\n        if resolved_identifier in self._models:\\n            return\\n        model_class = self.model_registry.get_model(resolved_identifier, api_key, countinference=countinference, service_secret=service_secret)\\n        model = model_class(model_id=model_id, api_key=api_key)\\n        self._models[resolved_identifier] = model\\n\\n    def remove(self, model_id, delete_from_disk=true):\\n        if model_id not in self._models:\\n            raise inferencemodelnotfound()\\n        self._models[model_id].clear_cache(delete_from_disk=delete_from_disk)\\n        del self._models[model_id]\\n\\n    def __contains__(self, model_id):\\n        return model_id in self._models\\n\\n    def __getitem__(self, key):\\n        if key not in self._models:\\n            raise inferencemodelnotfound()\\n        return self._models[key]\\n\\n    def __len__(self):\\n        return len(self._models)\\n\\n    def keys(self):\\n        return self._models.keys()\\n\\n# ========== unit tests below ==========\\n\\n@pytest.fixture\\ndef cache_manager():\\n    \"\"\"returns a withfixedsizecache with max_size=3 for testing.\"\"\"\\n    registry = dummymodelregistry()\\n    base_manager = modelmanager(registry)\\n    return withfixedsizecache(base_manager, max_size=3)\\n\\n@pytest.fixture\\ndef unique_model_id():\\n    \"\"\"returns a function to generate unique model_ids for tests.\"\"\"\\n    counter = [0]\\n    def _gen():\\n        counter[0] += 1\\n        return f\"dataset{counter[0]}/1\"\\n    return _gen\\n\\n# 1. basic test cases\\n\\ndef test_add_single_model(cache_manager, unique_model_id):\\n    \"\"\"test adding a single model works and is present.\"\"\"\\n    model_id = unique_model_id()\\n    cache_manager.add_model(model_id, api_key=\"key\")\\n\\ndef test_add_duplicate_model_noop(cache_manager, unique_model_id):\\n    \"\"\"adding the same model twice does not increase count.\"\"\"\\n    model_id = unique_model_id()\\n    cache_manager.add_model(model_id, api_key=\"key\")\\n    cache_manager.add_model(model_id, api_key=\"key\")\\n\\ndef test_add_model_with_alias(cache_manager, unique_model_id):\\n    \"\"\"adding with an alias stores under the alias, not model_id.\"\"\"\\n    model_id = unique_model_id()\\n    alias = \"alias1\"\\n    cache_manager.add_model(model_id, api_key=\"key\", model_id_alias=alias)\\n\\ndef test_add_multiple_models_up_to_capacity(cache_manager, unique_model_id):\\n    \"\"\"add up to max_size models, all should be present.\"\"\"\\n    ids = [unique_model_id() for _ in range(3)]\\n    for mid in ids:\\n        cache_manager.add_model(mid, api_key=\"key\")\\n    for mid in ids:\\n        pass\\n\\n# 2. edge test cases\\n\\ndef test_eviction_on_capacity(cache_manager, unique_model_id):\\n    \"\"\"adding more than max_size evicts least recently used.\"\"\"\\n    ids = [unique_model_id() for _ in range(4)]\\n    for mid in ids[:3]:\\n        cache_manager.add_model(mid, api_key=\"key\")\\n    # now add a 4th, should evict ids[0]\\n    cache_manager.add_model(ids[3], api_key=\"key\")\\n\\ndef test_eviction_marks_mru(cache_manager, unique_model_id):\\n    \"\"\"adding a model again marks it as most recently used (no eviction).\"\"\"\\n    ids = [unique_model_id() for _ in range(3)]\\n    for mid in ids:\\n        cache_manager.add_model(mid, api_key=\"key\")\\n    # access ids[0] to mark it as mru\\n    cache_manager.add_model(ids[0], api_key=\"key\")\\n    # add new model, should evict ids[1] now (was lru)\\n    new_id = unique_model_id()\\n    cache_manager.add_model(new_id, api_key=\"key\")\\n\\ndef test_add_model_with_alias_then_same_id(cache_manager, unique_model_id):\\n    \"\"\"adding with alias, then with same model_id, both can exist.\"\"\"\\n    model_id = unique_model_id()\\n    alias = \"alias2\"\\n    cache_manager.add_model(model_id, api_key=\"key\", model_id_alias=alias)\\n    cache_manager.add_model(model_id, api_key=\"key\")\\n\\ndef test_add_model_eviction_multiple_rounds(cache_manager, unique_model_id):\\n    \"\"\"eviction removes 3 at a time if possible when over threshold.\"\"\"\\n    # fill up to 3\\n    ids = [unique_model_id() for _ in range(3)]\\n    for mid in ids:\\n        cache_manager.add_model(mid, api_key=\"key\")\\n    # add 4th, should evict 1st\\n    cache_manager.add_model(\"dataset999/1\", api_key=\"key\")\\n    # add 5th, should evict 3 more (but only 3 in cache, so only possible to evict all)\\n    cache_manager.add_model(\"dataset1000/1\", api_key=\"key\")\\n\\ndef test_remove_model(cache_manager, unique_model_id):\\n    \"\"\"test removing a model actually removes it.\"\"\"\\n    model_id = unique_model_id()\\n    cache_manager.add_model(model_id, api_key=\"key\")\\n    cache_manager.remove(model_id)\\n\\ndef test_remove_nonexistent_model_raises(cache_manager):\\n    \"\"\"removing a model not present raises inferencemodelnotfound.\"\"\"\\n    with pytest.raises(inferencemodelnotfound):\\n        cache_manager.remove(\"not-present/1\")\\n\\n\\ndef test_add_model_with_alias_eviction(cache_manager, unique_model_id):\\n    \"\"\"eviction works when models are added by alias.\"\"\"\\n    ids = [unique_model_id() for _ in range(2)]\\n    alias = \"alias3\"\\n    cache_manager.add_model(ids[0], api_key=\"key\", model_id_alias=alias)\\n    cache_manager.add_model(ids[1], api_key=\"key\")\\n    cache_manager.add_model(\"dataset888/1\", api_key=\"key\")\\n    # now add another to force eviction\\n    cache_manager.add_model(\"dataset889/1\", api_key=\"key\")\\n    # at least one of the first 3 should be evicted\\n    count = sum(mid in cache_manager for mid in [alias, ids[1], \"dataset888/1\"])\\n\\ndef test_lru_eviction_order(cache_manager, unique_model_id):\\n    \"\"\"eviction order is lru, not fifo.\"\"\"\\n    ids = [unique_model_id() for _ in range(3)]\\n    for mid in ids:\\n        cache_manager.add_model(mid, api_key=\"key\")\\n    # access ids[1] to make it mru\\n    cache_manager.add_model(ids[1], api_key=\"key\")\\n    # add new model, should evict ids[0]\\n    new_id = unique_model_id()\\n    cache_manager.add_model(new_id, api_key=\"key\")\\n\\ndef test_add_model_memory_pressure(monkeypatch, cache_manager, unique_model_id):\\n    \"\"\"if memory_pressure_detected returns true, eviction is triggered.\"\"\"\\n    monkeypatch.setattr(cache_manager, \"memory_pressure_detected\", lambda: true)\\n    # fill up cache\\n    ids = [unique_model_id() for _ in range(3)]\\n    for mid in ids:\\n        cache_manager.add_model(mid, api_key=\"key\")\\n    # add another, should evict 3 at once\\n    cache_manager.add_model(\"dataset2000/1\", api_key=\"key\")\\n\\ndef test_add_model_exception_removes_from_queue(cache_manager, monkeypatch):\\n    \"\"\"if add_model raises, queue is cleaned up.\"\"\"\\n    # patch model_manager.add_model to raise\\n    def raise_exc(*a, **kw): raise runtimeerror(\"fail!\")\\n    monkeypatch.setattr(cache_manager.model_manager, \"add_model\", raise_exc)\\n    before_len = len(cache_manager._key_queue)\\n    with pytest.raises(runtimeerror):\\n        cache_manager.add_model(\"dataset/1\", api_key=\"key\")\\n\\n# 3. large scale test cases\\n\\ndef test_large_number_of_models_eviction():\\n    \"\"\"add 10 models to a cache of size 5, only last 5 remain.\"\"\"\\n    registry = dummymodelregistry()\\n    base_manager = modelmanager(registry)\\n    cache_manager = withfixedsizecache(base_manager, max_size=5)\\n    ids = [f\"ds{i}/1\" for i in range(10)]\\n    for mid in ids:\\n        cache_manager.add_model(mid, api_key=\"key\")\\n    # only last 5 should remain\\n    for mid in ids[:5]:\\n        pass\\n    for mid in ids[5:]:\\n        pass\\n\\ndef test_stress_add_and_access():\\n    \"\"\"add 20 models, repeatedly access some to keep them in cache.\"\"\"\\n    registry = dummymodelregistry()\\n    base_manager = modelmanager(registry)\\n    cache_manager = withfixedsizecache(base_manager, max_size=10)\\n    ids = [f\"ds{i}/1\" for i in range(20)]\\n    for mid in ids[:10]:\\n        cache_manager.add_model(mid, api_key=\"key\")\\n    # repeatedly access first 5 to keep them mru\\n    for _ in range(5):\\n        for mid in ids[:5]:\\n            cache_manager.add_model(mid, api_key=\"key\")\\n    # add next 10\\n    for mid in ids[10:]:\\n        cache_manager.add_model(mid, api_key=\"key\")\\n    # the first 5 should still be in cache, next 5 should have been evicted\\n    for mid in ids[:5]:\\n        pass\\n    for mid in ids[5:10]:\\n        pass\\n    for mid in ids[10:]:\\n        pass\\n\\ndef test_add_models_with_aliases_large_scale():\\n    \"\"\"add 50 models with unique aliases, only last 10 remain in cache.\"\"\"\\n    registry = dummymodelregistry()\\n    base_manager = modelmanager(registry)\\n    cache_manager = withfixedsizecache(base_manager, max_size=10)\\n    for i in range(50):\\n        model_id = f\"dataset{i}/1\"\\n        alias = f\"alias{i}\"\\n        cache_manager.add_model(model_id, api_key=\"key\", model_id_alias=alias)\\n    # only last 10 aliases should be present\\n    for i in range(40):\\n        pass\\n    for i in range(40, 50):\\n        pass\\n\\ndef test_eviction_never_exceeds_max_size():\\n    \"\"\"after many operations, cache never exceeds max_size.\"\"\"\\n    registry = dummymodelregistry()\\n    base_manager = modelmanager(registry)\\n    cache_manager = withfixedsizecache(base_manager, max_size=7)\\n    for i in range(30):\\n        cache_manager.add_model(f\"ds{i}/1\", api_key=\"key\")\\n\\ndef test_eviction_when_queue_empty_does_not_crash():\\n    \"\"\"eviction with empty queue does not raise.\"\"\"\\n    registry = dummymodelregistry()\\n    base_manager = modelmanager(registry)\\n    cache_manager = withfixedsizecache(base_manager, max_size=1)\\n    # remove all models to empty queue\\n    cache_manager._key_queue.clear()\\n    try:\\n        cache_manager.add_model(\"ds1/1\", api_key=\"key\")\\n    except exception:\\n        pytest.fail(\"add_model should not raise when queue is empty\")\\n# codeflash_output is used to check that the output of the original code is the same as that of the optimized code.\\n\\nfrom collections import deque\\n\\n# imports\\nimport pytest\\nfrom inference.core.managers.decorators.fixed_size_cache import \\\\\\n    withfixedsizecache\\n\\n# --- minimal stubs and mocks for dependencies ---\\n\\n# exception classes\\nclass roboflowapinotauthorizederror(exception):\\n    pass\\n\\nclass inferencemodelnotfound(exception):\\n    pass\\n\\n# modelendpointtype enum stub\\nclass modelendpointtype:\\n    ort = \"ort\"\\n\\n# model stub\\nclass dummymodel:\\n    def __init__(self, model_id, api_key):\\n        self.model_id = model_id\\n        self.api_key = api_key\\n        self.cleared = false\\n\\n    def clear_cache(self, delete_from_disk=true):\\n        self.cleared = true\\n\\n# modelregistry stub\\nclass dummymodelregistry:\\n    def get_model(self, resolved_identifier, api_key, countinference=none, service_secret=none):\\n        # always returns dummymodel constructor\\n        return dummymodel\\n\\n# --- the modelmanager, modelmanagerdecorator, and withfixedsizecache implementations ---\\n\\nclass modelmanager:\\n    def __init__(self, model_registry, models=none):\\n        self.model_registry = model_registry\\n        self._models = {} if models is none else models\\n\\n    def add_model(\\n        self,\\n        model_id,\\n        api_key,\\n        model_id_alias=none,\\n        endpoint_type=modelendpointtype.ort,\\n        countinference=none,\\n        service_secret=none,\\n    ):\\n        resolved_identifier = model_id if model_id_alias is none else model_id_alias\\n        if resolved_identifier in self._models:\\n            return\\n        model_class = self.model_registry.get_model(\\n            resolved_identifier, api_key, countinference=countinference, service_secret=service_secret\\n        )\\n        model = model_class(model_id=model_id, api_key=api_key)\\n        self._models[resolved_identifier] = model\\n\\n    def remove(self, model_id, delete_from_disk=true):\\n        if model_id not in self._models:\\n            raise inferencemodelnotfound(f\"model {model_id} not found\")\\n        self._models[model_id].clear_cache(delete_from_disk=delete_from_disk)\\n        del self._models[model_id]\\n\\n    def __contains__(self, model_id):\\n        return model_id in self._models\\n\\n    def __getitem__(self, key):\\n        if key not in self._models:\\n            raise inferencemodelnotfound(f\"model {key} not found\")\\n        return self._models[key]\\n\\n    def __len__(self):\\n        return len(self._models)\\n\\n    def keys(self):\\n        return self._models.keys()\\n\\n# global flag for api key check\\nmodels_cache_auth_enabled = false\\n\\n# --- unit tests ---\\n\\n@pytest.fixture\\ndef model_manager():\\n    # returns a fresh modelmanager with dummymodelregistry\\n    return modelmanager(dummymodelregistry())\\n\\n@pytest.fixture\\ndef cache_manager(model_manager):\\n    # returns a withfixedsizecache wrapping the above\\n    return withfixedsizecache(model_manager, max_size=4)\\n\\n# 1. basic test cases\\n\\ndef test_add_single_model_basic(cache_manager):\\n    \"\"\"test adding a single model to an empty cache.\"\"\"\\n    cache_manager.add_model(\"modela/1\", \"key\")\\n\\ndef test_add_duplicate_model_noop(cache_manager):\\n    \"\"\"test that adding the same model twice does not increase cache size.\"\"\"\\n    cache_manager.add_model(\"modela/1\", \"key\")\\n    cache_manager.add_model(\"modela/1\", \"key\")\\n\\ndef test_add_model_with_alias(cache_manager):\\n    \"\"\"test adding a model with an alias as queue id.\"\"\"\\n    cache_manager.add_model(\"modela/1\", \"key\", model_id_alias=\"aliasa\")\\n\\ndef test_add_model_with_different_aliases(cache_manager):\\n    \"\"\"test that different aliases are treated as different cache entries.\"\"\"\\n    cache_manager.add_model(\"modela/1\", \"key\", model_id_alias=\"aliasa\")\\n    cache_manager.add_model(\"modela/1\", \"key\", model_id_alias=\"aliasb\")\\n\\ndef test_add_multiple_models_basic(cache_manager):\\n    \"\"\"test adding multiple distinct models.\"\"\"\\n    cache_manager.add_model(\"modela/1\", \"key\")\\n    cache_manager.add_model(\"modelb/1\", \"key\")\\n    cache_manager.add_model(\"modelc/1\", \"key\")\\n\\n# 2. edge test cases\\n\\ndef test_add_model_eviction_lru(cache_manager):\\n    \"\"\"test that adding models over max_size evicts least recently used.\"\"\"\\n    # fill up cache\\n    cache_manager.add_model(\"a/1\", \"key\")\\n    cache_manager.add_model(\"b/1\", \"key\")\\n    cache_manager.add_model(\"c/1\", \"key\")\\n    cache_manager.add_model(\"d/1\", \"key\")\\n    # add one more, triggers eviction (removes a/1, b/1, c/1 in order)\\n    cache_manager.add_model(\"e/1\", \"key\")\\n    # add another, triggers more evictions\\n    cache_manager.add_model(\"f/1\", \"key\")\\n\\ndef test_add_model_lru_refresh(cache_manager):\\n    \"\"\"test that re-adding an existing model refreshes its lru position.\"\"\"\\n    cache_manager.add_model(\"a/1\", \"key\")\\n    cache_manager.add_model(\"b/1\", \"key\")\\n    cache_manager.add_model(\"c/1\", \"key\")\\n    cache_manager.add_model(\"d/1\", \"key\")\\n    # refresh a/1\\n    cache_manager.add_model(\"a/1\", \"key\")\\n    # add e/1, should evict b/1, c/1, d/1 (a/1 was refreshed)\\n    cache_manager.add_model(\"e/1\", \"key\")\\n\\n\\ndef test_add_model_with_invalid_model_id(cache_manager):\\n    \"\"\"test that a model_id_alias with same name as another model_id is treated as distinct.\"\"\"\\n    cache_manager.add_model(\"modela/1\", \"key\")\\n    cache_manager.add_model(\"modelb/1\", \"key\", model_id_alias=\"modela/1\")\\n\\ndef test_add_model_evicts_all_when_cache_full(cache_manager):\\n    \"\"\"test that if more than max_size+3 models are added, all old models are evicted.\"\"\"\\n    # fill cache\\n    cache_manager.add_model(\"a/1\", \"key\")\\n    cache_manager.add_model(\"b/1\", \"key\")\\n    cache_manager.add_model(\"c/1\", \"key\")\\n    cache_manager.add_model(\"d/1\", \"key\")\\n    # add 4 more, causing two eviction rounds\\n    cache_manager.add_model(\"e/1\", \"key\")\\n    cache_manager.add_model(\"f/1\", \"key\")\\n    cache_manager.add_model(\"g/1\", \"key\")\\n    cache_manager.add_model(\"h/1\", \"key\")\\n    # only last 4 models should remain\\n    for mid in [\"e/1\", \"f/1\", \"g/1\", \"h/1\"]:\\n        pass\\n    for mid in [\"a/1\", \"b/1\", \"c/1\", \"d/1\"]:\\n        pass\\n\\ndef test_add_model_handles_exception_and_removes_from_queue(cache_manager):\\n    \"\"\"test that if modelmanager.add_model raises, the queue is cleaned up.\"\"\"\\n    # patch model_manager.add_model to raise\\n    orig_add_model = cache_manager.model_manager.add_model\\n    def raise_exc(*a, **kw):\\n        raise valueerror(\"fail!\")\\n    cache_manager.model_manager.add_model = raise_exc\\n    with pytest.raises(valueerror):\\n        cache_manager.add_model(\"z/1\", \"key\")\\n    # restore\\n    cache_manager.model_manager.add_model = orig_add_model\\n\\ndef test_add_model_with_alias_and_duplicate(cache_manager):\\n    \"\"\"test that adding same model with and without alias treats them as separate.\"\"\"\\n    cache_manager.add_model(\"a/1\", \"key\")\\n    cache_manager.add_model(\"a/1\", \"key\", model_id_alias=\"aliasa\")\\n\\n# 3. large scale test cases\\n\\ndef test_add_many_models_and_evictions():\\n    \"\"\"test adding up to 20 models with cache size 10, check lru eviction.\"\"\"\\n    mm = modelmanager(dummymodelregistry())\\n    cache = withfixedsizecache(mm, max_size=10)\\n    # add 20 models\\n    for i in range(20):\\n        cache.add_model(f\"model{i}/1\", \"key\")\\n    # only last 10 should remain\\n    for i in range(10, 20):\\n        pass\\n    for i in range(10):\\n        pass\\n\\ndef test_add_models_with_aliases_large_scale():\\n    \"\"\"test adding models with unique aliases does not cause collisions.\"\"\"\\n    mm = modelmanager(dummymodelregistry())\\n    cache = withfixedsizecache(mm, max_size=50)\\n    # add 50 models with unique aliases\\n    for i in range(50):\\n        cache.add_model(f\"modelx/1\", \"key\", model_id_alias=f\"alias_{i}\")\\n    # all aliases should be present\\n    for i in range(50):\\n        pass\\n\\ndef test_lru_eviction_pattern_stress():\\n    \"\"\"test lru eviction pattern with repeated access and additions.\"\"\"\\n    mm = modelmanager(dummymodelregistry())\\n    cache = withfixedsizecache(mm, max_size=5)\\n    # add 5 models\\n    for i in range(5):\\n        cache.add_model(f\"m{i}/1\", \"key\")\\n    # access models to change lru order\\n    cache.add_model(\"m2/1\", \"key\")\\n    cache.add_model(\"m4/1\", \"key\")\\n    # add new model, should evict oldest (m0/1, m1/1, m3/1 in order)\\n    cache.add_model(\"m5/1\", \"key\")\\n    # only most recently used and new should remain\\n    for mid in [\"m2/1\", \"m4/1\", \"m5/1\"]:\\n        pass\\n\\ndef test_add_models_performance_under_load():\\n    \"\"\"test that adding 100 models with cache size 50 only keeps last 50.\"\"\"\\n    mm = modelmanager(dummymodelregistry())\\n    cache = withfixedsizecache(mm, max_size=50)\\n    for i in range(100):\\n        cache.add_model(f\"large_{i}/1\", \"key\")\\n    for i in range(50, 100):\\n        pass\\n    for i in range(50):\\n        pass\\n\\ndef test_add_models_with_same_alias_large_scale():\\n    \"\"\"test that adding many models with same alias overwrites previous.\"\"\"\\n    mm = modelmanager(dummymodelregistry())\\n    cache = withfixedsizecache(mm, max_size=10)\\n    for i in range(20):\\n        cache.add_model(f\"modelq_{i}/1\", \"key\", model_id_alias=\"shared_alias\")\\n# codeflash_output is used to check that the output of the original code is the same as that of the optimized code.\\n```\\n\\n\\nto edit these changes git checkout codeflash/optimize-pr1373-2025-06-24t21.57.17 and push.']"
        ],
        [
         "9",
         "9",
         "13",
         "9_sdk_flutter_cache_manager_apphost_git",
         "['sdk', 'flutter_cache_manager', 'apphost', 'git', 'ui', 'workflow', 'update', 'appcontrol', 'releases', 'improve']",
         "[\"finishing up work on appcontrol manager v.1.9.1.0added control flow guard support to the appcontrol manager. a great security feature that prevents certain exploits.\\n\\n\\nset intel's control-flow enforcement technology to explicitly enabled in the project.\\n\\n\\nthe native ahead-of-time compilation now favors execution speed and performance over package size. (will be monitoring this one as the app gets bigger when more features are implemented in the future. for now, it only increases the package size by 1mb)\\n\\n\\nenabled a new code analyzer to enforce more code best practices: dotnet_diagnostic.ca1724. (type names should not match namespaces)\\n\\n\\nupdated the appcontrol manager document.\\n\\n\\nformatted all xaml codes for better readability.\\n\\n\\nadjusted the app control simulation code based on the custom serialization/deserialization logic introduced in the previous update.\\n\\n\\nupdated microsoft.identitymodel.abstractions nuget dependency.\\n\\n\\naligned some columns in the new listviews.\", 'improve workflow system with bug fixes and optimizationsthis pr improves the workflow system with several bug fixes and optimizations:\\noverall, fixed mutliple warnings when running tests.\\nchanges made\\n\\nfix variable getter method redefinition: prevent redefining getter methods that already exist in each_step.rb\\noptimize sqlite state repository operations: improve database operations and state management\\nenhance step executor reporting: better error handling and reporting functionality\\nrefactor test suites: simplify and improve test maintainability across multiple test files\\nupdate interpolator test: use simpler expectation syntax for better readability\\n\\nfiles modified\\n\\nlib/roast/workflow/each_step.rb\\nlib/roast/workflow/sqlite_state_repository.rb\\nlib/roast/workflow/step_executor_with_reporting.rb\\ntest/roast/workflow/interpolator_test.rb\\ntest/roast/workflow/iteration_steps_test.rb\\ntest/roast/workflow/step_executor_with_reporting_test.rb\\ntest/roast/workflow/step_loader_test.rb\\n\\ntesting\\nall existing tests pass, and the refactored test suites are more maintainable and readable.\\nimpact\\nthese changes improve the robustness and maintainability of the workflow system while fixing potential bugs with method redefinition.', \"feat: add --download flag for faster flutter sdk installation feature: download flag for faster flutter sdk installation\\nadds a new --download flag to the fvm install command that downloads pre-built flutter sdk archives instead of git cloning, providing faster installation for official releases.\\n what's new\\ncommand usage\\n```bash\\ndownload pre-built flutter 3.24.0 archive\\nfvm install 3.24.0 --download\\nshort form\\nfvm install 3.24.0 -d\\nfalls back to git for unsupported versions\\nfvm install custom_version --download  # uses git cloning\\nfvm install myfork/stable --download    # uses git cloning\\n```\\nkey features\\n\\n faster installation: pre-built archives vs git clone + build\\n reduced bandwidth: smaller archive files vs full git repositories\\n smart fallback: automatically uses git when download isn't available\\n official releases only: supports stable, beta, dev channels\\n backward compatible: existing workflows unchanged\\n cross-platform: works on linux, macos, windows\\n\\n implementation details\\nfiles added\\n\\nlib/src/services/download_service.dart - handles archive downloads and extraction\\ntest/services/download_service_test.dart - comprehensive test suite\\n\\nfiles modified\\n\\nlib/src/commands/install_command.dart - added --download flag\\nlib/src/workflows/ensure_cache.workflow.dart - integrated download workflow\\nlib/src/utils/context.dart - registered downloadservice\\npubspec.yaml - added archive package dependency\\ntest/commands/install_command_test.dart - added flag parsing tests\\n\\narchitecture\\n\\ndownloadservice: handles tar.xz archive downloads and extraction\\nensurecacheworkflow: enhanced to support download mode with git fallback\\ninstallcommand: added --download flag integration\\nsmart detection: only official releases can be downloaded\\n\\n testing\\ntest coverage\\n\\n unit tests for downloadservice functionality\\n integration tests with real flutter releases api\\n command-line flag parsing tests\\n error scenarios and fallback behavior\\n archive structure validation\\n\\nquality checks\\n\\n dart analyze - no issues\\n dart fix --apply - applied fixes\\n all tests passing (11/11)\\n successful compilation\\n updated with latest main branch\\n\\n benefits\\n\\nperformance: significantly faster installation for official releases\\nreliability: less dependent on git infrastructure\\nbandwidth: reduced download size\\nuser experience: faster developer onboarding\\nrobustness: automatic fallback ensures compatibility\\n\\n backward compatibility\\n\\ndefault behavior unchanged - existing scripts continue to work\\ngit cloning remains the default installation method\\ndownload flag is opt-in only\\nall existing command options preserved\\n\\n code quality\\n\\nfollows dry, kiss, yagni principles\\nreuses existing services (flutterreleaseclient, cacheservice)\\nclean error handling with meaningful messages\\ncomprehensive test coverage\\nzero static analysis issues\\n\\n recent updates\\nlatest merge (a2d89c6): \\n-  merged with latest main branch\\n-  resolved conflicts in test files\\n-  maintained both download flag tests and new project config tests\\n-  all tests passing after merge\\n-  zero code quality issues\\n review notes\\n\\nimplementation focuses on official flutter releases only\\nsmart fallback ensures no breaking changes\\nuses existing fvm infrastructure and patterns\\nminimal code footprint with maximum benefit\\nbranch is up-to-date with main\\n\\ncloses #[issue-number] (if applicable)\"]"
        ],
        [
         "10",
         "10",
         "17",
         "10_backporting_fiber_benchmark_redirect_oldinputs_benchmark_redirect_parseandclearflashmessages",
         "['backporting', 'fiber', 'benchmark_redirect_oldinputs', 'benchmark_redirect_parseandclearflashmessages', 'backport', 'benchmarks', 'benchmark_redirect_route_withflashmessages', 'benchmark_redirect_route_withqueries', 'maintainers', 'changes']",
         "[\" refactor: the value of map is unused in uniqueroutestackdescription\\nthe value of map m in uniqueroutestack is unused. we can set the value type of m to struct{} instead of int to save some memory and improve code clarity and readability.\\ntype of change\\nplease delete options that are not relevant.\\n\\n[ ] new feature (non-breaking change which adds functionality)\\n[ ] enhancement (improvement to existing features and functionality)\\n[ ] documentation update (changes to documentation)\\n[ ] performance improvement (non-breaking change which improves efficiency)\\n[x] code consistency (non-breaking change which improves code reliability and robustness)\\n\\nchecklist\\nbefore you submit your pull request, please make sure you meet these requirements:\\n\\n[x] followed the inspiration of the express.js framework for new functionalities, making them similar in usage.\\n[x] conducted a self-review of the code and provided comments for complex or critical parts.\\n[ ] updated the documentation in the /docs/ directory for fiber's documentation.\\n[ ] added or updated unit tests to validate the effectiveness of the changes or new features.\\n[x] ensured that new and existing unit tests pass locally with the changes.\\n[ ] verified that any new dependencies are essential and have been agreed upon by the maintainers/community.\\n[ ] aimed for optimal performance with minimal allocations in the new code.\\n[ ] provided benchmarks for the new code to analyze and improve upon.\", \" refactor: migrate randstring to rand v2description\\nthis pr migrates randstring from math/rand to math/rand/v2 to improve randomness quality and concurrency safety. in addition, it reduces execution time by approximately 97%, as shown in the benchmark test below.\\ngo\\nfunc benchmark_randomstring(b *testing.b) {\\n    for i := 0; i < b.n; i++ {\\n        _ = randstring(100)\\n    }\\n}\\ngoos: linux\\ngoarch: amd64\\npkg: github.com/gofiber/fiber/v3/client\\ncpu: amd epyc 7763 64-core processor                \\n                   old.txt                   new.txt               \\n                    sec/op       sec/op     vs base                \\n_randomstring-4   9862.0n ± 0%   252.1n ± 3%  -97.44% (p=0.000 n=10)\\nchanges introduced\\nlist the new features or adjustments introduced in this pull request. provide details on benchmarks, documentation updates, changelog entries, and if applicable, the migration guide.\\n\\n[x] benchmarks: describe any performance benchmarks and improvements related to the changes.\\n[ ] documentation update: detail the updates made to the documentation and links to the changed files.\\n[ ] changelog/what's new: include a summary of the additions for the upcoming release notes.\\n[ ] migration guide: if necessary, provide a guide or steps for users to migrate their existing code to accommodate these changes.\\n[ ] api alignment with express: explain how the changes align with the express api.\\n[ ] api longevity: discuss the steps taken to ensure that the new or updated apis are consistent and not prone to breaking changes.\\n[ ] examples: provide examples demonstrating the new features or changes in action.\\n\\ntype of change\\n\\n[x] performance improvement (non-breaking change which improves efficiency)\\n[x] code consistency (non-breaking change which improves code reliability and robustness)\\n\\nchecklist\\nbefore you submit your pull request, please make sure you meet these requirements:\\n\\n[x] followed the inspiration of the express.js framework for new functionalities, making them similar in usage.\\n[x] conducted a self-review of the code and provided comments for complex or critical parts.\\n[ ] updated the documentation in the /docs/ directory for fiber's documentation.\\n[ ] added or updated unit tests to validate the effectiveness of the changes or new features.\\n[x] ensured that new and existing unit tests pass locally with the changes.\\n[ ] verified that any new dependencies are essential and have been agreed upon by the maintainers/community.\\n[x] aimed for optimal performance with minimal allocations in the new code.\\n[x] provided benchmarks for the new code to analyze and improve upon.\", \":bug: bug: fix redirection flash messages violate cookie structuredescription\\nthis issue fixes redirection with flash messages breaks http header format by encoding msgpack serialized binary data with hex. it has little overhead, but still we get benefit of using messagepack. here are the benchmark results:\\n```go\\nnew:\\nbenchmark_redirect_route-16                              6792206           173.7 ns/op        16 b/op          1 allocs/op\\nbenchmark_redirect_route_withqueries-16                  3043641           391.3 ns/op        40 b/op          2 allocs/op\\nbenchmark_redirect_route_withflashmessages-16            2439327           481.1 ns/op       293 b/op          3 allocs/op\\nbenchmark_redirect_parseandclearflashmessages-16         2350215           510.5 ns/op       192 b/op          7 allocs/op\\nbenchmark_redirect_processflashmessages-16               3483320           344.6 ns/op       288 b/op          2 allocs/op\\nbenchmark_redirect_messages-16                          13537572            87.54 ns/op      128 b/op          2 allocs/op\\nbenchmark_redirect_oldinputs-16                         14712171            77.44 ns/op       96 b/op          2 allocs/op\\nbenchmark_redirect_message-16                           83187558            14.43 ns/op        0 b/op          0 allocs/op\\nbenchmark_redirect_oldinput-16                          153820603            7.814 ns/op           0 b/op          0 allocs/op\\nold:\\nbenchmark_redirect_route-16                              6861298           175.9 ns/op        16 b/op          1 allocs/op\\nbenchmark_redirect_route_withqueries-16                  2901906           408.6 ns/op        40 b/op          2 allocs/op\\nbenchmark_redirect_route_withflashmessages-16            3456422           328.7 ns/op       117 b/op          2 allocs/op\\nbenchmark_redirect_parseandclearflashmessages-16         4298755           278.1 ns/op        32 b/op          6 allocs/op\\nbenchmark_redirect_processflashmessages-16               6022623           198.2 ns/op       112 b/op          1 allocs/op\\nbenchmark_redirect_messages-16                          13548512            87.84 ns/op      128 b/op          2 allocs/op\\nbenchmark_redirect_oldinputs-16                         14798696            78.96 ns/op       96 b/op          2 allocs/op\\nbenchmark_redirect_message-16                           82884747            14.44 ns/op        0 b/op          0 allocs/op\\nbenchmark_redirect_oldinput-16                          148364563            8.104 ns/op           0 b/op          0 allocs/op\\n```\\nfixes  <URL> \\ntype of change\\n\\n[x] enhancement (improvement to existing features and functionality)\\n\\nchecklist\\nbefore you submit your pull request, please make sure you meet these requirements:\\n\\n[ ] followed the inspiration of the express.js framework for new functionalities, making them similar in usage.\\n[ ] conducted a self-review of the code and provided comments for complex or critical parts.\\n[ ] updated the documentation in the /docs/ directory for fiber's documentation.\\n[ ] added or updated unit tests to validate the effectiveness of the changes or new features.\\n[ ] ensured that new and existing unit tests pass locally with the changes.\\n[ ] verified that any new dependencies are essential and have been agreed upon by the maintainers/community.\\n[ ] aimed for optimal performance with minimal allocations in the new code.\\n[ ] provided benchmarks for the new code to analyze and improve upon.\\n\\ncommit formatting\\nplease use emojis in commit messages for an easy way to identify the purpose or intention of a commit. check out the emoji cheatsheet here: contributing.md\"]"
        ],
        [
         "11",
         "11",
         "12",
         "11_optimization_datafix_prefetch_performance",
         "['optimization', 'datafix', 'prefetch', 'performance', 'filter', 'buildslotswithdateranges', 'algorithmic', 'predicates', 'index', 'benchmark_equalfieldtype']",
         "[\"index followers to better support our query patternswe frequently query followers filtered by followed_id and ordered by\\nconfirmed_at. (e.g. on the /followers page)\\nthis could take 80+ seconds for sellers with a large amount of\\nfollowing.\\ni am hoping that this composite index on (followed_id, confirmed_at) can\\nhelp more efficiently perform range scans without a filesort, improving\\nquery performances.\\ni left these out of the composite index:\\n\\ndeleted_at: vast majority of the records should have deleted_at is\\n  null\\nid: i think this is only going to help if we switch to -based\\n  pagination (unlikely to be prioritized)\\n\\ni had tested this with a local table of 1m records. will further\\nbench this once it's rolled out and make adjustments if needed.\", 'start with minimal prefetch distance to minimize prefetch overhead for exact or limited index scansproblem\\nsee  <URL> \\nin case of queries index scan with limit clause, multiple backends can concurrently send larger number of duplicated prefetch requests which are not stored in lfc and so actually do useless job.\\ncurrent implementation of index prefetch starts with maximal prefetch distance (10 by default now) when there are no key bounds, so in queries with limit clause like select * from t order by pk limit 1 compute can send a lot of useless prefetch requests to page server.\\nsummary of changes\\nalways start with minimal prefetch distance even if there are not key boundaries.\\nrelated postgres prs:\\n <URL> \\n <URL> \\n <URL> \\n <URL> ', \"perf: short-circuit selection evaluationshort-circuit evaluation of conjunction predicates in filters.\\noptimizer rule for reordering of conjunction filter predicates to take advantage of short-circuiting.\\n\\nbefore:\\nglaredb> select searchphrase, min(url), count(*) as c from hits where url like '%google%' and searc\\n     ... hphrase <> '' group by searchphrase order by c desc limit 10;\\n\\n searchphrase                    min                                                     c     \\n utf8                            utf8                                                    int64 \\n\\n прокур горбуши                   <URL>                         60 \\n римском качественны for cry      <URL>                           24 \\n стоит похуден                    <URL>      23 \\n испанч боб новости дейская       <URL>      21 \\n прокур готовки видеоэндоменя     <URL>                         14 \\n прокур гипоаллеры                <URL>                         11 \\n камедицинск авт…                 <URL>       9 \\n универ 11.6/1366x768/40…         <URL>       8 \\n купить трудован…                 <URL>       7 \\n вспомню о названы монстэр        <URL>       7 \\n\\nexecution duration (s): 3.31950\\nafter:\\nglaredb> select searchphrase, min(url), count(*) as c from hits where url like '%google%' and searc\\n     ... hphrase <> '' group by searchphrase order by c desc limit 10;\\n\\n searchphrase                    min                                                     c     \\n utf8                            utf8                                                    int64 \\n\\n прокур горбуши                   <URL>                         60 \\n римском качественны for cry      <URL>                           24 \\n стоит похуден                    <URL>      23 \\n испанч боб новости дейская       <URL>      21 \\n прокур готовки видеоэндоменя     <URL>                         14 \\n прокур гипоаллеры                <URL>                         11 \\n камедицинск авт…                 <URL>       9 \\n универ 11.6/1366x768/40…         <URL>       8 \\n вспомню о названы монстэр        <URL>       7 \\n купить трудован…                 <URL>       7 \\n\\nexecution duration (s): 2.55737\"]"
        ],
        [
         "12",
         "12",
         "6",
         "12_fixes_coderabbit_commit_automation",
         "['fixes', 'coderabbit', 'commit', 'automation', 'chore', 'workflow', 'dashboard', 'debounce', 'widgets', 'ide']",
         "['chore: remove analytics execution from the critical pathdescription\\n\\npushed out the sendexecuteanalyticsevent from the critical path of returning action\\'s execution result.\\nimproved the critical path of sendexecuteanalyticsevent by running the application mono concurrent to other events.\\nadded more telemetry code around the execution flow.\\n\\nfixes #issue number\\nor\\nfixes issue url\\n\\n[!warning]\\nif no issue exists, please create an issue first, and check with the maintainers if the issue is valid.\\n\\nautomation\\n/ok-to-test tags=\"@tag.all\"\\n:mag: cypress test results\\n this is an auto-generated comment: cypress test results  \\n\\n[!tip]\\n   all cypress tests have passed!   \\nworkflow run:  <URL> \\ncommit: ddf93dd06cd4facabdde5898d1cc40ce7dc4765f\\ncypress dashboard.\\ntags: @tag.all\\nspec:\\ntue, 18 mar 2025 10:28:52 utc\\n\\n end of auto-generated comment: cypress test results  \\ncommunication\\nshould the devrel and marketing teams inform users about this change?\\n- [ ] yes\\n- [ ] no\\n this is an auto-generated comment: release notes by coderabbit.ai \\nsummary by coderabbit\\n\\nnew features\\nintroduced additional action tracking identifiers to support enhanced analytics and authentication validation.\\n\\n\\nrefactor\\noptimized asynchronous operations for data retrieval to improve responsiveness.\\nenhanced the flow and error handling of action execution, ensuring smoother and more reliable performance.\\n\\n\\n\\n end of auto-generated comment: release notes by coderabbit.ai', 'chore: adding debounce to onvaluechange for input widgetsdescription\\nadding debounce to onvaluechange for input widgets to fix multiple execute api calls happening in reactive queries flow.\\nfixes #40813\\nautomation\\n/ok-to-test tags=\"@tag.all\"\\n:mag: cypress test results\\n this is an auto-generated comment: cypress test results  \\n\\n[!tip]\\n   all cypress tests have passed!   \\nworkflow run:  <URL> \\ncommit: 6943ba5d0df915256cf29831df53e9ff9880d617\\ncypress dashboard.\\ntags: @tag.all\\nspec:\\nfri, 06 jun 2025 09:40:52 utc\\n\\n end of auto-generated comment: cypress test results  \\ncommunication\\nshould the devrel and marketing teams inform users about this change?\\n- [ ] yes\\n- [ ] no\\n this is an auto-generated comment: release notes by coderabbit.ai \\nsummary by coderabbit\\n\\nnew features\\ninput widgets now update their displayed values instantly while saving changes in the background with a short delay, improving typing responsiveness.\\ninput changes are grouped and saved after a brief pause, reducing unnecessary updates and enhancing performance.\\nbug fixes\\ninput fields now stay in sync with external updates and clear any pending background updates when needed, preventing outdated or duplicate changes.\\nchores\\nimproved cleanup of background processes when input widgets are removed, ensuring smoother operation.\\ntests\\nadded typing delays in input simulation during cypress tests to better mimic real user input timing.\\n\\n end of auto-generated comment: release notes by coderabbit.ai', 'chore: ce changes related to decoupling webworkerdescription\\nwe are improving the lcp by reducing the time to reach the first evaluation, aiming for a 1.8 to 2.2 second reduction. to achieve this, we’ve implemented the following changes:\\ncode splitting of widgets: during page load, only the widgets required for the initial evaluation are loaded and registered. the remaining widgets are registered after the first evaluation message is sent. this parallelizes widget loading with evaluation computation, reducing the critical path.\\nweb worker offloading: macro tasks such as clearcache and javascript library installation have been moved to the web worker setup. these are now executed in a separate thread, allowing the firstunevaluatedtree to be computed in parallel with js library installation.\\nparallel js library loading: all javascript libraries are now loaded in parallel within the web worker, instead of sequentially, improving efficiency.\\ndeferred rendering of appviewer: we now render the appviewer component only after registering the remaining widgets. this ensures that heavy rendering tasks—such as expensive selector computations and loading additional chunks related to the appviewer—can execute in parallel with the first evaluation, further enhancing performance.\\nautomation\\n/ok-to-test tags=\"@tag.all\"\\n:mag: cypress test results\\n this is an auto-generated comment: cypress test results  \\n\\n[!caution]\\n   some tests have failed.\\nworkflow run:  <URL> \\ncommit: 2dc9dbcd6b60cb63ec954713dbf7335d788df9a4\\ncypress dashboard.\\ntags: @tag.all\\nspec: \\nthe following are new failures, please fix them before merging the pr: \\ncypress/e2e/regression/clientside/otheruifeatures/analytics_spec.js\\nlist of identified flaky tests.\\nthu, 26 jun 2025 07:57:26 utc\\n\\n end of auto-generated comment: cypress test results  \\ncommunication\\nshould the devrel and marketing teams inform users about this change?\\n- [ ] yes\\n- [ ] no\\n this is an auto-generated comment: release notes by coderabbit.ai \\nsummary by coderabbit\\n\\nnew features\\nadded support for deferred loading of javascript libraries and improved control over page rendering and first page load behavior.\\nintroduced granular widget registration, allowing partial widget initialization for faster initial rendering.\\nadded new redux actions and selectors to manage and track evaluation and rendering state.\\n\\nadded explicit cache clearing for widget factory memoization functions.\\n\\n\\nimprovements\\n\\nrefactored widget loading to be asynchronous and on-demand, reducing initial load time and improving modularity.\\nenhanced sagas and reducers to better handle first-time evaluations and widget registration.\\noptimized js library loading to occur in parallel for improved performance.\\nmodularized theme application and improved conditional rendering in the app viewer.\\nreorganized widget registration to initialize widgets individually rather than in bulk.\\nimproved memoization decorator to allow explicit cache clearing globally.\\nupdated evaluation sagas to support partial widget initialization and deferred js library loading.\\n\\nupdated widget loading utilities and tests to support asynchronous dynamic loading.\\n\\n\\nbug fixes\\n\\n\\nimproved conditional logic to prevent errors when rendering components with missing functions.\\n\\n\\ntests\\n\\nexpanded and refactored test suites to cover asynchronous widget loading, partial initialization, and evaluation saga behaviors.\\n\\nadded tests verifying widget factory cache behavior and first evaluation integration.\\n\\n\\nchores\\n\\nupdated imports and code structure for clarity and maintainability.\\nreorganized type imports and moved interface declarations to dedicated modules.\\n\\n end of auto-generated comment: release notes by coderabbit.ai']"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 13
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>54</td>\n",
       "      <td>0_github_refactor_lint_fixes</td>\n",
       "      <td>[github, refactor, lint, fixes, mlflow, pnpm, ...</td>\n",
       "      <td>[swap doc preview and test steps to view the p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>1_cuda_optimisations_bugfixes_fix</td>\n",
       "      <td>[cuda, optimisations, bugfixes, fix, optimisat...</td>\n",
       "      <td>[disable cache on ci on windows because downlo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>2_coderabbit_fixes_release_refactor</td>\n",
       "      <td>[coderabbit, fixes, release, refactor, renderi...</td>\n",
       "      <td>[grida canvas - skia-safe rust backend - stand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>35</td>\n",
       "      <td>3_benchmark_dotnet_msbuild_netperf</td>\n",
       "      <td>[benchmark, dotnet, msbuild, netperf, performa...</td>\n",
       "      <td>[add comprehensive vibetunnel protocol benchma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>4_builtins_bun_tests_typescript</td>\n",
       "      <td>[builtins, bun, tests, typescript, builtin, de...</td>\n",
       "      <td>[report memory cost of sourcemaps to gcwhat do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>5_parser_parse_chunksprefetch_preload</td>\n",
       "      <td>[parser, parse, chunksprefetch, preload, resiz...</td>\n",
       "      <td>[@remotion/renderer: consider --memory flag fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>6_cxx_compiler_c_compiler_cmake_compiler</td>\n",
       "      <td>[cxx_compiler, c_compiler, cmake, compiler, to...</td>\n",
       "      <td>[[mlas] dequantizelinear int8/uint8description...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>7_refactor_caching_invoices_coderabbit</td>\n",
       "      <td>[refactor, caching, invoices, coderabbit, impr...</td>\n",
       "      <td>[cached repetitive data lookups for creator an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>8_execute_gpt_4v_request_crewagentparser_jsonp...</td>\n",
       "      <td>[execute_gpt_4v_request, crewagentparser, json...</td>\n",
       "      <td>[ speed up function execute_gpt_4v_request by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>9_sdk_flutter_cache_manager_apphost_git</td>\n",
       "      <td>[sdk, flutter_cache_manager, apphost, git, ui,...</td>\n",
       "      <td>[finishing up work on appcontrol manager v.1.9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>10_backporting_fiber_benchmark_redirect_oldinp...</td>\n",
       "      <td>[backporting, fiber, benchmark_redirect_oldinp...</td>\n",
       "      <td>[ refactor: the value of map is unused in uniq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>11_optimization_datafix_prefetch_performance</td>\n",
       "      <td>[optimization, datafix, prefetch, performance,...</td>\n",
       "      <td>[index followers to better support our query p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>12_fixes_coderabbit_commit_automation</td>\n",
       "      <td>[fixes, coderabbit, commit, automation, chore,...</td>\n",
       "      <td>[chore: remove analytics execution from the cr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic  Count                                               Name  \\\n",
       "0       0     54                       0_github_refactor_lint_fixes   \n",
       "1       1     35                  1_cuda_optimisations_bugfixes_fix   \n",
       "2       2     27                2_coderabbit_fixes_release_refactor   \n",
       "3       3     35                 3_benchmark_dotnet_msbuild_netperf   \n",
       "4       4     18                    4_builtins_bun_tests_typescript   \n",
       "5       5     16              5_parser_parse_chunksprefetch_preload   \n",
       "6       6     15           6_cxx_compiler_c_compiler_cmake_compiler   \n",
       "7       7     13             7_refactor_caching_invoices_coderabbit   \n",
       "8       8     12  8_execute_gpt_4v_request_crewagentparser_jsonp...   \n",
       "9       9     13            9_sdk_flutter_cache_manager_apphost_git   \n",
       "10     10     17  10_backporting_fiber_benchmark_redirect_oldinp...   \n",
       "11     11     12       11_optimization_datafix_prefetch_performance   \n",
       "12     12      6              12_fixes_coderabbit_commit_automation   \n",
       "\n",
       "                                       Representation  \\\n",
       "0   [github, refactor, lint, fixes, mlflow, pnpm, ...   \n",
       "1   [cuda, optimisations, bugfixes, fix, optimisat...   \n",
       "2   [coderabbit, fixes, release, refactor, renderi...   \n",
       "3   [benchmark, dotnet, msbuild, netperf, performa...   \n",
       "4   [builtins, bun, tests, typescript, builtin, de...   \n",
       "5   [parser, parse, chunksprefetch, preload, resiz...   \n",
       "6   [cxx_compiler, c_compiler, cmake, compiler, to...   \n",
       "7   [refactor, caching, invoices, coderabbit, impr...   \n",
       "8   [execute_gpt_4v_request, crewagentparser, json...   \n",
       "9   [sdk, flutter_cache_manager, apphost, git, ui,...   \n",
       "10  [backporting, fiber, benchmark_redirect_oldinp...   \n",
       "11  [optimization, datafix, prefetch, performance,...   \n",
       "12  [fixes, coderabbit, commit, automation, chore,...   \n",
       "\n",
       "                                  Representative_Docs  \n",
       "0   [swap doc preview and test steps to view the p...  \n",
       "1   [disable cache on ci on windows because downlo...  \n",
       "2   [grida canvas - skia-safe rust backend - stand...  \n",
       "3   [add comprehensive vibetunnel protocol benchma...  \n",
       "4   [report memory cost of sourcemaps to gcwhat do...  \n",
       "5   [@remotion/renderer: consider --memory flag fr...  \n",
       "6   [[mlas] dequantizelinear int8/uint8description...  \n",
       "7   [cached repetitive data lookups for creator an...  \n",
       "8   [ speed up function execute_gpt_4v_request by ...  \n",
       "9   [finishing up work on appcontrol manager v.1.9...  \n",
       "10  [ refactor: the value of map is unused in uniq...  \n",
       "11  [index followers to better support our query p...  \n",
       "12  [chore: remove analytics execution from the cr...  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_topics = topic_model.reduce_outliers(docs, topics, embeddings=embs, strategy=\"embeddings\")\n",
    "#new_topics = topic_model.reduce_outliers(docs, topics, probabilities=probs, strategy=\"probabilities\")\n",
    "topic_model.update_topics(docs, topics=new_topics,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    representation_model=representation_model,\n",
    "    ctfidf_model=ctfidf_model )\n",
    "topic_model.get_topic_info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
